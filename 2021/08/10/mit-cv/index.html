<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-bounce.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"songtianhui.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="暑假上了mit的机器学习课程，虽然时间很短内容不多，但觉得第二部分cv的老师讲得很好，所以做一篇笔记。">
<meta property="og:type" content="article">
<meta property="og:title" content="mit机器学习cv">
<meta property="og:url" content="https://songtianhui.github.io/2021/08/10/mit-cv/index.html">
<meta property="og:site_name" content="Songtianhui&#39;s Blog">
<meta property="og:description" content="暑假上了mit的机器学习课程，虽然时间很短内容不多，但觉得第二部分cv的老师讲得很好，所以做一篇笔记。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/image_dev.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-10%2018-51-05.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-10%2018-57-36.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-10%2021-35-47.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-10%2022-47-54.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-11%2018-07-27.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-11%2018-47-07.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-11%2021-44-11.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-11%2021-44-40.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-11%2021-53-42.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-12%2018-36-59.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-12%2018-43-11.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-12%2018-46-16.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-12%2018-57-17.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-13%2000-33-13.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-13%2000-54-23.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-13%2001-05-39.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-13%2016-28-23.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-13%2017-01-47.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-14%2001-11-02.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-14%2001-26-54.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-14%2001-41-48.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-14%2001-57-13.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-14%2016-55-29.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-14%2022-45-32.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-14%2022-56-52.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-14%2022-59-50.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-14%2023-32-38.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-15%2018-45-33.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-15%2022-06-36.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/Screenshot%20from%202021-08-15%2023-13-44.png">
<meta property="article:published_time" content="2021-08-10T09:46:40.000Z">
<meta property="article:modified_time" content="2021-08-16T14:30:58.032Z">
<meta property="article:author" content="Hui hui">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://songtianhui.github.io/2021/08/10/mit-cv/image_dev.png">

<link rel="canonical" href="https://songtianhui.github.io/2021/08/10/mit-cv/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>mit机器学习cv | Songtianhui's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="/custom_css_source.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Songtianhui's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://songtianhui.github.io/2021/08/10/mit-cv/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Hui hui">
      <meta itemprop="description" content="Be happy forever!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Songtianhui's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          mit机器学习cv
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-08-10 17:46:40" itemprop="dateCreated datePublished" datetime="2021-08-10T17:46:40+08:00">2021-08-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-08-16 22:30:58" itemprop="dateModified" datetime="2021-08-16T22:30:58+08:00">2021-08-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>暑假上了mit的机器学习课程，虽然时间很短内容不多，但觉得第二部分cv的老师讲得很好，所以做一篇笔记。</p>
<a id="more"></a>
<h1 id="lecture-1"><a href="#lecture-1" class="headerlink" title="lecture 1"></a>lecture 1</h1><p>第一节课是对 deep learning 和 computer vision 的一个介绍。计算机视觉，就是指观察图像，像素，理解图像里出现了是什么，预测会发生什么。</p>
<h2 id="神经元（Perceptron）"><a href="#神经元（Perceptron）" class="headerlink" title="神经元（Perceptron）"></a>神经元（Perceptron）</h2><p>具体概念已经看了太多遍了，不再赘述。</p>
<p>对于优化的概念，$W \leftarrow W - \eta \frac{\partial J(W)}{\partial W}$，他解释的很好。这个偏导项就是代价函数随着 $W$ 的变化增长最快的方向，再加上一个小步长，就是移动一点点，因为是最小值所以是反方向移动，是负号。</p>
<h2 id="边缘检测-Edges"><a href="#边缘检测-Edges" class="headerlink" title="边缘检测 Edges"></a>边缘检测 Edges</h2><p>图上的边特征：</p>
<ul>
<li>图像梯度 Image gradient: $\nabla I = \left( \dfrac{\partial I}{\partial x}, \dfrac{\partial I}{\partial y} \right)$</li>
<li>图像导数的估算 Approximation image derivative: $\dfrac{\partial I}{\partial x} \simeq I(x, y) - I(x - 1, y)$</li>
<li>边缘强度 Edge strength: $E(x, y) = |\nabla I(x, y)|$</li>
<li>边缘取向 Edge orientation: $\theta(x, y) = \angle \nabla I = \arctan{\dfrac{\partial I /\partial y}{\partial I / \partial x}}$</li>
<li>边缘法线 Edge normal: $n = \dfrac{\nabla I}{|\nabla I|}$</li>
</ul>
<p>如果我们使用梯度算法，并绘制出边缘的强度，我们可以看到所有的边缘都暴露出来了。我们可以将这些微分方程或公式转换成图像强度的离散线性约束，这对于定义图像变化非常重要。</p>
<h2 id="卷积-Convolution"><a href="#卷积-Convolution" class="headerlink" title="卷积 Convolution"></a>卷积 Convolution</h2><p>卷积是一个操作两个函数的数学操作，输入两个函数输出一个（函数复合）。</p>
<p>通过滑动函数来计算，一个函数（kernel）划过另一个函数（template），在每一步把他们相乘再相加。</p>
<script type="math/tex; mode=display">
f[n] = h \circ g = \sum\limits_{k = 0}^{N - 1} h[n - k] g[k]</script><p>我们就可以用卷积来计算图像导数。</p>
<p>比如用</p>
<script type="math/tex; mode=display">
(E \otimes k)(x, y)=\sum\limits_{i=0}^{m-1} \sum\limits_{j=0}^{n-1} E(x+i, y+j) k(i, j)</script><p>能用来计算</p>
<script type="math/tex; mode=display">
\dfrac{\partial E(x, y)}{\partial x}=E(x+1, y)-E(x, y)</script><p>当 $k = [-1, 1]$。</p>
<p>然后我们可以对图像导数的定义再扩展，运用卷积计算更加复杂的一些特征。</p>
<p><img data-src="image_dev.png" alt></p>
<p>然后已知两个方向的导数，就可以用三角函数计算出任意方向的导数而不是真的创造任意方向的导数。</p>
<p>将导数可视化：</p>
<p><img data-src="Screenshot from 2021-08-10 18-51-05.png" alt></p>
<p>甚至我们可以创造不同类型的过滤器，不只是计算导数，实现更多的图像处理技术。</p>
<p><img data-src="Screenshot from 2021-08-10 18-57-36.png" alt></p>
<p><em>这是我对卷积和图像处理认识的第一步，我感到非常震撼，原来我们所在ps等图像处理软件上所做的处理，其原理真的就是矩阵对于像素的计算。不由得想到jyy的名言：计算机世界没有魔法(。</em></p>
<p>而我们在这节课学的是如何去学习卷积的参数而不是导数卷积，也就是说找到最好的 kernel。</p>
<p>我们可以把很多层卷基层堆积在一起，形成一个卷积块（我自己起的名字），维度是 $H \times W \times D$，$D$ 深度是过滤层的个数。</p>
<p>步幅（stride）。</p>
<p>卷基层的特点：</p>
<ul>
<li>空间不变性（Spatial invariance）</li>
<li>批量处理，高效并行</li>
<li>图像过滤</li>
<li>共享参数，高效</li>
<li>多种大小输入</li>
</ul>
<p><strong>gabor filters</strong></p>
<h2 id="池化-Pooling"><a href="#池化-Pooling" class="headerlink" title="池化 Pooling"></a>池化 Pooling</h2><p>Pooling 和 convolution 不一样的地方是，卷积在于从一块像素中提取信息，而 pooling 是为了将信息压缩，使得一块像素塌缩成一个单位，也是对数据的抽样，对信息流的限制。</p>
<p><strong>max pooling</strong></p>
<h2 id="结合起来"><a href="#结合起来" class="headerlink" title="结合起来"></a>结合起来</h2><p>我们就可以叠buff，一层conv一层relu一层pooling，一层conv一层relu一层pooling。。。实现一个端对端模型，一层层提取结构。</p>
<p>General CNN architecture:</p>
<p><img data-src="Screenshot from 2021-08-10 21-35-47.png" alt></p>
<h1 id="Lecture-2"><a href="#Lecture-2" class="headerlink" title="Lecture 2"></a>Lecture 2</h1><h2 id="CNN-Architectures"><a href="#CNN-Architectures" class="headerlink" title="CNN Architectures"></a>CNN Architectures</h2><ul>
<li>LeNet: LeCun et al. 1998. <ul>
<li>第一个视觉神经网络。</li>
</ul>
</li>
<li>AlexNet: Krizhevsky et al. NeurIPS 2012. <ul>
<li>两个信息流并行，更深度。</li>
</ul>
</li>
<li>GoogLeNet/Inception: Szegedy et al. CVPR 2015. <ul>
<li>Inception module</li>
<li>更小的卷积，更深，更精确。</li>
</ul>
</li>
<li>VGGNet: Simonyan et al. ICLR 2015.</li>
</ul>
<p><del>CV历史</del></p>
<h2 id="残差连接-Scaling-CNNs-residual-connections"><a href="#残差连接-Scaling-CNNs-residual-connections" class="headerlink" title="残差连接 Scaling CNNs: residual connections"></a>残差连接 Scaling CNNs: residual connections</h2><p>普通的卷积层的代价函数过于复杂，容易陷入局部最优，很难找到全局最优。简单的卷积加池化无法使模型更好。</p>
<p>我们需要学习残差的变化而不是实际的端对端函数。</p>
<h2 id="残差区块-Residual-blocks"><a href="#残差区块-Residual-blocks" class="headerlink" title="残差区块 Residual blocks"></a>残差区块 Residual blocks</h2><p><img data-src="Screenshot from 2021-08-10 22-47-54.png" alt></p>
<p>不是只学习从 $x$ 到输出，我们学习的是变化、残差，以达到输出的目的。它能够把问题变得更简单。</p>
<p>优点：</p>
<ul>
<li>更快的梯度传输。</li>
<li>只应用少量的残余值而不是整个函数值。</li>
<li>保持输入的结构。</li>
</ul>
<p>当输出和输入的结构不一样时，加一个权重层来投影到正确的维度。</p>
<p>有各种各样的残差连接。</p>
<ul>
<li>ResNet: He et al. CVPR 2016.<ul>
<li>使得机器图片识别准确性开始超过人类。</li>
<li>突破计算机可以训练的层数，$25 \to 150$。</li>
<li>代价函数更佳平滑，容易找到全局最优。</li>
</ul>
</li>
</ul>
<h2 id="数据集-Dataset"><a href="#数据集-Dataset" class="headerlink" title="数据集 Dataset"></a>数据集 Dataset</h2><p>深度学习不是什么都能学的，要仔细选择数据集，不能往里面“扔垃圾”。</p>
<blockquote>
<p>Garbage in, Garbage out.</p>
</blockquote>
<ul>
<li>MNIST</li>
<li>Image NET</li>
<li>CIFAR  10/100<ul>
<li>Facet: tool for vusualization of train data</li>
</ul>
</li>
<li>Object Net</li>
<li>MiniPlaces: scene recognition</li>
</ul>
<h1 id="Lecture-3"><a href="#Lecture-3" class="headerlink" title="Lecture 3"></a>Lecture 3</h1><p>本节主要讲图像序列，连续图像的处理。</p>
<h2 id="循环神经网络-Recurrent-neural-network"><a href="#循环神经网络-Recurrent-neural-network" class="headerlink" title="循环神经网络 Recurrent neural network"></a>循环神经网络 Recurrent neural network</h2><p>为了建模序列，我们需要：</p>
<ol>
<li>处理变化长度的序列。</li>
<li>跟踪长期的依赖关系。</li>
<li>保持信息的时间顺序。</li>
<li>在序列中的共享参数。</li>
</ol>
<p>原本我们的模型都是一对一的，现在我妈们可以考虑一对多、多对一、多对多预测，多对多分类。我们需要更新我们的模型，使得能够整合连续序列信息。</p>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>老师在这门课上并没有详细地讲解 RNN 的数学理论，简单介绍一下。</p>
<p>在 RNN 中，我们有时间 $t$ 这个概念，在每个时间点会有一个输入 $x_t$，然后有一个隐藏状态 $h_t$ 在每个时间点，状态序列就通过一个循环过程产生：</p>
<script type="math/tex; mode=display">
h_t = f_W(h_{t - 1}, x_t)</script><p>注意每个时间点所用的函数都是一样（保持时间对称性），也就是当前状态由前一个状态和当前输入决定，其实很像数电中状态机、时序的概念。</p>
<p>具体的，会由两个矩阵来线性组合出当前状态：</p>
<script type="math/tex; mode=display">
h_t = \tanh{(W_{hh}^Th_{t-1} + W_{xh}^{T}x_t)}\\
\hat{y}_t = W_{hy}^{T}h_t</script><p>这个 $\hat{y}$ 是对当前状态产生的一个输出，因为 $h$ 可以看作只是状态的一个编码。</p>
<p>然后其中的权重矩阵就会由学习得到。</p>
<p><img data-src="Screenshot from 2021-08-11 18-07-27.png" alt></p>
<p>如果我们考虑将所有时间的信息都记录下来，长距离的依赖会导致问题：</p>
<ul>
<li>需要记录的东西和内存会随着时间不断增加。</li>
<li>没有无限大的参数集来建模所有依赖。</li>
<li>RNN假设：当前隐藏层状态只依赖于前一个时间点的状态。</li>
<li>想法是建立隐藏状态来建模长距离的依赖。</li>
</ul>
<p>这些想法促使了我们需要一个有更好建模长距离依赖能力的新模型架构。</p>
<h3 id="长短时记忆单元-Long-short-term-memory-unit"><a href="#长短时记忆单元-Long-short-term-memory-unit" class="headerlink" title="长短时记忆单元 Long short term memory unit"></a>长短时记忆单元 Long short term memory unit</h3><p>也没有仔细描述 LSTM 的细节，主要思想是 not to forget &amp; learn to forget。</p>
<p>LSTM 使用<strong>门（gates）</strong> 来控制信息流：</p>
<ul>
<li><strong>遗忘</strong> 来减少无关信息</li>
<li><strong>储存</strong> 相关信息从当前输入</li>
<li>选择性 <strong>更新</strong> 单元状态</li>
<li><strong>输出</strong>一个过滤版本状态</li>
</ul>
<h2 id="RNNs-CNNs"><a href="#RNNs-CNNs" class="headerlink" title="RNNs + CNNs"></a>RNNs + CNNs</h2><p>核心思想就是 CNNs 来学习出2D图像的特征提取器，压缩后的特征喂入 RNNs 分析序列。</p>
<p><img data-src="Screenshot from 2021-08-11 18-47-07.png" alt></p>
<h2 id="CV中的应用"><a href="#CV中的应用" class="headerlink" title="CV中的应用"></a>CV中的应用</h2><h3 id="视频分类"><a href="#视频分类" class="headerlink" title="视频分类"></a>视频分类</h3><p><img data-src="Screenshot from 2021-08-11 21-44-11.png" alt></p>
<p>有多种整合特征的方法。</p>
<h3 id="图像标题-imgae-captioning"><a href="#图像标题-imgae-captioning" class="headerlink" title="图像标题 imgae captioning"></a>图像标题 imgae captioning</h3><p>目标是生成准确捕捉图像内容的句子。</p>
<ul>
<li>生成模型</li>
</ul>
<p><img data-src="Screenshot from 2021-08-11 21-44-40.png" alt></p>
<h3 id="动作预测"><a href="#动作预测" class="headerlink" title="动作预测"></a>动作预测</h3><p><img data-src="Screenshot from 2021-08-11 21-53-42.png" alt></p>
<h1 id="Lecture-4"><a href="#Lecture-4" class="headerlink" title="Lecture 4"></a>Lecture 4</h1><p>本章主要介绍<strong>图像生成模型（Generative Image Modeling）</strong>。</p>
<h2 id="生成模型-Generative-modeling"><a href="#生成模型-Generative-modeling" class="headerlink" title="生成模型 Generative modeling"></a>生成模型 Generative modeling</h2><p>在第二节课中我们学到了图像分类，我们思考能不能将这个一对一映射反过来，根据标签去生成图像。</p>
<p><strong>图像合成（image synthesis）</strong></p>
<p><strong>图像翻译（image translation）</strong></p>
<p>生成模型的目标就是：学习出一个代表并捕捉了一些控制相关数据的潜在概率分布深度学习模型。</p>
<p>在计算机视觉中：</p>
<ul>
<li>学习拟合一个数据流形（data manifold）。</li>
<li>从习得的分布产生新的样例。</li>
</ul>
<p>挑战：</p>
<ul>
<li>高维。</li>
<li>潜在分布的复杂性。</li>
</ul>
<p>所以我们用深度学习来建模因为它的数据驱动的拟合函数方式。</p>
<h2 id="模型架构-Architecture"><a href="#模型架构-Architecture" class="headerlink" title="模型架构 Architecture"></a>模型架构 Architecture</h2><p>分布转化（distribution transformers）</p>
<p><img data-src="Screenshot from 2021-08-12 18-36-59.png" alt></p>
<p>我们需要神经网络上非常聪明的方法，来实现端对端的动态模型。</p>
<p>我们从一个可以使低维的概率分布，习得一个神经网络模型，从原来的分布到目标数据的分布。在这个过程中我们可以生产落在目标数据中的合成图像，这就是生成模型的关键。</p>
<p><img data-src="Screenshot from 2021-08-12 18-43-11.png" alt></p>
<p>关于建立这种模型可以分为两类，一种是明确估计概率函数的，另一种是不明确地学习密度函数的，从隐式的学习到实现生成。我们将会主要讨论 <strong>VAE</strong> 和 <strong>GAN</strong>。</p>
<p><img data-src="Screenshot from 2021-08-12 18-46-16.png" alt></p>
<h3 id="变分自编码-Variational-autoencoders"><a href="#变分自编码-Variational-autoencoders" class="headerlink" title="变分自编码 Variational autoencoders"></a>变分自编码 Variational autoencoders</h3><p>我们是想要从真实的数据空间，模拟生成合成样本或真正数据空间的重构，能够概括数据的形态。</p>
<p><strong>VAE</strong> 先将真实数据编码成低维特征空间或隐空间（latent space），由一组隐变量编码而成，我们可以去学习这个特征空间而不需要任何监督。</p>
<p><strong>VAE</strong> 将问题分成两个部分，一个就是对于原来的高维的真实数据编码（encode）成低维的数据空间，从而来限制提取出有用的特征。基于此，我们要从这个低维的数据空间解码（decode）或习得一个重构的数据空间。</p>
<p>所以对于图像数据，编码和解码网络就可以由卷积结构来构建。</p>
<p><img data-src="Screenshot from 2021-08-12 18-57-17.png" alt></p>
<p>我们算出隐空间 $z$ 的概率分布，根据输入数据 $x$，它将被编码网络的权重矩阵参数化，我们用 $\phi$ 来表示。然后反向地我们可以做出解码网络，根据 $z$ 计算出 $x$，解码参数为 $\theta$。把两部分连起来。</p>
<p>训练的方法就是将两部分整合进代价函数，参数为 $\phi$ 和 $\theta$，一项是正则化项。</p>
<p>为了训练这个模型，我们需要预先给定一些概率分布，这取决于我们希望这些隐变量采取怎样的形态。我们会在每一个隐变量 $z$ 上加上一个高斯分布（Gaussian prior）。</p>
<p>为了不是绝对重建，我们还得想办法从 $z$ 中取样，我们要参数化 $z$，用均值和标准差根据之前的正太高斯分布，把标准差按照一个噪声因子进行缩放，该因子包含了随机性、概率元素和变分元素，使我们可以用 <strong>VAE</strong> 生成合成的数据实例。</p>
<p>更具体地说，当我们施加高斯分布，并将隐变量参数化为平均值和标准差并缩放一个噪声系数，这样子我们就可以算出代价函数，根据重建项和一个正则化项，其中正则化项可以得到我们根据隐变量学习解码出的结果和根据高斯分布指定的隐变量分布形态之间的差距。</p>
<script type="math/tex; mode=display">
\mathcal{L}(\phi, \theta, x) = \| x - \hat{x} \|^2 + D(q_\phi(z|x) | p(z))</script><script type="math/tex; mode=display">
z = \mu + \sigma \odot \varepsilon</script><p>这是一个很好的可跟踪的方法，来检查网络并理解网络到底学了什么信息。我们要做的，就是<strong>潜变量摄动（latent variable perturbation）</strong>就是改变 $z$ 然后观察输出结果，得到前变量的属性，是否相互独立等。</p>
<p>总结：</p>
<ol>
<li>压缩数据的表示到低维空间。</li>
<li>重建允许了无监督学习。</li>
<li>重参数化技巧来训练。</li>
<li>通过摄动解释潜变量。</li>
<li>生成新实例。</li>
</ol>
<h3 id="生成对抗网络-Generative-adversarial-networks"><a href="#生成对抗网络-Generative-adversarial-networks" class="headerlink" title="生成对抗网络 Generative adversarial networks"></a>生成对抗网络 Generative adversarial networks</h3><p>核心理念是：<br>我们先从完全高斯噪声作为初始的 $z$ 分布开始，去生成符合的图像数据。制定一条端到端的流水线，它们由两个网络组成，一个是<strong>生成器（generator）</strong>，一个是<strong>识别器（discriminator）</strong>，它们在一个敌对博弈中相互竞争。我们从一个高斯噪声输入开始，生成器生成合成图像，然后被喂到分类网络中，这个网路被训练去思考这个实例是真实的或人造的。整个过程通过迭代，训练识别器和生成器通过相互竞争实现优化。全局最优就是识别器识别不出来了。</p>
<p><img data-src="Screenshot from 2021-08-13 00-33-13.png" alt></p>
<p>从高斯噪声开始，生成器生成的合成图片被输入识别器，同时真实的图片也会被输入识别器，识别器被训练来判断输入的图像是真的还是假的，对于我们可以优化的损失这些损失都是交叉熵损失：</p>
<script type="math/tex; mode=display">
\arg \max\limits_{D} \mathbb{E}_{\mathbf{z}, \mathbf{x}}[\log D(G(\mathbf{z}))+\log (1-D(\mathbf{x}))</script><p>前一项描述的是识别器对于生成器的图像的真假判断的概率，后一项刻画的是一个给定的图片是否与一个真标签相关联。我们利用交叉熵损失和最大化分离目标来训练识别器。</p>
<script type="math/tex; mode=display">
\arg \min\limits_{G} \mathbb{E}_{\mathbf{z}, \mathbf{x}}[\log D(G(\mathbf{z}))+\log (1-D(\mathbf{x}))]</script><p>相反地，我们可以用同样的交叉熵，用最小化代替最大化，也就是说让它与识别器为敌。</p>
<p>我们可以通过内插，在高斯噪声中的两个数据点，实现由 GAN 生成的合成图像的扰动，观察其在学习到的数据空间流形中的距离，来调试 GAN。优点是保证了映射的条理性。</p>
<p>对传统 GAN 的优化：</p>
<ul>
<li><p>StyleGAN, Karras+ 2019</p>
</li>
<li><p>我看不懂（</p>
</li>
</ul>
<p><img data-src="Screenshot from 2021-08-13 00-54-23.png" alt></p>
<h2 id="Recent-Advances"><a href="#Recent-Advances" class="headerlink" title="Recent Advances"></a>Recent Advances</h2><ul>
<li>Paired translation<ul>
<li>pix2pix</li>
</ul>
</li>
<li>CycleGAN: domain transfer</li>
</ul>
<p><img data-src="Screenshot from 2021-08-13 01-05-39.png" alt></p>
<h1 id="Lecture-5"><a href="#Lecture-5" class="headerlink" title="Lecture 5"></a>Lecture 5</h1><p>本章主要讲的是神经绘图和渲染（Neural Graphics and Rendering）。</p>
<p><strong>神经渲染（Neural Rendering）</strong>：是将计算机图形学的物理知识和生成模型机器学习结合起来，通过融合不同的渲染到网络训练中。</p>
<ul>
<li>生成网络负责合成原始像素。</li>
<li>可解释的参数来控制输出。</li>
<li>光影，相机，位置，动作，结构。。</li>
</ul>
<h2 id="经典计算机图形学-Classical-computer-graphics"><a href="#经典计算机图形学-Classical-computer-graphics" class="headerlink" title="经典计算机图形学 Classical computer graphics"></a>经典计算机图形学 Classical computer graphics</h2><p><img data-src="Screenshot from 2021-08-13 16-28-23.png" alt></p>
<p>两个方向，一个forward从模型到图片的渲染，一个backward从图片到模型的重建。</p>
<p>forward 过程，对于一个模型，它可能有很多属性，光源、材料、相机等等，通过 CG 计算机绘图的函数，输出一张合理的从照相机视角看到的一个图片。</p>
<h3 id="渲染方程"><a href="#渲染方程" class="headerlink" title="渲染方程"></a>渲染方程</h3><p>有一个渲染方程不过我看不懂：</p>
<script type="math/tex; mode=display">
L_{\mathrm{o}}\left(\mathbf{p}, \omega_{\mathrm{o}}\right)=L_{\mathrm{e}}\left(\mathbf{p}, \omega_{\mathrm{o}}\right)+\int_{\Omega} L_{\mathrm{i}}\left(\mathbf{p}, \omega_{\mathrm{i}}\right) f_{\mathrm{r}}\left(\mathbf{p}, \omega_{\mathrm{i}}, \omega_{\mathrm{o}}\right)\left(\omega_{\mathrm{i}} \cdot \mathbf{n}\right) \mathrm{d} \omega_{\mathrm{i}}</script><p>简单理解就是等号左边是 Outgoing radiance，该点的出射光；右边第一项是该点的光源；积分项是入射光和一个 BRDF函数。总之这个方程非常强大，可以递归。</p>
<h3 id="BRDF"><a href="#BRDF" class="headerlink" title="BRDF"></a>BRDF</h3><p>它可以捕捉材料的变化，就可以处理反射特性，进行材料光泽的渲染。</p>
<h2 id="神经图像表示-Neural-scene-representations"><a href="#神经图像表示-Neural-scene-representations" class="headerlink" title="神经图像表示 Neural scene representations"></a>神经图像表示 Neural scene representations</h2><h3 id="语义渲染-Semantic-rendering"><a href="#语义渲染-Semantic-rendering" class="headerlink" title="语义渲染 Semantic rendering"></a>语义渲染 Semantic rendering</h3><p>思想是通过一种语义合理的方式，控制并合成场景的外观。</p>
<p><img data-src="Screenshot from 2021-08-13 17-01-47.png" alt></p>
<h3 id="pix2pix-生成模型"><a href="#pix2pix-生成模型" class="headerlink" title="pix2pix 生成模型"></a>pix2pix 生成模型</h3><p>通过不仅将生成器生成的合成图片喂入识别器，还喂入原始的输入，来让识别器判断真假。</p>
<p>这个模型工作地得很好，但它的问题是不能在高质量图片上表现得不错。</p>
<p>所以我们要增强模型的鲁棒性，渲染更高质量的图片。</p>
<ul>
<li>动态增长模型（progressive growing of generative models）</li>
<li>实时渲染</li>
</ul>
<h3 id="渲染风格的控制"><a href="#渲染风格的控制" class="headerlink" title="渲染风格的控制"></a>渲染风格的控制</h3><p>我们将输入编码到潜空间，单这个潜空间遵循更高维参数的分布，这些高阶参数可以用 <strong>KL分离损失</strong>来约束。</p>
<p>也就是说他们是概率的，我们可以在这个概率分布中取样，决定了渲染的风格，即使他们都是语义正确的。</p>
<p><img data-src="Screenshot from 2021-08-14 01-11-02.png" alt></p>
<h2 id="Implicit-neural-rendering"><a href="#Implicit-neural-rendering" class="headerlink" title="Implicit neural rendering"></a>Implicit neural rendering</h2><p>对于反向重建，是一个端对端的神经渲染训练。</p>
<p><img data-src="Screenshot from 2021-08-14 01-26-54.png" alt></p>
<h3 id="基于图像的复制-Image-based-reprojection"><a href="#基于图像的复制-Image-based-reprojection" class="headerlink" title="基于图像的复制 Image-based reprojection"></a>基于图像的复制 Image-based reprojection</h3><p>场景表示：RGB值+深度</p>
<h3 id="Point-based-rendering"><a href="#Point-based-rendering" class="headerlink" title="Point-based rendering"></a>Point-based rendering</h3><p><strong>神经隐式渲染（Neural implicit rendering）</strong>。</p>
<p><img data-src="Screenshot from 2021-08-14 01-41-48.png" alt></p>
<ul>
<li>如何得到数据？</li>
<li>什么是神经渲染的模型？</li>
</ul>
<h2 id="结合起来-1"><a href="#结合起来-1" class="headerlink" title="结合起来"></a>结合起来</h2><p><img data-src="Screenshot from 2021-08-14 01-57-13.png" alt></p>
<p>总之这节课所讲的内容，太过于前沿，模型太过于复杂，我基本上属于听不懂的状态，很难跟上，只能放一些slides的思路图。希望以后能懂orz。</p>
<h1 id="Lecture-6"><a href="#Lecture-6" class="headerlink" title="Lecture 6"></a>Lecture 6</h1><p>这节课主要讲神经视觉应用。</p>
<h2 id="目标检测-Object-detection"><a href="#目标检测-Object-detection" class="headerlink" title="目标检测 Object detection"></a>目标检测 Object detection</h2><p>目标检测指的是不仅输出图片的标签，还捕捉到这个目标在图像中的区块</p>
<h3 id="区域卷积神经网络-Regional-CNN"><a href="#区域卷积神经网络-Regional-CNN" class="headerlink" title="区域卷积神经网络 Regional CNN"></a>区域卷积神经网络 Regional CNN</h3><p>提取固定数量的区域，改变形状大小，通过 CNN 来分裂每个区域。</p>
<p>问题：</p>
<ul>
<li>训练缓慢因为每个图片要训练2000个区域。</li>
<li>不是实时的，每个测试图片有延迟。</li>
<li>区域的选择是经验性的，可能对于不同的训练集不能选到很好的区域。</li>
</ul>
<p>所以有了 Faster R-CNN，通过输入卷基层，只需要训练一次，数据驱动的区域选择。</p>
<p><img data-src="Screenshot from 2021-08-14 16-55-29.png" alt></p>
<h3 id="区域建议网络-Region-Proposal-Network"><a href="#区域建议网络-Region-Proposal-Network" class="headerlink" title="区域建议网络 Region Proposal Network"></a>区域建议网络 Region Proposal Network</h3><p>通过卷基层我们可以提取出一个 $m\times n$ 的特征映射（feature maps），图片中的每一个区域中心都可以被特征映射中的一个像素表示。</p>
<p>我们的 <strong>RPN</strong> 就是要找到 k 个目标块（anchor boxes）。</p>
<ul>
<li>辨别出块是是好的还是坏的。</li>
<li>在块上进行回归，来修改位置，宽和高。</li>
</ul>
<p>然后我们可以设计出代价函数：</p>
<script type="math/tex; mode=display">
L(\{p_i\}, \{ t_i\}) = \dfrac{1}{N_{cls}}\sum\limits_{i} L_{cls}(p_i, p_i^{*}) + \lambda \dfrac{1}{N_{reg}} p_i^{*} L_{reg} (t_i, t_i^{*})</script><p>第一部分是我们把区块中存在目标的真实概率分布与区块中存在目标的预测分布的差距最小化，第二部分是区块的真实位置和预测位置的回归损失。</p>
<h2 id="语义分割-Semantic-segmentation"><a href="#语义分割-Semantic-segmentation" class="headerlink" title="语义分割 Semantic segmentation"></a>语义分割 Semantic segmentation</h2><p>在高维上识别出每个像素是啥。</p>
<h3 id="全卷积神经网络-Fully-CNNs"><a href="#全卷积神经网络-Fully-CNNs" class="headerlink" title="全卷积神经网络 Fully CNNs"></a>全卷积神经网络 Fully CNNs</h3><p>在卷基层后不再是连到 Fully connected nn，而是通过 unpolling 等来up sampling，来增加视觉维度。</p>
<p><img data-src="Screenshot from 2021-08-14 22-45-32.png" alt></p>
<p>Unpooling 有几种：</p>
<ul>
<li>Nearest Neighbor</li>
<li>Bed of Nails</li>
</ul>
<p>代价函数我们用二元交叉熵：$- \sum\limits_{classes} y_{true} \log{(y_{pred})}$</p>
<p>当我们需要对边缘进行更精确的预测时，加大边缘的权重。</p>
<p>因为我们的编码器会把图像编码到更低维的特征空间，所以导致解码器很难捕捉到足够的特征。就有了<strong>跳跃连接（skip connect）</strong>，把编码部分的卷基层连接到解码部分对应的卷基层，避免了信息的损失。这个思想很类似残差网络。</p>
<p><img data-src="Screenshot from 2021-08-14 22-56-52.png" alt></p>
<h3 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h3><p><strong>U-net</strong> 就是一种通过大规模的跳跃连接来在学习过程中分享细节的架构。</p>
<p><img data-src="Screenshot from 2021-08-14 22-59-50.png" alt></p>
<h2 id="自监督学习-Self-supervised-learning"><a href="#自监督学习-Self-supervised-learning" class="headerlink" title="自监督学习 Self-supervised learning"></a>自监督学习 Self-supervised learning</h2><h3 id="深度估计-Depth-estimation"><a href="#深度估计-Depth-estimation" class="headerlink" title="深度估计 Depth estimation"></a>深度估计 Depth estimation</h3><p>在高维上识别出每个像素深度是多少。</p>
<p>因为深度是一个实数，所以是一个回归问题，我们一般用平方代价函数：$\text{MSE} = \dfrac{1}{N} \sum\limits_{i=1}^{N} (y_i - \hat{y}_i)^2$。</p>
<p>但是对于一个监督学习，我们需要知道真实的标签，也就是真实的深度，但这是很难办到的。我们需要一些其他的技术来得到深度。</p>
<h3 id="先验知识"><a href="#先验知识" class="headerlink" title="先验知识"></a>先验知识</h3><p>在没有标记数据的情况下，我们可以注入一些真实世界的先验知识。</p>
<ul>
<li><strong>视察（disparity）</strong>和深度成反比。</li>
<li>通过左右眼视察算出深度。</li>
</ul>
<p>我们就可以用这些知识来<strong>自监督</strong>。</p>
<h3 id="自监督"><a href="#自监督" class="headerlink" title="自监督"></a>自监督</h3><p><img data-src="Screenshot from 2021-08-14 23-32-38.png" alt></p>
<p>这是一个自监督单眼深度模型，我们用一个左眼摄像头作为输入，这很好得到。</p>
<p>我们要训练一个模型来预测我们希望看到的视察图，但这是没有办法被优化的，单纯只由网络预测。</p>
<p>但是模型和图的关系才是最重要的，我们通过把输入的左眼图和预测的视察图结合起来，就可以估算出右眼图。然后我们就可以用左右眼摄像机来训练这个网络，到达自我优化。</p>
<p>在测试的时候，我们只关心视察层，也就是输出的视察图是否准确。这太 amazing 了，这意味着我们只使用一张图片且没有任何标签，就可以估算深度。它运行地也非常快，因为我们只需要运行一半的网络。</p>
<p>没有传统的配对调整。</p>
<h1 id="Lecture-7"><a href="#Lecture-7" class="headerlink" title="Lecture 7"></a>Lecture 7</h1><p>本节主要讲了计算机视觉中的可解释性和不确定性。</p>
<h2 id="可解释性和可视化-Interpretability-and-Visualization"><a href="#可解释性和可视化-Interpretability-and-Visualization" class="headerlink" title="可解释性和可视化 Interpretability and Visualization"></a>可解释性和可视化 Interpretability and Visualization</h2><p>像素级的解释：</p>
<ul>
<li>哪些像素在图中是最重要的。</li>
<li>如果移除了这些像素预测将会发生变化。</li>
</ul>
<p>一个神经网络有百万个参数，如何判断它们控制哪些模式？</p>
<p>对于一个卷基层，我们可以把它可视化。我们要怎么把多个 feature maps 结合成一个可视化的特征？</p>
<h3 id="视觉反向传播VisualBackprop"><a href="#视觉反向传播VisualBackprop" class="headerlink" title="视觉反向传播VisualBackprop"></a>视觉反向传播VisualBackprop</h3><ul>
<li>平均（average）</li>
<li><p>扩大（upscale）</p>
</li>
<li><p>相乘（multiply）</p>
</li>
</ul>
<p><img data-src="Screenshot from 2021-08-15 18-45-33.png" alt></p>
<p>我们最终会得到一个可视化掩模（mask），和我们的输入图像在相同的维度上。</p>
<p>通过这种方式我们可以生成输入图像中不同像素特性的可视化。</p>
<h3 id="类激活映射-Class-Activation-Maps"><a href="#类激活映射-Class-Activation-Maps" class="headerlink" title="类激活映射 Class Activation Maps"></a>类激活映射 Class Activation Maps</h3><p>思考更新参数时 $W \leftarrow W - \eta \dfrac{\partial J(W, x, y)}{\partial W}$，一个参数的改变是如何增加我们的损失。</p>
<p><strong>CAM</strong> 的想法是让图像通过一系列的卷积层，从而产生一系列的特征向量。然后我们要做的是有一个<strong>全局平均池化操作（Global average pooling）</strong>，所有的特征映射被聚合，也就是说这个向量的每个分量就是所有特征向量的平均值。 然后我们可以考虑网络上每一个权重的聚合值，其最终输出，然后把这些权重乘以激活映射，生成最终的聚合激活映射。</p>
<p><img data-src="Screenshot from 2021-08-15 22-06-36.png" alt></p>
<p>它捕获了图像的各个区域，导致一系列的激活和特征映射，有助于网络的决策。</p>
<p>这个 GAP 会完全减少空间的信息，只保留了深度维度，导致了特征映射和类别之间的对应。</p>
<h2 id="不确定性估计-Uncertainty-estimation"><a href="#不确定性估计-Uncertainty-estimation" class="headerlink" title="不确定性估计 Uncertainty estimation"></a>不确定性估计 Uncertainty estimation</h2><p>不确定性的种类</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Known Knowns</td>
<td style="text-align:center">Known Unknows</td>
</tr>
<tr>
<td style="text-align:center">Unknown Known</td>
<td style="text-align:center">Unknown Unknows</td>
</tr>
</tbody>
</table>
</div>
<p>最主要两个方面的不确定性：</p>
<ul>
<li>任意的不确定性（aleatoric）<ul>
<li>数据的不确定性</li>
<li>描述了输入数据的信心</li>
<li>输入有噪声时高</li>
</ul>
</li>
<li>认知的不确定性（epistemic）<ul>
<li>模型的不确定性</li>
<li>描述了预测的信心（confidence）</li>
<li>当缺少训练数据时很高</li>
<li>通过增加数据来减少</li>
</ul>
</li>
</ul>
<p><strong>Aleatoric uncertainty</strong> 可以直接通过神经网络来学习。</p>
<p><strong>Epistemic uncertainty</strong> 很难用神经网络学习，我们不去训练确定性网络，而训练<strong>贝叶斯神经网络（Bayesian NN）</strong>。</p>
<h3 id="BNN"><a href="#BNN" class="headerlink" title="BNN"></a>BNN</h3><p>对网络中每一个权值建立概率分布模型，然后根据正态分布将这些权重参数化，我们要训练这个模型来预测一个平均值、一个不变值，它定义了网络上的概率分布。这意味着我们要学习网络权值的后验（posterior）概率分布，$P(W | X, Y) = \dfrac{P(Y|X,W) P(W)}{P(Y|X)}$。</p>
<p>但是这在分析上是不可解的，所以我们试图近似。</p>
<p><strong>dropout</strong>，每次前向传播中随机选择一些权重剔除，按照伯努利分布 $z_{w,t} \sim Bernoulli(p), \forall w  \in W$。</p>
<p><img data-src="Screenshot from 2021-08-15 23-13-44.png" alt></p>
<h1 id="Lecture-8"><a href="#Lecture-8" class="headerlink" title="Lecture 8"></a>Lecture 8</h1><p>本章最后一节，主要讨论计算机视觉的进展，回顾一下这门课所学。</p>
<p>主要是解决偏差的方法。</p>
<ul>
<li>Adversarial debiasing</li>
<li>Debiasing variational autoencoder</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/08/04/DL-AN/" rel="prev" title="吴恩达深度学习笔记">
      <i class="fa fa-chevron-left"></i> 吴恩达深度学习笔记
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/08/11/tensorflow/" rel="next" title="tensorflow">
      tensorflow <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-1"><span class="nav-number">1.</span> <span class="nav-text">lecture 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83%EF%BC%88Perceptron%EF%BC%89"><span class="nav-number">1.1.</span> <span class="nav-text">神经元（Perceptron）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B-Edges"><span class="nav-number">1.2.</span> <span class="nav-text">边缘检测 Edges</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF-Convolution"><span class="nav-number">1.3.</span> <span class="nav-text">卷积 Convolution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96-Pooling"><span class="nav-number">1.4.</span> <span class="nav-text">池化 Pooling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%93%E5%90%88%E8%B5%B7%E6%9D%A5"><span class="nav-number">1.5.</span> <span class="nav-text">结合起来</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-2"><span class="nav-number">2.</span> <span class="nav-text">Lecture 2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN-Architectures"><span class="nav-number">2.1.</span> <span class="nav-text">CNN Architectures</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5-Scaling-CNNs-residual-connections"><span class="nav-number">2.2.</span> <span class="nav-text">残差连接 Scaling CNNs: residual connections</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AE%8B%E5%B7%AE%E5%8C%BA%E5%9D%97-Residual-blocks"><span class="nav-number">2.3.</span> <span class="nav-text">残差区块 Residual blocks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86-Dataset"><span class="nav-number">2.4.</span> <span class="nav-text">数据集 Dataset</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-3"><span class="nav-number">3.</span> <span class="nav-text">Lecture 3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Recurrent-neural-network"><span class="nav-number">3.1.</span> <span class="nav-text">循环神经网络 Recurrent neural network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN"><span class="nav-number">3.1.1.</span> <span class="nav-text">RNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E5%8D%95%E5%85%83-Long-short-term-memory-unit"><span class="nav-number">3.1.2.</span> <span class="nav-text">长短时记忆单元 Long short term memory unit</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNNs-CNNs"><span class="nav-number">3.2.</span> <span class="nav-text">RNNs + CNNs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CV%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="nav-number">3.3.</span> <span class="nav-text">CV中的应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%86%E9%A2%91%E5%88%86%E7%B1%BB"><span class="nav-number">3.3.1.</span> <span class="nav-text">视频分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E6%A0%87%E9%A2%98-imgae-captioning"><span class="nav-number">3.3.2.</span> <span class="nav-text">图像标题 imgae captioning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E4%BD%9C%E9%A2%84%E6%B5%8B"><span class="nav-number">3.3.3.</span> <span class="nav-text">动作预测</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-4"><span class="nav-number">4.</span> <span class="nav-text">Lecture 4</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B-Generative-modeling"><span class="nav-number">4.1.</span> <span class="nav-text">生成模型 Generative modeling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84-Architecture"><span class="nav-number">4.2.</span> <span class="nav-text">模型架构 Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81-Variational-autoencoders"><span class="nav-number">4.2.1.</span> <span class="nav-text">变分自编码 Variational autoencoders</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Generative-adversarial-networks"><span class="nav-number">4.2.2.</span> <span class="nav-text">生成对抗网络 Generative adversarial networks</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Recent-Advances"><span class="nav-number">4.3.</span> <span class="nav-text">Recent Advances</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-5"><span class="nav-number">5.</span> <span class="nav-text">Lecture 5</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%8F%E5%85%B8%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6-Classical-computer-graphics"><span class="nav-number">5.1.</span> <span class="nav-text">经典计算机图形学 Classical computer graphics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B8%B2%E6%9F%93%E6%96%B9%E7%A8%8B"><span class="nav-number">5.1.1.</span> <span class="nav-text">渲染方程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BRDF"><span class="nav-number">5.1.2.</span> <span class="nav-text">BRDF</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E5%9B%BE%E5%83%8F%E8%A1%A8%E7%A4%BA-Neural-scene-representations"><span class="nav-number">5.2.</span> <span class="nav-text">神经图像表示 Neural scene representations</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%AD%E4%B9%89%E6%B8%B2%E6%9F%93-Semantic-rendering"><span class="nav-number">5.2.1.</span> <span class="nav-text">语义渲染 Semantic rendering</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pix2pix-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.2.2.</span> <span class="nav-text">pix2pix 生成模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B8%B2%E6%9F%93%E9%A3%8E%E6%A0%BC%E7%9A%84%E6%8E%A7%E5%88%B6"><span class="nav-number">5.2.3.</span> <span class="nav-text">渲染风格的控制</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Implicit-neural-rendering"><span class="nav-number">5.3.</span> <span class="nav-text">Implicit neural rendering</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%83%8F%E7%9A%84%E5%A4%8D%E5%88%B6-Image-based-reprojection"><span class="nav-number">5.3.1.</span> <span class="nav-text">基于图像的复制 Image-based reprojection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Point-based-rendering"><span class="nav-number">5.3.2.</span> <span class="nav-text">Point-based rendering</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%93%E5%90%88%E8%B5%B7%E6%9D%A5-1"><span class="nav-number">5.4.</span> <span class="nav-text">结合起来</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-6"><span class="nav-number">6.</span> <span class="nav-text">Lecture 6</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-Object-detection"><span class="nav-number">6.1.</span> <span class="nav-text">目标检测 Object detection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8C%BA%E5%9F%9F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Regional-CNN"><span class="nav-number">6.1.1.</span> <span class="nav-text">区域卷积神经网络 Regional CNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8C%BA%E5%9F%9F%E5%BB%BA%E8%AE%AE%E7%BD%91%E7%BB%9C-Region-Proposal-Network"><span class="nav-number">6.1.2.</span> <span class="nav-text">区域建议网络 Region Proposal Network</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2-Semantic-segmentation"><span class="nav-number">6.2.</span> <span class="nav-text">语义分割 Semantic segmentation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Fully-CNNs"><span class="nav-number">6.2.1.</span> <span class="nav-text">全卷积神经网络 Fully CNNs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#U-Net"><span class="nav-number">6.2.2.</span> <span class="nav-text">U-Net</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-Self-supervised-learning"><span class="nav-number">6.3.</span> <span class="nav-text">自监督学习 Self-supervised learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1-Depth-estimation"><span class="nav-number">6.3.1.</span> <span class="nav-text">深度估计 Depth estimation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%88%E9%AA%8C%E7%9F%A5%E8%AF%86"><span class="nav-number">6.3.2.</span> <span class="nav-text">先验知识</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E7%9B%91%E7%9D%A3"><span class="nav-number">6.3.3.</span> <span class="nav-text">自监督</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-7"><span class="nav-number">7.</span> <span class="nav-text">Lecture 7</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E5%92%8C%E5%8F%AF%E8%A7%86%E5%8C%96-Interpretability-and-Visualization"><span class="nav-number">7.1.</span> <span class="nav-text">可解释性和可视化 Interpretability and Visualization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%86%E8%A7%89%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADVisualBackprop"><span class="nav-number">7.1.1.</span> <span class="nav-text">视觉反向传播VisualBackprop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B1%BB%E6%BF%80%E6%B4%BB%E6%98%A0%E5%B0%84-Class-Activation-Maps"><span class="nav-number">7.1.2.</span> <span class="nav-text">类激活映射 Class Activation Maps</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E4%BC%B0%E8%AE%A1-Uncertainty-estimation"><span class="nav-number">7.2.</span> <span class="nav-text">不确定性估计 Uncertainty estimation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BNN"><span class="nav-number">7.2.1.</span> <span class="nav-text">BNN</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lecture-8"><span class="nav-number">8.</span> <span class="nav-text">Lecture 8</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hui hui"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Hui hui</p>
  <div class="site-description" itemprop="description">Be happy forever!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">36</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/songtianhui" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;songtianhui" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:191098194@smail.nju.edu.cn" title="E-Mail → mailto:191098194@smail.nju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hui hui</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>



        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  

  <script async src="/js/cursor/fireworks.js"></script>


 
<script>
  var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*)" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });
</script>


 
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20210126,"YYYYMMDD"));
    ages = ages.replace(/years?/, "years");
    ages = ages.replace(/months?/, "moths");
    ages = ages.replace(/days?/, "days");
    ages = ages.replace(/hours?/, "hours");
    ages = ages.replace(/minutes?/, "minutes");
    ages = ages.replace(/seconds?/, "seconds");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `I have been here waiting for you for ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


 
<script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginrootpath":"live2dw/","pluginjspath":"lib/","pluginmodelpath":"assets/ relative)","scriptfrom":"local","tagmode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":180,"height":300},"mobile":{"show":true},"react":{"opacitydefault":0.7},"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
