<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-bounce.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"songtianhui.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="视频指路">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达深度学习笔记">
<meta property="og:url" content="https://songtianhui.github.io/2021/08/04/DL-AN/index.html">
<meta property="og:site_name" content="Songtianhui&#39;s Blog">
<meta property="og:description" content="视频指路">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/04/DL-AN/L1_week3_6.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/04/DL-AN/network.png">
<meta property="article:published_time" content="2021-08-04T14:51:29.000Z">
<meta property="article:modified_time" content="2021-08-25T15:03:09.804Z">
<meta property="article:author" content="Hui hui">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://songtianhui.github.io/2021/08/04/DL-AN/L1_week3_6.png">

<link rel="canonical" href="https://songtianhui.github.io/2021/08/04/DL-AN/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>吴恩达深度学习笔记 | Songtianhui's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="/custom_css_source.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Songtianhui's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://songtianhui.github.io/2021/08/04/DL-AN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Hui hui">
      <meta itemprop="description" content="Be happy forever!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Songtianhui's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          吴恩达深度学习笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-08-04 22:51:29" itemprop="dateCreated datePublished" datetime="2021-08-04T22:51:29+08:00">2021-08-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-08-25 23:03:09" itemprop="dateModified" datetime="2021-08-25T23:03:09+08:00">2021-08-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1FT4y1E74V?from=search&amp;seid=7469215768123017337">视频指路</a></p>
<a id="more"></a>
<hr>
<h1 id="Lesson-1-1"><a href="#Lesson-1-1" class="headerlink" title="Lesson 1.1"></a>Lesson 1.1</h1><p>本章是对深度学习神经网络的一个介绍，课程的概览。</p>
<h1 id="Lesson-1-2"><a href="#Lesson-1-2" class="headerlink" title="Lesson 1.2"></a>Lesson 1.2</h1><p>本章主要讲了一下 logisitic 回归，梯度下降，在ML中已经学习了。</p>
<p>然后介绍了 <code>python</code> 中的一些知识，向量化、广播、<code>numpy</code>、jupyter notebook等，都可以 <strong>rtfm</strong>，不在此赘述。</p>
<p>唯一有一点就是符号上的表示，andrew 喜欢用 $m$ 表示样本个数，$n$ 表示特征数。我习惯 mit 的课教的 $n$ 表示样本数，$d$ 表示特征维数。</p>
<h1 id="Lesson-1-3"><a href="#Lesson-1-3" class="headerlink" title="Lesson 1.3"></a>Lesson 1.3</h1><p>本章算是一个神经网络的引入，介绍一些基本概念，浅层神经网络（shallow neural network）。</p>
<p><img data-src="L1_week3_6.png" alt></p>
<p>每个神经单元就是这么个计算过程，有输入 $x$，权重 $w$，偏移 $b$，激活函数（activation function） $h$。</p>
<ul>
<li>$z = w^T x + b$</li>
<li>$a = h(z)$</li>
</ul>
<p>符号： $a_1^{<a href="1">1</a>} $ ，右上角中括号表示层数，右上角括号中表示第几个样本，右下角表示该层第几个神经元。</p>
<p>对于一个输入样本，避免一层内的 for 循环，向量化计算：</p>
<script type="math/tex; mode=display">
z^{[i]} = W^{[i]} a^{[i-1]} + b^{[i]}\\
a^{[i]} = h(z^{[i]})</script><p>$W^{[i]}$ 是第 $i$ 层每个神经元的权重排成的矩阵。</p>
<p>举例图示：</p>
<script type="math/tex; mode=display">
\left[
        \begin{array}{c}
        z^{[1]}_{1}\\
        z^{[1]}_{2}\\
        z^{[1]}_{3}\\
        z^{[1]}_{4}\\
        \end{array}
        \right]
         =
    \overbrace{
    \left[
        \begin{array}{c}
        ...W^{[1]T}_{1}...\\
        ...W^{[1]T}_{2}...\\
        ...W^{[1]T}_{3}...\\
        ...W^{[1]T}_{4}...
        \end{array}
        \right]
        }^{W^{[1]}}
        *
    \overbrace{
    \left[
        \begin{array}{c}
        x_1\\
        x_2\\
        x_3\\
        \end{array}
        \right]
        }^{input}
        +
    \overbrace{
    \left[
        \begin{array}{c}
        b^{[1]}_1\\
        b^{[1]}_2\\
        b^{[1]}_3\\
        b^{[1]}_4\\
        \end{array}
        \right]
        }^{b^{[1]}}</script><p>对于所有样本，避免 for 遍历样本，向量化计算：</p>
<script type="math/tex; mode=display">
Z^{[i]} = W^{[i]}A^{[i-1]} + b^{[i]}\\
A^{[i]} = h(Z^{[i]})</script><p>其中 $Z^{[i]},A^{[i]}$ 是第 $i$ 层所有样本输出排成的矩阵。</p>
<p>激活函数：<strong>sigmoid</strong>（主要在二分类）, <strong>tanh</strong>（比 sigmoid 常用），<strong>ReLU</strong>（在神经网络中很常用），Leaky ReLU。</p>
<p>前两个有梯度消失的风险。</p>
<p>这里提到一个很重要的问题就是为什么要使用非线性函数而不是直接 $a = z$。因为全用线性激活函数（identity）会使神经网络退化成一个单层模型。</p>
<h2 id="反向传播-Back-Propagation"><a href="#反向传播-Back-Propagation" class="headerlink" title="反向传播 Back Propagation"></a>反向传播 Back Propagation</h2><p>也就是神经网络的梯度下降（Gradient Descent）。比较重要，理解一下推倒，本质上是函数求导的链式法则。</p>
<p>代价函数：</p>
<script type="math/tex; mode=display">
J(W, b) = \dfrac{1}{m}\sum\limits_{i= 1}^{m}L(\hat{y}, y)</script><p>当参数初始化成某些值后，每次梯度下降都会循环计算以下预测值：$\hat{y}^{(i)},(i=1,2,…,m)$。</p>
<p>有 $dW^{[i]} = \dfrac{\partial J}{\partial W^{[i]}}$, $d b^{[i]} = \dfrac{\partial J}{\partial b^{[i]}}$。</p>
<p>在梯度下降时每一次更新：$W^{[i]}\implies{W^{[i]} - \eta dW^{[i]}},b^{[i]}\implies{b^{[i]} -\eta db^{[i]}}$, $\eta$ 为步长。</p>
<p>反向传播时，就是一个链式求导：</p>
<script type="math/tex; mode=display">
\underbrace{
    \left.
    \begin{array}{l}
    x \\
    w \\
    b 
    \end{array}
    \right\}
    }_{dw=dz \cdot x, db =dz}
    \impliedby \underbrace{z=w^Tx+b}_{dz=da\cdot g^{'}(z),
    g(z)=\sigma(z),
    \frac{dL}{dz}} = \frac{dL}{da} \cdot \frac{da}{dz},
    \frac{d}{ dz} g(z)=g^{'}(z)
    \impliedby \underbrace{a = \sigma(z) 
    \impliedby L(a,y)}_{da=\frac{d}{da}L\left(a,y \right)=(-y\log{\alpha} - (1 - y)\log(1 - a))^{'}=-\frac{y}{a} + \frac{1 - y}{1 - a} }</script><p>所以有：</p>
<p>$dz^{[L]} = A^{[L]} - Y$</p>
<p>$dW^{[i]} = \dfrac{1}{m} dZ^{[i]}A^{[i-1]T}$</p>
<p>$db^{[i]} = \dfrac{1}{m}$ <code>np.sum(dZ^&#123;[i]&#125;, axis=1)​</code></p>
<script type="math/tex; mode=display">
dz^{[i]} = \underbrace{W^{[i + 1]T} dz^{[i+1]}}_{(n^{[i]},m)}\quad \times  \underbrace{g^{[i]'}}_{activation \; function \; of \; hidden \; layer}\times  \quad\underbrace{(z^{[i]})}_{(n^{[1]},m)}</script><ul>
<li>随机初始化，不要初始化成相同的参数。</li>
</ul>
<h1 id="Lesson-1-4"><a href="#Lesson-1-4" class="headerlink" title="Lesson 1.4"></a>Lesson 1.4</h1><p>本章介绍深层神经网络，主要就是把前一章讲的只有两层的网络更推广一下，而我们在上一章其实已经推广过了。</p>
<h2 id="为什么使用深层表示？"><a href="#为什么使用深层表示？" class="headerlink" title="为什么使用深层表示？"></a>为什么使用深层表示？</h2><p>深度神经网络的这许多隐藏层中，较早的前几层能学习一些低层次的简单特征，等到后几层，就能把简单的特征结合起来，去探测更加复杂的东西。</p>
<p>深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。</p>
<blockquote>
<p>说实话，我认为“深度学习”这个名字挺唬人的，这些概念以前都统称为有很多隐藏层的神经网络，但是深度学习听起来多高大上，太深奥了，对么？这个词流传出去以后，这是神经网络的重新包装或是多隐藏层神经网络的重新包装，激发了大众的想象力。    ——Andrew</p>
</blockquote>
<h2 id="搭建神经网络块"><a href="#搭建神经网络块" class="headerlink" title="搭建神经网络块"></a>搭建神经网络块</h2><p>其实就是对于每一层，权重矩阵，偏移值，激活函数，在前向传播的时候缓存（cache）好 $z,a$ 等值，用反向传播时计算 $dW,db$ 等。</p>
<p>就放一张老师的板书吧（</p>
<p><img data-src="network.png" alt="building blocks"></p>
<h2 id="参数和超参数"><a href="#参数和超参数" class="headerlink" title="参数和超参数"></a>参数和超参数</h2><p>算法中的<strong>learning rate</strong> $a$（学习率）、<strong>iterations</strong>(梯度下降法循环的数量)、$L$（隐藏层数目）、$n^{[l]}$（隐藏层单元数目）、<strong>choice of activation function</strong>（激活函数的选择）都需要你来设置，这些数字实际上控制了最后的参数$W$和$b$的值，所以它们被称作超参数（Hyperparameter）。</p>
<p>如何寻找超参数：走<strong>Idea—Code—Experiment—Idea</strong>这个循环，尝试各种不同的参数，实现模型并观察是否成功，然后再迭代。</p>
<blockquote>
<p>应用深度学习领域，一个很大程度基于经验的过程，凭经验的过程通俗来说，就是试直到你找到合适的数值。</p>
<p>如果你所解决的问题需要很多年时间，只要经常试试不同的超参数，勤于检验结果，看看有没有更好的超参数数值，相信你慢慢会得到设定超参数的直觉，知道你的问题最好用什么数值。</p>
</blockquote>
<hr>
<h1 id="Lesson-2-1"><a href="#Lesson-2-1" class="headerlink" title="Lesson 2.1"></a>Lesson 2.1</h1><p>本章主要讲改善神经网络，超参数调试、正则化等内容。</p>
<ul>
<li>当我们有百万量级以上的数据，可以拿 99% 以上的数据来进行训练，几万条用来交叉验证（dev）和测试就可以了。</li>
</ul>
<h2 id="方差-偏差"><a href="#方差-偏差" class="headerlink" title="方差/偏差"></a>方差/偏差</h2><ul>
<li>高偏差（<strong>high bias</strong>），欠拟合（<strong>underfitting</strong>）。</li>
<li>高方差（<strong>high variance</strong>），过拟合（<strong>overfitting</strong>）。</li>
</ul>
<p>通过训练集和验证集误差判断：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Training set error</th>
<th style="text-align:center">Dev set error</th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1%</td>
<td style="text-align:center">15%</td>
<td style="text-align:center">high variance</td>
</tr>
<tr>
<td style="text-align:center">15%</td>
<td style="text-align:center">16%</td>
<td style="text-align:center">high bias</td>
</tr>
<tr>
<td style="text-align:center">15%</td>
<td style="text-align:center">30%</td>
<td style="text-align:center">high variance &amp; bias</td>
</tr>
<tr>
<td style="text-align:center">0.5%</td>
<td style="text-align:center">1%</td>
<td style="text-align:center">low variance &amp; bias</td>
</tr>
</tbody>
</table>
</div>
<h2 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h2><blockquote>
<p>初始模型训练完成后，我首先要知道算法的偏差高不高，如果偏差较高，试着评估训练集或训练数据的性能。如果偏差的确很高，甚至无法拟合训练集，那么你要做的就是选择一个新的网络，比如含有更多隐藏层或者隐藏单元的网络，或者花费更多时间来训练网络，或者尝试更先进的优化算法，后面我们会讲到这部分内容。</p>
</blockquote>
<ul>
<li>只要正则适度，通常构建一个更大的网络便可以，在不影响方差的同时减少偏差，而采用更多数据通常可以在不过多影响偏差的同时减少方差。</li>
<li>训练网络，选择网络或者准备更多数据，现在我们有工具可以做到在减少偏差或方差的同时，不对另一方产生过多不良影响。</li>
</ul>
<h2 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h2><p>深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，正则化通常有助于避免过拟合或减少你的网络误差。</p>
<p>就是在代价函数里加一个正则化项，一般用 $\dfrac{\lambda}{2m}$乘以$w$范数的平方,其中$\left| w \right|_2^2$是$w$的欧几里德范数的平方，$L2$ 正则化。</p>
<p>神经网络中的正则项就是为$\dfrac{\lambda }{2m}\sum\limits_{l = 1}^{L}| W^{[l]}|^{2}$，我们称${||W^{\left[l\right]}||}^{2}$为范数平方，这个矩阵范数${||W^{\left[l\right]}||}^{2}$（即平方范数），被定义为矩阵中所有元素的平方求和。该矩阵范数被称作“弗罗贝尼乌斯范数”，用下标$F$标注。</p>
<p>带正则化的梯度下降中，对$W^{[l]}$的偏导数，把$W^{[l]}$替换为$W^{[l]}$减去学习率乘以$dW$。现在我们要做的就是给$dW$加上这一项$\dfrac {\lambda}{m}W^{[l]}$，然后计算这个更新项，使用新定义的$dW^{[l]}$，它的定义含有相关参数代价函数导数和，以及最后添加的额外正则项。</p>
<script type="math/tex; mode=display">
\begin{aligned}
W^{[l]} :&= W^{[l]}  - \alpha \times \left[(\text{from backpap}) + \dfrac{\lambda}{m}W^{[l]}\right]\\ &= (1 - \frac{\alpha \lambda}{m}) W^{[l]} - \alpha \times (\text{from backpap})
\end{aligned}</script><p><em>正则化预防过拟合的原因：极限思想，当lambda很大权重为0，退化成欠拟合，有个right fit 的中间态。</em></p>
<h2 id="dropout-正则化"><a href="#dropout-正则化" class="headerlink" title="dropout 正则化"></a>dropout 正则化</h2><p>就是随机消掉一些神经元。。。</p>
<ul>
<li><strong>inverted dropout</strong>（反向随机失活）<ul>
<li>首先要定义向量$d$，$d^{[3]}$表示网络第三层的<strong>dropout</strong>向量：<code>d3 = np.random.rand(a3.shape[0],a3.shape[1])</code> 。</li>
<li>然后看它是否小于某数，我们称之为<strong>keep-prob</strong>，<strong>keep-prob</strong>是一个具体数字，它表示保留某个隐藏单元的概率。</li>
<li>接下来要做的就是从第三层中获取$a^{[3]}$，$a^{[3]}$含有要计算的激活函数，$a^{[3]}$等于上面的$a^{[3]}$乘以 $d^{[3]}$，就是把 $d$ 中 0 对应位置的数归零。（$d$ 实际上是一个布尔数组）</li>
<li>最后，我们向外扩展$a^{[3]}$，用它除以<strong>keep-prob</strong>参数。</li>
</ul>
</li>
</ul>
<p>显然在测试阶段，我们不使用<strong>dropout</strong>。要同时在 <strong>fpp</strong> 和 <strong>bpp</strong> 中使用 dropout。</p>
<p><em>dropout 预防过拟合的原因，dropout 的功能类似于$L2$正则化，与$L2$正则化不同的是应用方式不同会带来一点点小变化，甚至更适用于不同的输入范围。</em></p>
<ul>
<li><p>如果你担心某些层比其它层更容易发生过拟合，可以把某些层的<strong>keep-prob</strong>值设置得比其它层更低，缺点是为了使用交叉验证，你要搜索更多的超级参数，另一种方案是在一些层上应用<strong>dropout</strong>，而有些层不用<strong>dropout</strong>，应用<strong>dropout</strong>的层只含有一个超级参数，就是<strong>keep-prob</strong>。</p>
</li>
<li><p>它在其它领域应用得比较少，主要存在于计算机视觉领域，因为我们通常没有足够的数据，所以一直存在过拟合。</p>
</li>
<li><strong>dropout</strong>一大缺点就是代价函数$J$不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。</li>
</ul>
<h2 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h2><ul>
<li>数据扩增</li>
<li><strong>early stopping</strong><ul>
<li>缺点是不能独立处理梯度下降和优化代价函数。</li>
</ul>
</li>
</ul>
<h2 id="归一化输入（Normalizing）"><a href="#归一化输入（Normalizing）" class="headerlink" title="归一化输入（Normalizing）"></a>归一化输入（Normalizing）</h2><p>训练神经网络，其中一个加速训练的方法就是归一化输入。归一化需要两个步骤：</p>
<ol>
<li>零均值<ul>
<li>$\mu = \frac{1}{m}\sum\limits_{i =1}^{m}x^{(i)}$，它是一个向量，$x$ 等于每个训练数据 $x$ 减去 $\mu$，意思是移动训练集，直到它完成零均值化。 </li>
</ul>
</li>
<li>归一化方差<ul>
<li>$ \sigma^{2}= \frac{1}{m}\sum\limits_{i =1}^{m}(x^{(i)})^{2} $，$\sigma^{2}$是一个向量，它的每个特征都有方差，把所有数据除以向量$\sigma^{2}$。</li>
</ul>
</li>
</ol>
<h2 id="梯度消失-梯度爆炸（Vanishing-Exploding-gradients）"><a href="#梯度消失-梯度爆炸（Vanishing-Exploding-gradients）" class="headerlink" title="梯度消失/梯度爆炸（Vanishing / Exploding gradients）"></a>梯度消失/梯度爆炸（Vanishing / Exploding gradients）</h2><p>神经网络层数多了，激活函数就会以指数级递增或递减。</p>
<h2 id="神经网络权重的初始化"><a href="#神经网络权重的初始化" class="headerlink" title="神经网络权重的初始化"></a>神经网络权重的初始化</h2><p>针对梯度消失/爆炸，有一个方案就是更谨慎地选择随机初始化参数。</p>
<p>$z = w_{1}x_{1} + w_{2}x_{2} + \ldots +w_{n}x_{n}$，为了预防$z$值过大或过小，希望每项值更小，最合理的方法就是设置$w_{i}=\frac{1}{n}$，$n$表示神经元的输入特征数量。</p>
<p>实际上，你要做的就是设置某层权重矩阵 <code>W[l] = np.random.randn(shape) * np.sqrt(1 / n[l-1])</code>，$n^{[l - 1]}$ 就是我喂给第$l$层神经单元的数量（即第$l-1$层神经元数量）。</p>
<p>如果你是用的是<strong>Relu</strong>激活函数，而不是$\frac{1}{n}$，方差设置为$\frac{2}{n}$，效果会更好。</p>
<p>对于<strong>tanh</strong>函数来说，用$\sqrt{\frac{1}{n^{[l-1]}}}$。</p>
<h2 id="梯度的数值逼近-Numerical-approximation-of-gradients"><a href="#梯度的数值逼近-Numerical-approximation-of-gradients" class="headerlink" title="梯度的数值逼近 Numerical approximation of gradients"></a>梯度的数值逼近 Numerical approximation of gradients</h2><p>就是导数定义，双边误差，即$\frac{f\left(\theta + \varepsilon \right) - f(\theta -\varepsilon)}{2\varepsilon}$。</p>
<p>先将所有的参数 $W, b$ 展开成一个大向量 $\theta$，在<strong>bpp</strong>中，算完梯度之后所有的梯度 $dW, db$ 就是 $d\theta$。</p>
<p>然后比较 $d\theta_{\text{approx}}\left[i \right] = \frac{J\left( \theta_{1},\theta_{2},\ldots\theta_{i} + \varepsilon,\ldots \right) - J\left( \theta_{1},\theta_{2},\ldots\theta_{i} - \varepsilon,\ldots \right)}{2\varepsilon}$ 和 $d\theta[i]$ 的值接不接近。</p>
<p>就计算它们的欧式距离再归一化，$\dfrac{||d\theta_{\text{approx}} -d\theta||_{2}}{||d\theta_{\text{approx}}||_2 + ||d\theta||_2}$。计算得到的值为$10^{-7}$或更小，这就很好；如果它的值在$10^{-5}$范围内，就要小心了，也许这个值没问题，但再次检查这个向量的所有项，确保没有一项误差过大，可能这里有<strong>bug</strong>。如果比$10^{-3}$大很多，就会很担心是否存在<strong>bug</strong>，这时应该仔细检查所有$\theta$项，看是否有一个具体的$i$值，使得$d\theta_{\text{approx}}\left[i \right]$与$ d\theta[i]$大不相同，并用它来追踪一些求导计算是否正确。</p>
<h2 id="梯度检验的注意事项"><a href="#梯度检验的注意事项" class="headerlink" title="梯度检验的注意事项"></a>梯度检验的注意事项</h2><ol>
<li>不要在训练中使用梯度检验，它只用于调试。</li>
<li>如果算法的梯度检验失败，要检查所有项，检查每一项，并试着找出<strong>bug</strong>。</li>
<li>在实施梯度检验时，如果使用正则化，请注意正则项。</li>
<li>梯度检验不能与<strong>dropout</strong>同时使用，因为每次迭代过程中，<strong>dropout</strong>会随机消除隐藏层单元的不同子集，难以计算<strong>dropout</strong>在梯度下降上的代价函数$J$。</li>
</ol>
<h1 id="Lesson-2-2"><a href="#Lesson-2-2" class="headerlink" title="Lesson 2.2"></a>Lesson 2.2</h1><p>本节课主要讲优化算法，也就是我们如何更新参数。</p>
<h2 id="Mini-batch-梯度下降"><a href="#Mini-batch-梯度下降" class="headerlink" title="Mini-batch 梯度下降"></a>Mini-batch 梯度下降</h2><p>你可以把训练集分割为小一点的子集训练，这些子集被取名为<strong>mini-batch</strong>，每个子集记作 $X^{\{i\}}$。就是把原来梯度下降时代入整个训练集改成代入一个mini-batch，然后多梯度下降几次。</p>
<p>使用<strong>mini-batch</strong>梯度下降法，如果作出成本函数在整个过程中的图，则并不是每次迭代都是下降的，特别是在每次迭代中，你要处理的是$X^{\{t\}}$和$Y^{\{ t\}}$。如果要作出成本函数$J$的图，你很可能会看到这样的结果，走向朝下，但有更多的噪声。</p>
<p>需要决定的变量之一是<strong>mini-batch</strong>的大小。首先，如果训练集较小，直接使用<strong>batch</strong>梯度下降法。样本数目较大的话，一般的<strong>mini-batch</strong>大小为64到512，考虑到电脑内存设置和使用的方式，如果<strong>mini-batch</strong>大小是2的$n$次方，代码会运行地快一些。</p>
<h2 id="指数加权平均数-Exponentially-weighted-averages"><a href="#指数加权平均数-Exponentially-weighted-averages" class="headerlink" title="指数加权平均数 Exponentially weighted averages"></a>指数加权平均数 Exponentially weighted averages</h2><p>递推式：</p>
<script type="math/tex; mode=display">
v_t = \beta v_{t - 1} + (1 - \beta)\theta_t</script><p>如果我们将其展开，这就是一个加权平均，是从 $0$ 到 $t$ 每个 $\theta_i$ 的平均，越远权重越小。</p>
<p>考虑 $\beta^{x} = \frac{1}{e}$，这个算的大约就是 $x$ 天的平均数。</p>
<p>指数加权平均数公式的好处之一在于，它占用极少内存，电脑内存中只占用一行数字而已，然后把最新数据代入公式，不断覆盖就可以了。</p>
<p>不过有可能会遇到 <strong>偏差修正（bias corrections）的问题</strong></p>
<p>因为我们取 $v_0 = 0$，所以会使得 $i$ 较小时 $v_i$ 所占权重都很小，估算不准确。</p>
<p>我们可以在估测初期，不用 $v_t$，而是 $\dfrac{v_t}{1 - \beta^t}$。</p>
<p>不过在机器学习中，在计算指数加权平均数的大部分时候，大家不在乎执行偏差修正。</p>
<h2 id="动量梯度下降法-Gradient-descent-with-Momentum"><a href="#动量梯度下降法-Gradient-descent-with-Momentum" class="headerlink" title="动量梯度下降法 Gradient descent with Momentum"></a>动量梯度下降法 Gradient descent with Momentum</h2><p>有一种算法叫做<strong>Momentum</strong>，或者叫做动量梯度下降法，运行速度几乎总是快于标准的梯度下降算法，基本的想法就是计算梯度的指数加权平均数，并利用该梯度更新你的权重。</p>
<p>计算动量：</p>
<script type="math/tex; mode=display">
v_{dW} = \beta v_{dW} + (1 - \beta) dW</script><script type="math/tex; mode=display">
v_{db} = \beta v_{db} + (1 - \beta) db</script><p>再更新参数：</p>
<script type="math/tex; mode=display">
W := W - \alpha v_{dW}</script><script type="math/tex; mode=display">
b := b - \alpha v_{db}</script><p>这样就可以减缓梯度下降的幅度。<em>它们能够最小化碗状函数，这些微分项，想象它们为从山上往下滚的一个球，提供了加速度，<strong>Momentum</strong>项相当于速度。</em></p>
<p>所以有两个超参数，学习率 $a$ 以及参数 $\beta$，$\beta$ 控制着指数加权平均数。$\beta$ 最常用的值是0.9，是很棒的鲁棒数。</p>
<p>有一个版本是 $v_{dW} = \beta v_{dW} + dW$，本质上没有区别。</p>
<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p><strong>root mean square prop</strong>算法，它也可以加速梯度下降，通过加快损失下降的方向，减缓无关方向，减少摆动。</p>
<script type="math/tex; mode=display">
S_{dW}= \beta S_{dW} + (1 -\beta) (dW)^{2}</script><script type="math/tex; mode=display">
S_{db}= \beta S_{db} + (1 - \beta)(db)^{2}</script><p>再更新参数：</p>
<script type="math/tex; mode=display">
W:= W -\alpha \dfrac{dW}{\sqrt{S_{dW}}}</script><script type="math/tex; mode=display">
b:=b -\alpha \dfrac{db}{\sqrt{S_{db}}}</script><p>为了确保数值稳定，在实际操练的时候，要在分母上加上一个很小很小的$\varepsilon$，$\varepsilon$是多少没关系，$10^{-8}$是个不错的选择.</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>把前面两个缝起来。</p>
<p>首先初始化：$v_{dW} = 0$，$S_{dW} =0$，$v_{db} = 0$，$S_{db} =0$。</p>
<script type="math/tex; mode=display">
v_{dW}= \beta_{1}v_{dW} + ( 1 - \beta_{1})dW</script><script type="math/tex; mode=display">
v_{db}= \beta_{1}v_{db} + ( 1 -\beta_{1} )db</script><script type="math/tex; mode=display">
S_{dW}=\beta_{2}S_{dW} + ( 1 - \beta_{2})(dW)^{2}</script><script type="math/tex; mode=display">
S_{db} =\beta_{2}S_{db} + ( 1 - \beta_{2} )(db)^{2}</script><p>一般使用<strong>Adam</strong>算法的时候，要计算偏差修正，$v_{dW}^{\text{corrected}}$，修正也就是在偏差修正之后：</p>
<script type="math/tex; mode=display">
v_{dW}^{\text{corrected}}= \dfrac{v_{dW}}{1 - \beta_{1}^{t}}</script><script type="math/tex; mode=display">
v_{db}^{\text{corrected}} =\dfrac{v_{db}}{1 -\beta_{1}^{t}}</script><script type="math/tex; mode=display">
S_{dW}^{\text{corrected}} =\dfrac{S_{dW}}{1 - \beta_{2}^{t}}</script><script type="math/tex; mode=display">
S_{db}^{\text{corrected}} =\dfrac{S_{db}}{1 - \beta_{2}^{t}}</script><p>最后更新权重：</p>
<script type="math/tex; mode=display">
W:= W - \dfrac{\alpha v_{dW}^{\text{corrected}}}{\sqrt{S_{dW}^{\text{corrected}}} +\varepsilon}</script><script type="math/tex; mode=display">
b:=b - \frac{\alpha v_{\text{db}}^{\text{corrected}}}{\sqrt{S_{\text{db}}^{\text{corrected}}} +\varepsilon}</script><p><strong>Adam</strong> 是一种极其常用的学习算法，被证明能有效适用于不同神经网络，适用于广泛的结构。</p>
<p>本算法中有很多超参数，超参数学习率$a$很重要，也经常需要调试，你可以尝试一系列值，然后看哪个有效。$\beta_{1}$常用的缺省值为0.9。超参数$\beta_{2}$，<strong>Adam</strong>论文作者，也就是<strong>Adam</strong>算法的发明者，推荐使用0.999。关于$\varepsilon$的选择其实没那么重要，<strong>Adam</strong>论文的作者建议$\varepsilon$为$10^{-8}$。</p>
<h2 id="学习率衰减-Learning-rate-decay"><a href="#学习率衰减-Learning-rate-decay" class="headerlink" title="学习率衰减 Learning rate decay"></a>学习率衰减 Learning rate decay</h2><p>慢慢减少$a$的本质在于，在学习初期，你能承受较大的步伐，但当开始收敛的时候，小一些的学习率能让你步伐小一些。</p>
<p>$a= \dfrac{1}{1 + decayrate \times \text{epoch}\text{-num}}a_{0}$，（<strong>decay-rate</strong>称为衰减率，<strong>epoch-num</strong>为代数，$\alpha_{0}$为初始学习率），注意这个衰减率是另一个需要调整的超参数。</p>
<p>人们用到的其它公式有$a =\dfrac{k}{\sqrt{\text{epoch-num}}}a_{0}$或者$a =\dfrac{k}{\sqrt{t}}a_{0}$（$t$为<strong>mini-batch</strong>的数字）。</p>
<h2 id="局部最优问题-Local-optima"><a href="#局部最优问题-Local-optima" class="headerlink" title="局部最优问题 Local optima"></a>局部最优问题 Local optima</h2><p>在深度学习研究早期，人们总是担心优化算法会困在极差的局部最优，不过随着深度学习理论不断发展，我们对局部最优的理解也发生了改变。</p>
<p>事实上，如果你要创建一个神经网络，通常梯度为零的点并不是这个图中的局部最优点，实际上成本函数的零梯度点，通常是鞍点。</p>
<p>意思就是一个最优点需要所有维度都是极小，在现在高维特征下很难遇到，所以碰到的所谓局部最优一般都不是最优，都可以跑出来。</p>
<h1 id="Lesson-2-3"><a href="#Lesson-2-3" class="headerlink" title="Lesson 2.3"></a>Lesson 2.3</h1><p>本章主要讲了超参数调试，归一化和深度学习框架。</p>
<h2 id="调试处理-Tuning-process"><a href="#调试处理-Tuning-process" class="headerlink" title="调试处理 Tuning process"></a>调试处理 Tuning process</h2><p>画格子取点。</p>
<p>超参数范围，对数尺。</p>
<p><strong>Pandas</strong>，小模型，每天逐渐变化超参数。</p>
<p><strong>Caviar</strong>，大模型，多个参数同时跑几天。</p>
<h2 id="归一化激活函数-Normalizing-activation"><a href="#归一化激活函数-Normalizing-activation" class="headerlink" title="归一化激活函数 Normalizing activation"></a>归一化激活函数 Normalizing activation</h2><p>就是 <strong>Batch 归一化</strong>，会使你的参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定。</p>
<p>实践中，经常做的是归一化$z^{[i]}$。</p>
<script type="math/tex; mode=display">
\mu = \dfrac{1}{m} \sum\limits_i z^{(i)}</script><script type="math/tex; mode=display">
\sigma^2 = \dfrac{1}{m} \sum\limits_i (z_i - \mu)^2</script><script type="math/tex; mode=display">
z_{norm}^{(i)} = \dfrac{z^{(i)} - \mu}{\sqrt{\sigma^2 + \varepsilon}}</script><p>所以现在我们已把这些$z$值标准化，化为含平均值0和标准单位方差，所以$z$的每一个分量都含有平均值0和方差1，但我们不想让隐藏单元总是含有平均值0和方差1，也许隐藏单元有了不同的分布会有意义，所以我们所要做的就是计算 $\tilde{z}^{(i)} = \gamma z_{norm}^{(i)} + \beta$，这里$\gamma$和$\beta$是你模型的学习参数，作用是可以随意设置${\tilde{z}}^{(i)}$的平均值。</p>
<p><em>应用Batch归一化了一些隐藏单元值中的平均值和方差，不过训练输入和这些隐藏单元值的一个区别是，你也许不想隐藏单元值必须是平均值0和方差1。它真正的作用是，使隐藏单元值的均值和方差标准化，即$z^{(i)}$有固定的均值和方差，均值和方差可以是0和1，也可以是其它值，它是由$\gamma$和$\beta$两参数控制的。</em></p>
<p>至于如何将 <em>BN</em> 层放进神经网络——tensorflow(</p>
<h2 id="Batch-Norm-为什么有用"><a href="#Batch-Norm-为什么有用" class="headerlink" title="Batch Norm 为什么有用"></a>Batch Norm 为什么有用</h2><p>通过归一化所有的输入特征值$x$，以获得类似范围的值，可以加速学习，不仅仅对于这里的输入值，还有隐藏单元的值。</p>
<p><strong>Batch</strong>归一化有效的第二个原因是，它可以使权重比你的网络更滞后或更深层。对于网络的泛化能力，浅层网络不一定能做的很好，所以我们尝试改变数据的分布，有个有点怪的名字“<strong>Covariate shift</strong>”。如果你已经学习了$x$到$y$ 的映射，如果$x$ 的分布改变了，那么你可能需要重新训练你的学习算法。这种做法同样适用于，如果真实函数由$x$ 到$y$ 映射保持不变。</p>
<p>按我的理解，就是前一层对于后一层的影响。当前一层的分布变化之后，后一层就要面临 <strong>Covariate shift</strong> 的问题，就会不稳定。而 Batch 归一化做的就是它减少了这些隐藏值分布变化的数量，使分布更稳定，神经网络的之后层就会有更坚实的基础。它限制了在前层的参数更新，会影响数值分布的程度，在后一层看到的这种情况，因此得到学习。</p>
<p><em>即使使输入分布改变了一些，它会改变得更少。它做的是当前层保持学习，当改变时，迫使后层适应的程度减小了，你可以这样想，它减弱了前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习，稍稍独立于其它层，这有助于加速整个网络的学习。</em></p>
<p>还有轻微的正则化的作用。因为均值和方差有一点小噪音，因为它只是由一小部分数据估计得出的。</p>
<h2 id="Softmax-回归"><a href="#Softmax-回归" class="headerlink" title="Softmax 回归"></a>Softmax 回归</h2><p>有一种<strong>logistic</strong>回归的一般形式，叫做<strong>Softmax</strong>回归，能让你在试图识别某一分类时做出预测，或者说是多种分类中的一个，不只是识别两个分类。</p>
<script type="math/tex; mode=display">
t=e^{z^{[l]}}</script><script type="math/tex; mode=display">
a_{i}^{[l]} = \dfrac{t_{i}}{\sum\limits_i t_{i}}</script><p>输出的 $a_i$ 就是第 $i$ 类的概率。</p>
<p>训练 softmax 分类器用的损失    函数一般是 $L(\hat{y},y ) = - \sum\limits_{j}y_{j}\log{\hat{y}_{j}}$。</p>
<h2 id="深度学习框架"><a href="#深度学习框架" class="headerlink" title="深度学习框架"></a>深度学习框架</h2><ul>
<li>Caffe/Caffe2</li>
<li>NCTK</li>
<li>DL4J</li>
<li>Keras</li>
<li>Lasagne</li>
<li>mxnet</li>
<li>PaddlePaddle</li>
<li>TensorFlow</li>
<li>Theano</li>
<li>Torch</li>
</ul>
<h1 id="Lesson-3-1"><a href="#Lesson-3-1" class="headerlink" title="Lesson 3.1"></a>Lesson 3.1</h1><p>本章主要介绍机器学习策略。很多事思想意识上的东西，可能很多照搬原话（</p>
<h2 id="正交化-Orthogonalization"><a href="#正交化-Orthogonalization" class="headerlink" title="正交化 Orthogonalization"></a>正交化 Orthogonalization</h2><p>意思大概就是把所有特征正交到几个相互独立的特征上。</p>
<blockquote>
<p>在机器学习中，如果你可以观察你的系统，然后说这一部分是错的，它在训练集上做的不好、在开发集上做的不好、它在测试集上做的不好，或者它在测试集上做的不错，但在现实世界中不好，这就很好。必须弄清楚到底是什么地方出问题了，然后我们刚好有对应的旋钮，或者一组对应的旋钮，刚好可以解决那个问题，那个限制了机器学习系统性能的问题。</p>
</blockquote>
<h2 id="单一数字评估指标-Single-number-evaluation-metric"><a href="#单一数字评估指标-Single-number-evaluation-metric" class="headerlink" title="单一数字评估指标 Single number evaluation metric"></a>单一数字评估指标 Single number evaluation metric</h2><p>无论是调整超参数，或者是尝试不同的学习算法，或者在搭建机器学习系统时尝试不同手段，如果你有一个单实数评估指标，进展会快得多，它可以快速告诉你，新尝试的手段比之前的手段好还是差。所以当团队开始进行机器学习项目时，推荐为问题设置一个单实数评估指标（F1 分数，平均值）。</p>
<p>有一个开发集，加上单实数评估指标，迭代速度肯定会很快，它可以加速改进机器学习算法的迭代过程。</p>
<h2 id="满足和优化指标-Satisficing-and-optimizing-metrics"><a href="#满足和优化指标-Satisficing-and-optimizing-metrics" class="headerlink" title="满足和优化指标 Satisficing and optimizing metrics"></a>满足和优化指标 Satisficing and optimizing metrics</h2><p>要把顾及到的所有事情组合成单实数评估指标有时并不容易，在那些情况里，有时候设立满足和优化指标是很重要的。</p>
<p>优化指标，想要准确度最大化，想做的尽可能准确。</p>
<p>满足指标，意思是它必须足够好，达到标准之后，不那么在乎这指标有多好。</p>
<p>通过定义优化和满足指标，就可以提供一个明确的方式，去选择“最好的”分类器。</p>
<h2 id="训练-开发-测试集划分-train-dev-test-distributions"><a href="#训练-开发-测试集划分-train-dev-test-distributions" class="headerlink" title="训练/开发/测试集划分 train/dev/test distributions"></a>训练/开发/测试集划分 train/dev/test distributions</h2><p>开发（<strong>dev</strong>）集也叫做开发集（<strong>development set</strong>），有时称为保留交叉验证集（<strong>hold out cross validation set</strong>）。然后，机器学习中的工作流程是，尝试很多思路，用训练集训练不同的模型，然后使用开发集来评估不同的思路，然后选择一个，然后不断迭代去改善开发集的性能，直到最后可以得到一个令你满意的成本，然后你再用测试集去评估。</p>
<p>让开发集和测试集来自同一分布，通过将所有数据随机洗牌。</p>
<p>大小 98:1:1.</p>
<p><em>处理机器学习问题时，应该把它切分成独立的步骤。一步是弄清楚如何定义一个指标来衡量你想做的事情的表现，然后我们可以分开考虑如何改善系统在这个指标上的表现。要把机器学习任务看成两个独立的步骤，用目标这个比喻，第一步就是设定目标。所以要定义你要瞄准的目标，这是完全独立的一步，这是你可以调节的一个旋钮。如何设立目标是一个完全独立的问题，把它看成是一个单独的旋钮，可以调试算法表现的旋钮，如何精确瞄准，如何命中目标，定义指标是第一步。</em></p>
<p><em>然后第二步要做别的事情，在逼近目标的时候，也许学习算法针对某个成本函数优化，要最小化训练集上的损失。可以做的其中一件事是，修改这个，为了引入权重，也许最后需要修改这个归一化常数。</em></p>
<p>如何定义$J$并不重要，关键在于正交化的思路。将定义指标看成一步，然后在定义了指标之后，你才能想如何优化系统来提高这个指标评分。比如改变你神经网络要优化的成本函数$J$。</p>
<blockquote>
<p>所以我的建议是，即使你无法定义出一个很完美的评估指标和开发集，你直接快速设立出来，然后使用它们来驱动你们团队的迭代速度。如果在这之后，你发现选的不好，你有更好的想法，那么完全可以马上改。对于大多数团队，我建议最好不要在没有评估指标和开发集时跑太久，因为那样可能会减慢你的团队迭代和改善算法的速度。</p>
</blockquote>
<h2 id="为什么是人的表现-Why-human-level-performance"><a href="#为什么是人的表现-Why-human-level-performance" class="headerlink" title="为什么是人的表现 Why human-level performance?"></a>为什么是人的表现 Why human-level performance?</h2><p>在过去的几年里，更多的机器学习团队一直在讨论如何比较机器学习系统和人类的表现。</p>
<blockquote>
<p>我认为有两个主要原因，首先是因为深度学习系统的进步，机器学习算法突然变得更好了。在许多机器学习的应用领域已经开始见到算法已经可以威胁到人类的表现了。其次，事实证明，当你试图让机器做人类能做的事情时，可以精心设计机器学习系统的工作流程，让工作流程效率更高，所以在这些场合，比较人类和机器是很自然的，或者你要让机器模仿人类的行为。</p>
</blockquote>
<p>当这个算法表现比人类更好时，进展和精确度的提升就变得更慢了。也许它还会越来越好，但是在超越人类水平之后，它还可以变得更好，但性能增速，准确度上升的速度这个斜率，会变得越来越平缓，我们都希望能达到理论最佳性能水平。随着时间的推移，当继续训练算法时，可能模型越来越大，数据越来越多，但是性能无法超过某个理论上限，这就是所谓的贝叶斯最优错误率（<strong>Bayes optimal error</strong>）。</p>
<p>所以贝叶斯最优错误率一般认为是理论上可能达到的最优错误率，就是说没有任何办法设计出一个$x$到$y$的函数，让它能够超过一定的准确度。</p>
<h2 id="可避免偏差-Avoidable-bias"><a href="#可避免偏差-Avoidable-bias" class="headerlink" title="可避免偏差 Avoidable bias"></a>可避免偏差 Avoidable bias</h2><p>在之前的课程关于偏差和方差的讨论中，我们主要假设有一些任务的贝叶斯错误率几乎为0。根据定义，人类水平错误率比贝叶斯错误率高一点，因为贝叶斯错误率是理论上限，但人类水平错误率离贝叶斯错误率不会太远。</p>
<p>对贝叶斯错误率的估计和训练错误率之间的差值称为可避免偏差。这个训练错误率和开发错误率之前的差值，就大概说明你的算法在方差问题上还有多少改善空间。</p>
<p><em>理解偏差和方差，那么在人类可以做得很好的任务中，你可以估计人类水平的错误率，你可以使用人类水平错误率来估计贝叶斯错误率。所以你到贝叶斯错误率估计值的差距，告诉你可避免偏差问题有多大，可避免偏差问题有多严重，而训练错误率和开发错误率之间的差值告诉你方差上的问题有多大，你的算法是否能够从训练集泛化推广到开发集。</em></p>
<p>要达到超越人类的表现往往不容易，但如果有足够多的数据，已经有很多深度学习系统，在单一监督学习问题上已经超越了人类的水平，所以这对在开发的应用是有意义的。</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/08/02/scikit-learn/" rel="prev" title="scikit-learn">
      <i class="fa fa-chevron-left"></i> scikit-learn
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/08/10/mit-cv/" rel="next" title="mit机器学习cv">
      mit机器学习cv <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-1-1"><span class="nav-number">1.</span> <span class="nav-text">Lesson 1.1</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-1-2"><span class="nav-number">2.</span> <span class="nav-text">Lesson 1.2</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-1-3"><span class="nav-number">3.</span> <span class="nav-text">Lesson 1.3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-Back-Propagation"><span class="nav-number">3.1.</span> <span class="nav-text">反向传播 Back Propagation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-1-4"><span class="nav-number">4.</span> <span class="nav-text">Lesson 1.4</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%B1%82%E8%A1%A8%E7%A4%BA%EF%BC%9F"><span class="nav-number">4.1.</span> <span class="nav-text">为什么使用深层表示？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9D%97"><span class="nav-number">4.2.</span> <span class="nav-text">搭建神经网络块</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%92%8C%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-number">4.3.</span> <span class="nav-text">参数和超参数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-2-1"><span class="nav-number">5.</span> <span class="nav-text">Lesson 2.1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E5%B7%AE-%E5%81%8F%E5%B7%AE"><span class="nav-number">5.1.</span> <span class="nav-text">方差&#x2F;偏差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95"><span class="nav-number">5.2.</span> <span class="nav-text">基本方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Regularization%EF%BC%89"><span class="nav-number">5.3.</span> <span class="nav-text">正则化（Regularization）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dropout-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">5.4.</span> <span class="nav-text">dropout 正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-number">5.5.</span> <span class="nav-text">其他正则化方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E8%BE%93%E5%85%A5%EF%BC%88Normalizing%EF%BC%89"><span class="nav-number">5.6.</span> <span class="nav-text">归一化输入（Normalizing）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1-%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%EF%BC%88Vanishing-Exploding-gradients%EF%BC%89"><span class="nav-number">5.7.</span> <span class="nav-text">梯度消失&#x2F;梯度爆炸（Vanishing &#x2F; Exploding gradients）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">5.8.</span> <span class="nav-text">神经网络权重的初始化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%95%B0%E5%80%BC%E9%80%BC%E8%BF%91-Numerical-approximation-of-gradients"><span class="nav-number">5.9.</span> <span class="nav-text">梯度的数值逼近 Numerical approximation of gradients</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-number">5.10.</span> <span class="nav-text">梯度检验的注意事项</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-2-2"><span class="nav-number">6.</span> <span class="nav-text">Lesson 2.2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Mini-batch-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">6.1.</span> <span class="nav-text">Mini-batch 梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E6%95%B0-Exponentially-weighted-averages"><span class="nav-number">6.2.</span> <span class="nav-text">指数加权平均数 Exponentially weighted averages</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-Gradient-descent-with-Momentum"><span class="nav-number">6.3.</span> <span class="nav-text">动量梯度下降法 Gradient descent with Momentum</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RMSprop"><span class="nav-number">6.4.</span> <span class="nav-text">RMSprop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adam"><span class="nav-number">6.5.</span> <span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F-Learning-rate-decay"><span class="nav-number">6.6.</span> <span class="nav-text">学习率衰减 Learning rate decay</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E9%97%AE%E9%A2%98-Local-optima"><span class="nav-number">6.7.</span> <span class="nav-text">局部最优问题 Local optima</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-2-3"><span class="nav-number">7.</span> <span class="nav-text">Lesson 2.3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B0%83%E8%AF%95%E5%A4%84%E7%90%86-Tuning-process"><span class="nav-number">7.1.</span> <span class="nav-text">调试处理 Tuning process</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Normalizing-activation"><span class="nav-number">7.2.</span> <span class="nav-text">归一化激活函数 Normalizing activation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-Norm-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E7%94%A8"><span class="nav-number">7.3.</span> <span class="nav-text">Batch Norm 为什么有用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Softmax-%E5%9B%9E%E5%BD%92"><span class="nav-number">7.4.</span> <span class="nav-text">Softmax 回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6"><span class="nav-number">7.5.</span> <span class="nav-text">深度学习框架</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-3-1"><span class="nav-number">8.</span> <span class="nav-text">Lesson 3.1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E4%BA%A4%E5%8C%96-Orthogonalization"><span class="nav-number">8.1.</span> <span class="nav-text">正交化 Orthogonalization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E4%B8%80%E6%95%B0%E5%AD%97%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87-Single-number-evaluation-metric"><span class="nav-number">8.2.</span> <span class="nav-text">单一数字评估指标 Single number evaluation metric</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BB%A1%E8%B6%B3%E5%92%8C%E4%BC%98%E5%8C%96%E6%8C%87%E6%A0%87-Satisficing-and-optimizing-metrics"><span class="nav-number">8.3.</span> <span class="nav-text">满足和优化指标 Satisficing and optimizing metrics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83-%E5%BC%80%E5%8F%91-%E6%B5%8B%E8%AF%95%E9%9B%86%E5%88%92%E5%88%86-train-dev-test-distributions"><span class="nav-number">8.4.</span> <span class="nav-text">训练&#x2F;开发&#x2F;测试集划分 train&#x2F;dev&#x2F;test distributions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%BA%E7%9A%84%E8%A1%A8%E7%8E%B0-Why-human-level-performance"><span class="nav-number">8.5.</span> <span class="nav-text">为什么是人的表现 Why human-level performance?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%AF%E9%81%BF%E5%85%8D%E5%81%8F%E5%B7%AE-Avoidable-bias"><span class="nav-number">8.6.</span> <span class="nav-text">可避免偏差 Avoidable bias</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hui hui"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Hui hui</p>
  <div class="site-description" itemprop="description">Be happy forever!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">30</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/songtianhui" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;songtianhui" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:191098194@smail.nju.edu.cn" title="E-Mail → mailto:191098194@smail.nju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hui hui</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>



        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  

  <script async src="/js/cursor/fireworks.js"></script>


 
<script>
  var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });
</script>


 
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20210126,"YYYYMMDD"));
    ages = ages.replace(/years?/, "years");
    ages = ages.replace(/months?/, "moths");
    ages = ages.replace(/days?/, "days");
    ages = ages.replace(/hours?/, "hours");
    ages = ages.replace(/minutes?/, "minutes");
    ages = ages.replace(/seconds?/, "seconds");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `I have been here waiting for you for ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


 
<script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginrootpath":"live2dw/","pluginjspath":"lib/","pluginmodelpath":"assets/ relative)","scriptfrom":"local","tagmode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":180,"height":300},"mobile":{"show":true},"react":{"opacitydefault":0.7},"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
