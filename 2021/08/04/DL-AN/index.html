<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-bounce.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"songtianhui.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="视频指路">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达深度学习笔记">
<meta property="og:url" content="https://songtianhui.github.io/2021/08/04/DL-AN/index.html">
<meta property="og:site_name" content="Songtianhui&#39;s Blog">
<meta property="og:description" content="视频指路">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/04/DL-AN/L1_week3_6.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/04/DL-AN/network.png">
<meta property="article:published_time" content="2021-08-04T14:51:29.000Z">
<meta property="article:modified_time" content="2021-08-09T18:06:20.909Z">
<meta property="article:author" content="Hui hui">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://songtianhui.github.io/2021/08/04/DL-AN/L1_week3_6.png">

<link rel="canonical" href="https://songtianhui.github.io/2021/08/04/DL-AN/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>吴恩达深度学习笔记 | Songtianhui's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="/custom_css_source.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Songtianhui's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://songtianhui.github.io/2021/08/04/DL-AN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Hui hui">
      <meta itemprop="description" content="Be happy forever!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Songtianhui's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          吴恩达深度学习笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-08-04 22:51:29" itemprop="dateCreated datePublished" datetime="2021-08-04T22:51:29+08:00">2021-08-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-08-10 02:06:20" itemprop="dateModified" datetime="2021-08-10T02:06:20+08:00">2021-08-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1FT4y1E74V?from=search&amp;seid=7469215768123017337">视频指路</a></p>
<a id="more"></a>
<hr>
<h1 id="Lesson-1-1"><a href="#Lesson-1-1" class="headerlink" title="Lesson 1.1"></a>Lesson 1.1</h1><p>本章是对深度学习神经网络的一个介绍，课程的概览。</p>
<h1 id="Lesson-1-2"><a href="#Lesson-1-2" class="headerlink" title="Lesson 1.2"></a>Lesson 1.2</h1><p>本章主要讲了一下 logisitic 回归，梯度下降，在ML中已经学习了。</p>
<p>然后介绍了 <code>python</code> 中的一些知识，向量化、广播、<code>numpy</code>、jupyter notebook等，都可以 <strong>rtfm</strong>，不在此赘述。</p>
<p>唯一有一点就是符号上的表示，andrew 喜欢用 $m$ 表示样本个数，$n$ 表示特征数。我习惯 mit 的课教的 $n$ 表示样本数，$d$ 表示特征维数。</p>
<h1 id="Lesson-1-3"><a href="#Lesson-1-3" class="headerlink" title="Lesson 1.3"></a>Lesson 1.3</h1><p>本章算是一个神经网络的引入，介绍一些基本概念，浅层神经网络（shallow neural network）。</p>
<p><img data-src="L1_week3_6.png" alt></p>
<p>每个神经单元就是这么个计算过程，有输入 $x$，权重 $w$，偏移 $b$，激活函数（activation function） $h$。</p>
<ul>
<li>$z = w^T x + b$</li>
<li>$a = h(z)$</li>
</ul>
<p>符号： $a_1^{<a href="1">1</a>} $ ，右上角中括号表示层数，右上角括号中表示第几个样本，右下角表示该层第几个神经元。</p>
<p>对于一个输入样本，避免一层内的 for 循环，向量化计算：</p>
<script type="math/tex; mode=display">
z^{[i]} = W^{[i]} a^{[i-1]} + b^{[i]}\\
a^{[i]} = h(z^{[i]})</script><p>$W^{[i]}$ 是第 $i$ 层每个神经元的权重排成的矩阵。</p>
<p>举例图示：</p>
<script type="math/tex; mode=display">
\left[
        \begin{array}{c}
        z^{[1]}_{1}\\
        z^{[1]}_{2}\\
        z^{[1]}_{3}\\
        z^{[1]}_{4}\\
        \end{array}
        \right]
         =
    \overbrace{
    \left[
        \begin{array}{c}
        ...W^{[1]T}_{1}...\\
        ...W^{[1]T}_{2}...\\
        ...W^{[1]T}_{3}...\\
        ...W^{[1]T}_{4}...
        \end{array}
        \right]
        }^{W^{[1]}}
        *
    \overbrace{
    \left[
        \begin{array}{c}
        x_1\\
        x_2\\
        x_3\\
        \end{array}
        \right]
        }^{input}
        +
    \overbrace{
    \left[
        \begin{array}{c}
        b^{[1]}_1\\
        b^{[1]}_2\\
        b^{[1]}_3\\
        b^{[1]}_4\\
        \end{array}
        \right]
        }^{b^{[1]}}</script><p>对于所有样本，避免 for 遍历样本，向量化计算：</p>
<script type="math/tex; mode=display">
Z^{[i]} = W^{[i]}A^{[i-1]} + b^{[i]}\\
A^{[i]} = h(Z^{[i]})</script><p>其中 $Z^{[i]},A^{[i]}$ 是第 $i$ 层所有样本输出排成的矩阵。</p>
<p>激活函数：<strong>sigmoid</strong>（主要在二分类）, <strong>tanh</strong>（比 sigmoid 常用），<strong>ReLU</strong>（在神经网络中很常用），Leaky ReLU。</p>
<p>前两个有梯度消失的风险。</p>
<p>这里提到一个很重要的问题就是为什么要使用非线性函数而不是直接 $a = z$。因为全用线性激活函数（identity）会使神经网络退化成一个单层模型。</p>
<h2 id="反向传播-Back-Propagation"><a href="#反向传播-Back-Propagation" class="headerlink" title="反向传播 Back Propagation"></a>反向传播 Back Propagation</h2><p>也就是神经网络的梯度下降（Gradient Descent）。比较重要，理解一下推倒，本质上是函数求导的链式法则。</p>
<p>代价函数：</p>
<script type="math/tex; mode=display">
J(W, b) = \dfrac{1}{m}\sum\limits_{i= 1}^{m}L(\hat{y}, y)</script><p>当参数初始化成某些值后，每次梯度下降都会循环计算以下预测值：$\hat{y}^{(i)},(i=1,2,…,m)$。</p>
<p>有 $dW^{[i]} = \dfrac{\partial J}{\partial W^{[i]}}$, $d b^{[i]} = \dfrac{\partial J}{\partial b^{[i]}}$。</p>
<p>在梯度下降时每一次更新：$W^{[i]}\implies{W^{[i]} - \eta dW^{[i]}},b^{[i]}\implies{b^{[i]} -\eta db^{[i]}}$, $\eta$ 为步长。</p>
<p>反向传播时，就是一个链式求导：</p>
<script type="math/tex; mode=display">
\underbrace{
    \left.
    \begin{array}{l}
    x \\
    w \\
    b 
    \end{array}
    \right\}
    }_{dw=dz \cdot x, db =dz}
    \impliedby \underbrace{z=w^Tx+b}_{dz=da\cdot g^{'}(z),
    g(z)=\sigma(z),
    \frac{dL}{dz}} = \frac{dL}{da} \cdot \frac{da}{dz},
    \frac{d}{ dz} g(z)=g^{'}(z)
    \impliedby \underbrace{a = \sigma(z) 
    \impliedby L(a,y)}_{da=\frac{d}{da}L\left(a,y \right)=(-y\log{\alpha} - (1 - y)\log(1 - a))^{'}=-\frac{y}{a} + \frac{1 - y}{1 - a} }</script><p>所以有：</p>
<p>$dz^{[L]} = A^{[L]} - Y$</p>
<p>$dW^{[i]} = \dfrac{1}{m} dZ^{[i]}A^{[i-1]T}$</p>
<p>$db^{[i]} = \dfrac{1}{m}$ <code>np.sum(dZ^&#123;[i]&#125;, axis=1)​</code></p>
<script type="math/tex; mode=display">
dz^{[i]} = \underbrace{W^{[i + 1]T} dz^{[i+1]}}_{(n^{[i]},m)}\quad \times  \underbrace{g^{[i]'}}_{activation \; function \; of \; hidden \; layer}\times  \quad\underbrace{(z^{[i]})}_{(n^{[1]},m)}</script><ul>
<li>随机初始化，不要初始化成相同的参数。</li>
</ul>
<h1 id="Lesson-1-4"><a href="#Lesson-1-4" class="headerlink" title="Lesson 1.4"></a>Lesson 1.4</h1><p>本章介绍深层神经网络，主要就是把前一章讲的只有两层的网络更推广一下，而我们在上一章其实已经推广过了。</p>
<h2 id="为什么使用深层表示？"><a href="#为什么使用深层表示？" class="headerlink" title="为什么使用深层表示？"></a>为什么使用深层表示？</h2><p>深度神经网络的这许多隐藏层中，较早的前几层能学习一些低层次的简单特征，等到后几层，就能把简单的特征结合起来，去探测更加复杂的东西。</p>
<p>深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。</p>
<blockquote>
<p>说实话，我认为“深度学习”这个名字挺唬人的，这些概念以前都统称为有很多隐藏层的神经网络，但是深度学习听起来多高大上，太深奥了，对么？这个词流传出去以后，这是神经网络的重新包装或是多隐藏层神经网络的重新包装，激发了大众的想象力。    ——Andrew</p>
</blockquote>
<h2 id="搭建神经网络块"><a href="#搭建神经网络块" class="headerlink" title="搭建神经网络块"></a>搭建神经网络块</h2><p>其实就是对于每一层，权重矩阵，偏移值，激活函数，在前向传播的时候缓存（cache）好 $z,a$ 等值，用反向传播时计算 $dW,db$ 等。</p>
<p>就放一张老师的板书吧（</p>
<p><img data-src="network.png" alt="building blocks"></p>
<h2 id="参数和超参数"><a href="#参数和超参数" class="headerlink" title="参数和超参数"></a>参数和超参数</h2><p>算法中的<strong>learning rate</strong> $a$（学习率）、<strong>iterations</strong>(梯度下降法循环的数量)、$L$（隐藏层数目）、$n^{[l]}$（隐藏层单元数目）、<strong>choice of activation function</strong>（激活函数的选择）都需要你来设置，这些数字实际上控制了最后的参数$W$和$b$的值，所以它们被称作超参数（Hyperparameter）。</p>
<p>如何寻找超参数：走<strong>Idea—Code—Experiment—Idea</strong>这个循环，尝试各种不同的参数，实现模型并观察是否成功，然后再迭代。</p>
<blockquote>
<p>应用深度学习领域，一个很大程度基于经验的过程，凭经验的过程通俗来说，就是试直到你找到合适的数值。</p>
<p>如果你所解决的问题需要很多年时间，只要经常试试不同的超参数，勤于检验结果，看看有没有更好的超参数数值，相信你慢慢会得到设定超参数的直觉，知道你的问题最好用什么数值。</p>
</blockquote>
<hr>
<h1 id="Lesson-2-1"><a href="#Lesson-2-1" class="headerlink" title="Lesson 2.1"></a>Lesson 2.1</h1><p>本章主要讲改善神经网络，超参数调试、正则化等内容。</p>
<ul>
<li>当我们有百万量级以上的数据，可以拿 99% 以上的数据来进行训练，几万条用来交叉验证（dev）和测试就可以了。</li>
</ul>
<h2 id="方差-偏差"><a href="#方差-偏差" class="headerlink" title="方差/偏差"></a>方差/偏差</h2><ul>
<li>高偏差（<strong>high bias</strong>），欠拟合（<strong>underfitting</strong>）。</li>
<li>高方差（<strong>high variance</strong>），过拟合（<strong>overfitting</strong>）。</li>
</ul>
<p>通过训练集和验证集误差判断：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Training set error</th>
<th style="text-align:center">Dev set error</th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1%</td>
<td style="text-align:center">15%</td>
<td style="text-align:center">high variance</td>
</tr>
<tr>
<td style="text-align:center">15%</td>
<td style="text-align:center">16%</td>
<td style="text-align:center">high bias</td>
</tr>
<tr>
<td style="text-align:center">15%</td>
<td style="text-align:center">30%</td>
<td style="text-align:center">high variance &amp; bias</td>
</tr>
<tr>
<td style="text-align:center">0.5%</td>
<td style="text-align:center">1%</td>
<td style="text-align:center">low variance &amp; bias</td>
</tr>
</tbody>
</table>
</div>
<h2 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h2><blockquote>
<p>初始模型训练完成后，我首先要知道算法的偏差高不高，如果偏差较高，试着评估训练集或训练数据的性能。如果偏差的确很高，甚至无法拟合训练集，那么你要做的就是选择一个新的网络，比如含有更多隐藏层或者隐藏单元的网络，或者花费更多时间来训练网络，或者尝试更先进的优化算法，后面我们会讲到这部分内容。</p>
</blockquote>
<ul>
<li>只要正则适度，通常构建一个更大的网络便可以，在不影响方差的同时减少偏差，而采用更多数据通常可以在不过多影响偏差的同时减少方差。</li>
<li>训练网络，选择网络或者准备更多数据，现在我们有工具可以做到在减少偏差或方差的同时，不对另一方产生过多不良影响。</li>
</ul>
<h2 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h2><p>深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，正则化通常有助于避免过拟合或减少你的网络误差。</p>
<p>就是在代价函数里加一个正则化项，一般用 $\dfrac{\lambda}{2m}$乘以$w$范数的平方,其中$\left| w \right|_2^2$是$w$的欧几里德范数的平方，$L2$ 正则化。</p>
<p>神经网络中的正则项就是为$\dfrac{\lambda }{2m}\sum\limits_{l = 1}^{L}| W^{[l]}|^{2}$，我们称${||W^{\left[l\right]}||}^{2}$为范数平方，这个矩阵范数${||W^{\left[l\right]}||}^{2}$（即平方范数），被定义为矩阵中所有元素的平方求和。该矩阵范数被称作“弗罗贝尼乌斯范数”，用下标$F$标注。</p>
<p>带正则化的梯度下降中，对$W^{[l]}$的偏导数，把$W^{[l]}$替换为$W^{[l]}$减去学习率乘以$dW$。现在我们要做的就是给$dW$加上这一项$\dfrac {\lambda}{m}W^{[l]}$，然后计算这个更新项，使用新定义的$dW^{[l]}$，它的定义含有相关参数代价函数导数和，以及最后添加的额外正则项。</p>
<script type="math/tex; mode=display">
\begin{aligned}
W^{[l]} :&= W^{[l]}  - \alpha \times \left[(\text{from backpap}) + \dfrac{\lambda}{m}W^{[l]}\right]\\ &= (1 - \frac{\alpha \lambda}{m}) W^{[l]} - \alpha \times (\text{from backpap})
\end{aligned}</script><p><em>正则化预防过拟合的原因：极限思想，当lambda很大权重为0，退化成欠拟合，有个right fit 的中间态。</em></p>
<h2 id="dropout-正则化"><a href="#dropout-正则化" class="headerlink" title="dropout 正则化"></a>dropout 正则化</h2><p>就是随机消掉一些神经元。。。</p>
<ul>
<li><strong>inverted dropout</strong>（反向随机失活）<ul>
<li>首先要定义向量$d$，$d^{[3]}$表示网络第三层的<strong>dropout</strong>向量：<code>d3 = np.random.rand(a3.shape[0],a3.shape[1])</code> 。</li>
<li>然后看它是否小于某数，我们称之为<strong>keep-prob</strong>，<strong>keep-prob</strong>是一个具体数字，它表示保留某个隐藏单元的概率。</li>
<li>接下来要做的就是从第三层中获取$a^{[3]}$，$a^{[3]}$含有要计算的激活函数，$a^{[3]}$等于上面的$a^{[3]}$乘以 $d^{[3]}$，就是把 $d$ 中 0 对应位置的数归零。（$d$ 实际上是一个布尔数组）</li>
<li>最后，我们向外扩展$a^{[3]}$，用它除以<strong>keep-prob</strong>参数。</li>
</ul>
</li>
</ul>
<p>显然在测试阶段，我们不使用<strong>dropout</strong>。要同时在 <strong>fpp</strong> 和 <strong>bpp</strong> 中使用 dropout。</p>
<p><em>dropout 预防过拟合的原因，dropout 的功能类似于$L2$正则化，与$L2$正则化不同的是应用方式不同会带来一点点小变化，甚至更适用于不同的输入范围。</em></p>
<ul>
<li><p>如果你担心某些层比其它层更容易发生过拟合，可以把某些层的<strong>keep-prob</strong>值设置得比其它层更低，缺点是为了使用交叉验证，你要搜索更多的超级参数，另一种方案是在一些层上应用<strong>dropout</strong>，而有些层不用<strong>dropout</strong>，应用<strong>dropout</strong>的层只含有一个超级参数，就是<strong>keep-prob</strong>。</p>
</li>
<li><p>它在其它领域应用得比较少，主要存在于计算机视觉领域，因为我们通常没有足够的数据，所以一直存在过拟合。</p>
</li>
<li><strong>dropout</strong>一大缺点就是代价函数$J$不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。</li>
</ul>
<h2 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h2><ul>
<li>数据扩增</li>
<li><strong>early stopping</strong><ul>
<li>缺点是不能独立处理梯度下降和优化代价函数。</li>
</ul>
</li>
</ul>
<h2 id="归一化输入（Normalizing）"><a href="#归一化输入（Normalizing）" class="headerlink" title="归一化输入（Normalizing）"></a>归一化输入（Normalizing）</h2><p>训练神经网络，其中一个加速训练的方法就是归一化输入。归一化需要两个步骤：</p>
<ol>
<li>零均值<ul>
<li>$\mu = \frac{1}{m}\sum\limits_{i =1}^{m}x^{(i)}$，它是一个向量，$x$ 等于每个训练数据 $x$ 减去 $\mu$，意思是移动训练集，直到它完成零均值化。 </li>
</ul>
</li>
<li>归一化方差<ul>
<li>$ \sigma^{2}= \frac{1}{m}\sum\limits_{i =1}^{m}(x^{(i)})^{2} $，$\sigma^{2}$是一个向量，它的每个特征都有方差，把所有数据除以向量$\sigma^{2}$。</li>
</ul>
</li>
</ol>
<h2 id="梯度消失-梯度爆炸（Vanishing-Exploding-gradients）"><a href="#梯度消失-梯度爆炸（Vanishing-Exploding-gradients）" class="headerlink" title="梯度消失/梯度爆炸（Vanishing / Exploding gradients）"></a>梯度消失/梯度爆炸（Vanishing / Exploding gradients）</h2><p>神经网络层数多了，激活函数就会以指数级递增或递减。</p>
<h2 id="神经网络权重的初始化"><a href="#神经网络权重的初始化" class="headerlink" title="神经网络权重的初始化"></a>神经网络权重的初始化</h2><p>针对梯度消失/爆炸，有一个方案就是更谨慎地选择随机初始化参数。</p>
<p>$z = w_{1}x_{1} + w_{2}x_{2} + \ldots +w_{n}x_{n}$，为了预防$z$值过大或过小，希望每项值更小，最合理的方法就是设置$w_{i}=\frac{1}{n}$，$n$表示神经元的输入特征数量。</p>
<p>实际上，你要做的就是设置某层权重矩阵 <code>W[l] = np.random.randn(shape) * np.sqrt(1 / n[l-1])</code>，$n^{[l - 1]}$ 就是我喂给第$l$层神经单元的数量（即第$l-1$层神经元数量）。</p>
<p>如果你是用的是<strong>Relu</strong>激活函数，而不是$\frac{1}{n}$，方差设置为$\frac{2}{n}$，效果会更好。</p>
<p>对于<strong>tanh</strong>函数来说，用$\sqrt{\frac{1}{n^{[l-1]}}}$。</p>
<h2 id="梯度的数值逼近-Numerical-approximation-of-gradients"><a href="#梯度的数值逼近-Numerical-approximation-of-gradients" class="headerlink" title="梯度的数值逼近 Numerical approximation of gradients"></a>梯度的数值逼近 Numerical approximation of gradients</h2><p>就是导数定义，双边误差，即$\frac{f\left(\theta + \varepsilon \right) - f(\theta -\varepsilon)}{2\varepsilon}$。</p>
<p>先将所有的参数 $W, b$ 展开成一个大向量 $\theta$，在<strong>bpp</strong>中，算完梯度之后所有的梯度 $dW, db$ 就是 $d\theta$。</p>
<p>然后比较 $d\theta_{\text{approx}}\left[i \right] = \frac{J\left( \theta_{1},\theta_{2},\ldots\theta_{i} + \varepsilon,\ldots \right) - J\left( \theta_{1},\theta_{2},\ldots\theta_{i} - \varepsilon,\ldots \right)}{2\varepsilon}$ 和 $d\theta[i]$ 的值接不接近。</p>
<p>就计算它们的欧式距离再归一化，$\dfrac{||d\theta_{\text{approx}} -d\theta||_{2}}{||d\theta_{\text{approx}}||_2 + ||d\theta||_2}$。计算得到的值为$10^{-7}$或更小，这就很好；如果它的值在$10^{-5}$范围内，就要小心了，也许这个值没问题，但再次检查这个向量的所有项，确保没有一项误差过大，可能这里有<strong>bug</strong>。如果比$10^{-3}$大很多，就会很担心是否存在<strong>bug</strong>，这时应该仔细检查所有$\theta$项，看是否有一个具体的$i$值，使得$d\theta_{\text{approx}}\left[i \right]$与$ d\theta[i]$大不相同，并用它来追踪一些求导计算是否正确。</p>
<h2 id="梯度检验的注意事项"><a href="#梯度检验的注意事项" class="headerlink" title="梯度检验的注意事项"></a>梯度检验的注意事项</h2><ol>
<li>不要在训练中使用梯度检验，它只用于调试。</li>
<li>如果算法的梯度检验失败，要检查所有项，检查每一项，并试着找出<strong>bug</strong>。</li>
<li>在实施梯度检验时，如果使用正则化，请注意正则项。</li>
<li>梯度检验不能与<strong>dropout</strong>同时使用，因为每次迭代过程中，<strong>dropout</strong>会随机消除隐藏层单元的不同子集，难以计算<strong>dropout</strong>在梯度下降上的代价函数$J$。</li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/08/02/scikit-learn/" rel="prev" title="scikit-learn">
      <i class="fa fa-chevron-left"></i> scikit-learn
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/08/10/mit-cv/" rel="next" title="mit机器学习cv">
      mit机器学习cv <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-1-1"><span class="nav-number">1.</span> <span class="nav-text">Lesson 1.1</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-1-2"><span class="nav-number">2.</span> <span class="nav-text">Lesson 1.2</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-1-3"><span class="nav-number">3.</span> <span class="nav-text">Lesson 1.3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-Back-Propagation"><span class="nav-number">3.1.</span> <span class="nav-text">反向传播 Back Propagation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-1-4"><span class="nav-number">4.</span> <span class="nav-text">Lesson 1.4</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%B1%82%E8%A1%A8%E7%A4%BA%EF%BC%9F"><span class="nav-number">4.1.</span> <span class="nav-text">为什么使用深层表示？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9D%97"><span class="nav-number">4.2.</span> <span class="nav-text">搭建神经网络块</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%92%8C%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-number">4.3.</span> <span class="nav-text">参数和超参数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-2-1"><span class="nav-number">5.</span> <span class="nav-text">Lesson 2.1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E5%B7%AE-%E5%81%8F%E5%B7%AE"><span class="nav-number">5.1.</span> <span class="nav-text">方差&#x2F;偏差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95"><span class="nav-number">5.2.</span> <span class="nav-text">基本方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Regularization%EF%BC%89"><span class="nav-number">5.3.</span> <span class="nav-text">正则化（Regularization）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dropout-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">5.4.</span> <span class="nav-text">dropout 正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-number">5.5.</span> <span class="nav-text">其他正则化方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E8%BE%93%E5%85%A5%EF%BC%88Normalizing%EF%BC%89"><span class="nav-number">5.6.</span> <span class="nav-text">归一化输入（Normalizing）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1-%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%EF%BC%88Vanishing-Exploding-gradients%EF%BC%89"><span class="nav-number">5.7.</span> <span class="nav-text">梯度消失&#x2F;梯度爆炸（Vanishing &#x2F; Exploding gradients）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">5.8.</span> <span class="nav-text">神经网络权重的初始化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%95%B0%E5%80%BC%E9%80%BC%E8%BF%91-Numerical-approximation-of-gradients"><span class="nav-number">5.9.</span> <span class="nav-text">梯度的数值逼近 Numerical approximation of gradients</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-number">5.10.</span> <span class="nav-text">梯度检验的注意事项</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hui hui"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Hui hui</p>
  <div class="site-description" itemprop="description">Be happy forever!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">30</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/songtianhui" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;songtianhui" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:191098194@smail.nju.edu.cn" title="E-Mail → mailto:191098194@smail.nju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hui hui</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>



        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  

  <script async src="/js/cursor/fireworks.js"></script>


 
<script>
  var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)我藏好了哦~" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*) 被你发现啦~" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });
</script>


 
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20210126,"YYYYMMDD"));
    ages = ages.replace(/years?/, "years");
    ages = ages.replace(/months?/, "moths");
    ages = ages.replace(/days?/, "days");
    ages = ages.replace(/hours?/, "hours");
    ages = ages.replace(/minutes?/, "minutes");
    ages = ages.replace(/seconds?/, "seconds");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `I have been here waiting for you for ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


 
<script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginrootpath":"live2dw/","pluginjspath":"lib/","pluginmodelpath":"assets/ relative)","scriptfrom":"local","tagmode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":180,"height":300},"mobile":{"show":true},"react":{"opacitydefault":0.7},"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
