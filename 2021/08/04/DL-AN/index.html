<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-bounce.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"songtianhui.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="视频指路">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达深度学习笔记">
<meta property="og:url" content="https://songtianhui.github.io/2021/08/04/DL-AN/index.html">
<meta property="og:site_name" content="Songtianhui&#39;s Blog">
<meta property="og:description" content="视频指路">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/04/DL-AN/L1_week3_6.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/04/DL-AN/network.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/04/DL-AN/rnn.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/04/DL-AN/rnn-f.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/04/DL-AN/rnn_cell_backprop.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/04/DL-AN/LSTM.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/04/DL-AN/brnn.png">
<meta property="og:image" content="https://songtianhui.github.io/2021/08/04/DL-AN/drnn.png">
<meta property="article:published_time" content="2021-08-04T14:51:29.000Z">
<meta property="article:modified_time" content="2021-09-03T09:43:08.139Z">
<meta property="article:author" content="Hui hui">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://songtianhui.github.io/2021/08/04/DL-AN/L1_week3_6.png">

<link rel="canonical" href="https://songtianhui.github.io/2021/08/04/DL-AN/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>吴恩达深度学习笔记 | Songtianhui's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="/custom_css_source.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Songtianhui's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://songtianhui.github.io/2021/08/04/DL-AN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Hui hui">
      <meta itemprop="description" content="Be happy forever!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Songtianhui's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          吴恩达深度学习笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-08-04 22:51:29" itemprop="dateCreated datePublished" datetime="2021-08-04T22:51:29+08:00">2021-08-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-09-03 17:43:08" itemprop="dateModified" datetime="2021-09-03T17:43:08+08:00">2021-09-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1FT4y1E74V?from=search&amp;seid=7469215768123017337">视频指路</a></p>
<a id="more"></a>
<hr>
<h1 id="Lesson-1-1"><a href="#Lesson-1-1" class="headerlink" title="Lesson 1.1"></a>Lesson 1.1</h1><p>本章是对深度学习神经网络的一个介绍，课程的概览。</p>
<h1 id="Lesson-1-2"><a href="#Lesson-1-2" class="headerlink" title="Lesson 1.2"></a>Lesson 1.2</h1><p>本章主要讲了一下 logisitic 回归，梯度下降，在ML中已经学习了。</p>
<p>然后介绍了 <code>python</code> 中的一些知识，向量化、广播、<code>numpy</code>、jupyter notebook等，都可以 <strong>rtfm</strong>，不在此赘述。</p>
<p>唯一有一点就是符号上的表示，andrew 喜欢用 $m$ 表示样本个数，$n$ 表示特征数。我习惯 mit 的课教的 $n$ 表示样本数，$d$ 表示特征维数。</p>
<h1 id="Lesson-1-3"><a href="#Lesson-1-3" class="headerlink" title="Lesson 1.3"></a>Lesson 1.3</h1><p>本章算是一个神经网络的引入，介绍一些基本概念，浅层神经网络（shallow neural network）。</p>
<p><img data-src="L1_week3_6.png" alt></p>
<p>每个神经单元就是这么个计算过程，有输入 $x$，权重 $w$，偏移 $b$，激活函数（activation function） $h$。</p>
<ul>
<li>$z = w^T x + b$</li>
<li>$a = h(z)$</li>
</ul>
<p>符号： $a_1^{<a href="1">1</a>} $ ，右上角中括号表示层数，右上角括号中表示第几个样本，右下角表示该层第几个神经元。</p>
<p>对于一个输入样本，避免一层内的 for 循环，向量化计算：</p>
<script type="math/tex; mode=display">
z^{[i]} = W^{[i]} a^{[i-1]} + b^{[i]}\\
a^{[i]} = h(z^{[i]})</script><p>$W^{[i]}$ 是第 $i$ 层每个神经元的权重排成的矩阵。</p>
<p>举例图示：</p>
<script type="math/tex; mode=display">
\left[
        \begin{array}{c}
        z^{[1]}_{1}\\
        z^{[1]}_{2}\\
        z^{[1]}_{3}\\
        z^{[1]}_{4}\\
        \end{array}
        \right]
         =
    \overbrace{
    \left[
        \begin{array}{c}
        ...W^{[1]T}_{1}...\\
        ...W^{[1]T}_{2}...\\
        ...W^{[1]T}_{3}...\\
        ...W^{[1]T}_{4}...
        \end{array}
        \right]
        }^{W^{[1]}}
        *
    \overbrace{
    \left[
        \begin{array}{c}
        x_1\\
        x_2\\
        x_3\\
        \end{array}
        \right]
        }^{input}
        +
    \overbrace{
    \left[
        \begin{array}{c}
        b^{[1]}_1\\
        b^{[1]}_2\\
        b^{[1]}_3\\
        b^{[1]}_4\\
        \end{array}
        \right]
        }^{b^{[1]}}</script><p>对于所有样本，避免 for 遍历样本，向量化计算：</p>
<script type="math/tex; mode=display">
Z^{[i]} = W^{[i]}A^{[i-1]} + b^{[i]}\\
A^{[i]} = h(Z^{[i]})</script><p>其中 $Z^{[i]},A^{[i]}$ 是第 $i$ 层所有样本输出排成的矩阵。</p>
<p>激活函数：<strong>sigmoid</strong>（主要在二分类）, <strong>tanh</strong>（比 sigmoid 常用），<strong>ReLU</strong>（在神经网络中很常用），Leaky ReLU。</p>
<p>前两个有梯度消失的风险。</p>
<p>这里提到一个很重要的问题就是为什么要使用非线性函数而不是直接 $a = z$。因为全用线性激活函数（identity）会使神经网络退化成一个单层模型。</p>
<h2 id="反向传播-Back-Propagation"><a href="#反向传播-Back-Propagation" class="headerlink" title="反向传播 Back Propagation"></a>反向传播 Back Propagation</h2><p>也就是神经网络的梯度下降（Gradient Descent）。比较重要，理解一下推倒，本质上是函数求导的链式法则。</p>
<p>代价函数：</p>
<script type="math/tex; mode=display">
J(W, b) = \dfrac{1}{m}\sum\limits_{i= 1}^{m}L(\hat{y}, y)</script><p>当参数初始化成某些值后，每次梯度下降都会循环计算以下预测值：$\hat{y}^{(i)},(i=1,2,…,m)$。</p>
<p>有 $dW^{[i]} = \dfrac{\partial J}{\partial W^{[i]}}$, $d b^{[i]} = \dfrac{\partial J}{\partial b^{[i]}}$。</p>
<p>在梯度下降时每一次更新：$W^{[i]}\implies{W^{[i]} - \eta dW^{[i]}},b^{[i]}\implies{b^{[i]} -\eta db^{[i]}}$, $\eta$ 为步长。</p>
<p>反向传播时，就是一个链式求导：</p>
<script type="math/tex; mode=display">
\underbrace{
    \left.
    \begin{array}{l}
    x \\
    w \\
    b 
    \end{array}
    \right\}
    }_{dw=dz \cdot x, db =dz}
    \impliedby \underbrace{z=w^Tx+b}_{dz=da\cdot g^{'}(z),
    g(z)=\sigma(z),
    \frac{dL}{dz}} = \frac{dL}{da} \cdot \frac{da}{dz},
    \frac{d}{ dz} g(z)=g^{'}(z)
    \impliedby \underbrace{a = \sigma(z) 
    \impliedby L(a,y)}_{da=\frac{d}{da}L\left(a,y \right)=(-y\log{\alpha} - (1 - y)\log(1 - a))^{'}=-\frac{y}{a} + \frac{1 - y}{1 - a} }</script><p>所以有：</p>
<p>$dz^{[L]} = A^{[L]} - Y$</p>
<p>$dW^{[i]} = \dfrac{1}{m} dZ^{[i]}A^{[i-1]T}$</p>
<p>$db^{[i]} = \dfrac{1}{m}$ <code>np.sum(dZ^&#123;[i]&#125;, axis=1)​</code></p>
<script type="math/tex; mode=display">
dz^{[i]} = \underbrace{W^{[i + 1]T} dz^{[i+1]}}_{(n^{[i]},m)}\quad \times  \underbrace{g^{[i]'}}_{activation \; function \; of \; hidden \; layer}\times  \quad\underbrace{(z^{[i]})}_{(n^{[1]},m)}</script><ul>
<li>随机初始化，不要初始化成相同的参数。</li>
</ul>
<h1 id="Lesson-1-4"><a href="#Lesson-1-4" class="headerlink" title="Lesson 1.4"></a>Lesson 1.4</h1><p>本章介绍深层神经网络，主要就是把前一章讲的只有两层的网络更推广一下，而我们在上一章其实已经推广过了。</p>
<h2 id="为什么使用深层表示？"><a href="#为什么使用深层表示？" class="headerlink" title="为什么使用深层表示？"></a>为什么使用深层表示？</h2><p>深度神经网络的这许多隐藏层中，较早的前几层能学习一些低层次的简单特征，等到后几层，就能把简单的特征结合起来，去探测更加复杂的东西。</p>
<p>深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。</p>
<blockquote>
<p>说实话，我认为“深度学习”这个名字挺唬人的，这些概念以前都统称为有很多隐藏层的神经网络，但是深度学习听起来多高大上，太深奥了，对么？这个词流传出去以后，这是神经网络的重新包装或是多隐藏层神经网络的重新包装，激发了大众的想象力。    ——Andrew</p>
</blockquote>
<h2 id="搭建神经网络块"><a href="#搭建神经网络块" class="headerlink" title="搭建神经网络块"></a>搭建神经网络块</h2><p>其实就是对于每一层，权重矩阵，偏移值，激活函数，在前向传播的时候缓存（cache）好 $z,a$ 等值，用反向传播时计算 $dW,db$ 等。</p>
<p>就放一张老师的板书吧（</p>
<p><img data-src="network.png" alt="building blocks"></p>
<h2 id="参数和超参数"><a href="#参数和超参数" class="headerlink" title="参数和超参数"></a>参数和超参数</h2><p>算法中的<strong>learning rate</strong> $a$（学习率）、<strong>iterations</strong>(梯度下降法循环的数量)、$L$（隐藏层数目）、$n^{[l]}$（隐藏层单元数目）、<strong>choice of activation function</strong>（激活函数的选择）都需要你来设置，这些数字实际上控制了最后的参数$W$和$b$的值，所以它们被称作超参数（Hyperparameter）。</p>
<p>如何寻找超参数：走<strong>Idea—Code—Experiment—Idea</strong>这个循环，尝试各种不同的参数，实现模型并观察是否成功，然后再迭代。</p>
<blockquote>
<p>应用深度学习领域，一个很大程度基于经验的过程，凭经验的过程通俗来说，就是试直到你找到合适的数值。</p>
<p>如果你所解决的问题需要很多年时间，只要经常试试不同的超参数，勤于检验结果，看看有没有更好的超参数数值，相信你慢慢会得到设定超参数的直觉，知道你的问题最好用什么数值。</p>
</blockquote>
<hr>
<h1 id="Lesson-2-1"><a href="#Lesson-2-1" class="headerlink" title="Lesson 2.1"></a>Lesson 2.1</h1><p>本章主要讲改善神经网络，超参数调试、正则化等内容。</p>
<ul>
<li>当我们有百万量级以上的数据，可以拿 99% 以上的数据来进行训练，几万条用来交叉验证（dev）和测试就可以了。</li>
</ul>
<h2 id="方差-偏差"><a href="#方差-偏差" class="headerlink" title="方差/偏差"></a>方差/偏差</h2><ul>
<li>高偏差（<strong>high bias</strong>），欠拟合（<strong>underfitting</strong>）。</li>
<li>高方差（<strong>high variance</strong>），过拟合（<strong>overfitting</strong>）。</li>
</ul>
<p>通过训练集和验证集误差判断：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Training set error</th>
<th style="text-align:center">Dev set error</th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1%</td>
<td style="text-align:center">15%</td>
<td style="text-align:center">high variance</td>
</tr>
<tr>
<td style="text-align:center">15%</td>
<td style="text-align:center">16%</td>
<td style="text-align:center">high bias</td>
</tr>
<tr>
<td style="text-align:center">15%</td>
<td style="text-align:center">30%</td>
<td style="text-align:center">high variance &amp; bias</td>
</tr>
<tr>
<td style="text-align:center">0.5%</td>
<td style="text-align:center">1%</td>
<td style="text-align:center">low variance &amp; bias</td>
</tr>
</tbody>
</table>
</div>
<h2 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h2><blockquote>
<p>初始模型训练完成后，我首先要知道算法的偏差高不高，如果偏差较高，试着评估训练集或训练数据的性能。如果偏差的确很高，甚至无法拟合训练集，那么你要做的就是选择一个新的网络，比如含有更多隐藏层或者隐藏单元的网络，或者花费更多时间来训练网络，或者尝试更先进的优化算法，后面我们会讲到这部分内容。</p>
</blockquote>
<ul>
<li>只要正则适度，通常构建一个更大的网络便可以，在不影响方差的同时减少偏差，而采用更多数据通常可以在不过多影响偏差的同时减少方差。</li>
<li>训练网络，选择网络或者准备更多数据，现在我们有工具可以做到在减少偏差或方差的同时，不对另一方产生过多不良影响。</li>
</ul>
<h2 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h2><p>深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，正则化通常有助于避免过拟合或减少你的网络误差。</p>
<p>就是在代价函数里加一个正则化项，一般用 $\dfrac{\lambda}{2m}$乘以$w$范数的平方,其中$\left| w \right|_2^2$是$w$的欧几里德范数的平方，$L2$ 正则化。</p>
<p>神经网络中的正则项就是为$\dfrac{\lambda }{2m}\sum\limits_{l = 1}^{L}| W^{[l]}|^{2}$，我们称${||W^{\left[l\right]}||}^{2}$为范数平方，这个矩阵范数${||W^{\left[l\right]}||}^{2}$（即平方范数），被定义为矩阵中所有元素的平方求和。该矩阵范数被称作“弗罗贝尼乌斯范数”，用下标$F$标注。</p>
<p>带正则化的梯度下降中，对$W^{[l]}$的偏导数，把$W^{[l]}$替换为$W^{[l]}$减去学习率乘以$dW$。现在我们要做的就是给$dW$加上这一项$\dfrac {\lambda}{m}W^{[l]}$，然后计算这个更新项，使用新定义的$dW^{[l]}$，它的定义含有相关参数代价函数导数和，以及最后添加的额外正则项。</p>
<script type="math/tex; mode=display">
\begin{aligned}
W^{[l]} :&= W^{[l]}  - \alpha \times \left[(\text{from backpap}) + \dfrac{\lambda}{m}W^{[l]}\right]\\ &= (1 - \frac{\alpha \lambda}{m}) W^{[l]} - \alpha \times (\text{from backpap})
\end{aligned}</script><p><em>正则化预防过拟合的原因：极限思想，当lambda很大权重为0，退化成欠拟合，有个right fit 的中间态。</em></p>
<h2 id="dropout-正则化"><a href="#dropout-正则化" class="headerlink" title="dropout 正则化"></a>dropout 正则化</h2><p>就是随机消掉一些神经元。。。</p>
<ul>
<li><strong>inverted dropout</strong>（反向随机失活）<ul>
<li>首先要定义向量$d$，$d^{[3]}$表示网络第三层的<strong>dropout</strong>向量：<code>d3 = np.random.rand(a3.shape[0],a3.shape[1])</code> 。</li>
<li>然后看它是否小于某数，我们称之为<strong>keep-prob</strong>，<strong>keep-prob</strong>是一个具体数字，它表示保留某个隐藏单元的概率。</li>
<li>接下来要做的就是从第三层中获取$a^{[3]}$，$a^{[3]}$含有要计算的激活函数，$a^{[3]}$等于上面的$a^{[3]}$乘以 $d^{[3]}$，就是把 $d$ 中 0 对应位置的数归零。（$d$ 实际上是一个布尔数组）</li>
<li>最后，我们向外扩展$a^{[3]}$，用它除以<strong>keep-prob</strong>参数。</li>
</ul>
</li>
</ul>
<p>显然在测试阶段，我们不使用<strong>dropout</strong>。要同时在 <strong>fpp</strong> 和 <strong>bpp</strong> 中使用 dropout。</p>
<p><em>dropout 预防过拟合的原因，dropout 的功能类似于$L2$正则化，与$L2$正则化不同的是应用方式不同会带来一点点小变化，甚至更适用于不同的输入范围。</em></p>
<ul>
<li><p>如果你担心某些层比其它层更容易发生过拟合，可以把某些层的<strong>keep-prob</strong>值设置得比其它层更低，缺点是为了使用交叉验证，你要搜索更多的超级参数，另一种方案是在一些层上应用<strong>dropout</strong>，而有些层不用<strong>dropout</strong>，应用<strong>dropout</strong>的层只含有一个超级参数，就是<strong>keep-prob</strong>。</p>
</li>
<li><p>它在其它领域应用得比较少，主要存在于计算机视觉领域，因为我们通常没有足够的数据，所以一直存在过拟合。</p>
</li>
<li><strong>dropout</strong>一大缺点就是代价函数$J$不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。</li>
</ul>
<h2 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h2><ul>
<li>数据扩增</li>
<li><strong>early stopping</strong><ul>
<li>缺点是不能独立处理梯度下降和优化代价函数。</li>
</ul>
</li>
</ul>
<h2 id="归一化输入（Normalizing）"><a href="#归一化输入（Normalizing）" class="headerlink" title="归一化输入（Normalizing）"></a>归一化输入（Normalizing）</h2><p>训练神经网络，其中一个加速训练的方法就是归一化输入。归一化需要两个步骤：</p>
<ol>
<li>零均值<ul>
<li>$\mu = \frac{1}{m}\sum\limits_{i =1}^{m}x^{(i)}$，它是一个向量，$x$ 等于每个训练数据 $x$ 减去 $\mu$，意思是移动训练集，直到它完成零均值化。 </li>
</ul>
</li>
<li>归一化方差<ul>
<li>$ \sigma^{2}= \frac{1}{m}\sum\limits_{i =1}^{m}(x^{(i)})^{2} $，$\sigma^{2}$是一个向量，它的每个特征都有方差，把所有数据除以向量$\sigma^{2}$。</li>
</ul>
</li>
</ol>
<h2 id="梯度消失-梯度爆炸（Vanishing-Exploding-gradients）"><a href="#梯度消失-梯度爆炸（Vanishing-Exploding-gradients）" class="headerlink" title="梯度消失/梯度爆炸（Vanishing / Exploding gradients）"></a>梯度消失/梯度爆炸（Vanishing / Exploding gradients）</h2><p>神经网络层数多了，激活函数就会以指数级递增或递减。</p>
<h2 id="神经网络权重的初始化"><a href="#神经网络权重的初始化" class="headerlink" title="神经网络权重的初始化"></a>神经网络权重的初始化</h2><p>针对梯度消失/爆炸，有一个方案就是更谨慎地选择随机初始化参数。</p>
<p>$z = w_{1}x_{1} + w_{2}x_{2} + \ldots +w_{n}x_{n}$，为了预防$z$值过大或过小，希望每项值更小，最合理的方法就是设置$w_{i}=\frac{1}{n}$，$n$表示神经元的输入特征数量。</p>
<p>实际上，你要做的就是设置某层权重矩阵 <code>W[l] = np.random.randn(shape) * np.sqrt(1 / n[l-1])</code>，$n^{[l - 1]}$ 就是我喂给第$l$层神经单元的数量（即第$l-1$层神经元数量）。</p>
<p>如果你是用的是<strong>Relu</strong>激活函数，而不是$\frac{1}{n}$，方差设置为$\frac{2}{n}$，效果会更好。</p>
<p>对于<strong>tanh</strong>函数来说，用$\sqrt{\frac{1}{n^{[l-1]}}}$。</p>
<h2 id="梯度的数值逼近-Numerical-approximation-of-gradients"><a href="#梯度的数值逼近-Numerical-approximation-of-gradients" class="headerlink" title="梯度的数值逼近 Numerical approximation of gradients"></a>梯度的数值逼近 Numerical approximation of gradients</h2><p>就是导数定义，双边误差，即$\frac{f\left(\theta + \varepsilon \right) - f(\theta -\varepsilon)}{2\varepsilon}$。</p>
<p>先将所有的参数 $W, b$ 展开成一个大向量 $\theta$，在<strong>bpp</strong>中，算完梯度之后所有的梯度 $dW, db$ 就是 $d\theta$。</p>
<p>然后比较 $d\theta_{\text{approx}}\left[i \right] = \frac{J\left( \theta_{1},\theta_{2},\ldots\theta_{i} + \varepsilon,\ldots \right) - J\left( \theta_{1},\theta_{2},\ldots\theta_{i} - \varepsilon,\ldots \right)}{2\varepsilon}$ 和 $d\theta[i]$ 的值接不接近。</p>
<p>就计算它们的欧式距离再归一化，$\dfrac{||d\theta_{\text{approx}} -d\theta||_{2}}{||d\theta_{\text{approx}}||_2 + ||d\theta||_2}$。计算得到的值为$10^{-7}$或更小，这就很好；如果它的值在$10^{-5}$范围内，就要小心了，也许这个值没问题，但再次检查这个向量的所有项，确保没有一项误差过大，可能这里有<strong>bug</strong>。如果比$10^{-3}$大很多，就会很担心是否存在<strong>bug</strong>，这时应该仔细检查所有$\theta$项，看是否有一个具体的$i$值，使得$d\theta_{\text{approx}}\left[i \right]$与$ d\theta[i]$大不相同，并用它来追踪一些求导计算是否正确。</p>
<h2 id="梯度检验的注意事项"><a href="#梯度检验的注意事项" class="headerlink" title="梯度检验的注意事项"></a>梯度检验的注意事项</h2><ol>
<li>不要在训练中使用梯度检验，它只用于调试。</li>
<li>如果算法的梯度检验失败，要检查所有项，检查每一项，并试着找出<strong>bug</strong>。</li>
<li>在实施梯度检验时，如果使用正则化，请注意正则项。</li>
<li>梯度检验不能与<strong>dropout</strong>同时使用，因为每次迭代过程中，<strong>dropout</strong>会随机消除隐藏层单元的不同子集，难以计算<strong>dropout</strong>在梯度下降上的代价函数$J$。</li>
</ol>
<h1 id="Lesson-2-2"><a href="#Lesson-2-2" class="headerlink" title="Lesson 2.2"></a>Lesson 2.2</h1><p>本节课主要讲优化算法，也就是我们如何更新参数。</p>
<h2 id="Mini-batch-梯度下降"><a href="#Mini-batch-梯度下降" class="headerlink" title="Mini-batch 梯度下降"></a>Mini-batch 梯度下降</h2><p>你可以把训练集分割为小一点的子集训练，这些子集被取名为<strong>mini-batch</strong>，每个子集记作 $X^{\{i\}}$。就是把原来梯度下降时代入整个训练集改成代入一个mini-batch，然后多梯度下降几次。</p>
<p>使用<strong>mini-batch</strong>梯度下降法，如果作出成本函数在整个过程中的图，则并不是每次迭代都是下降的，特别是在每次迭代中，你要处理的是$X^{\{t\}}$和$Y^{\{ t\}}$。如果要作出成本函数$J$的图，你很可能会看到这样的结果，走向朝下，但有更多的噪声。</p>
<p>需要决定的变量之一是<strong>mini-batch</strong>的大小。首先，如果训练集较小，直接使用<strong>batch</strong>梯度下降法。样本数目较大的话，一般的<strong>mini-batch</strong>大小为64到512，考虑到电脑内存设置和使用的方式，如果<strong>mini-batch</strong>大小是2的$n$次方，代码会运行地快一些。</p>
<h2 id="指数加权平均数-Exponentially-weighted-averages"><a href="#指数加权平均数-Exponentially-weighted-averages" class="headerlink" title="指数加权平均数 Exponentially weighted averages"></a>指数加权平均数 Exponentially weighted averages</h2><p>递推式：</p>
<script type="math/tex; mode=display">
v_t = \beta v_{t - 1} + (1 - \beta)\theta_t</script><p>如果我们将其展开，这就是一个加权平均，是从 $0$ 到 $t$ 每个 $\theta_i$ 的平均，越远权重越小。</p>
<p>考虑 $\beta^{x} = \frac{1}{e}$，这个算的大约就是 $x$ 天的平均数。</p>
<p>指数加权平均数公式的好处之一在于，它占用极少内存，电脑内存中只占用一行数字而已，然后把最新数据代入公式，不断覆盖就可以了。</p>
<p>不过有可能会遇到 <strong>偏差修正（bias corrections）的问题</strong></p>
<p>因为我们取 $v_0 = 0$，所以会使得 $i$ 较小时 $v_i$ 所占权重都很小，估算不准确。</p>
<p>我们可以在估测初期，不用 $v_t$，而是 $\dfrac{v_t}{1 - \beta^t}$。</p>
<p>不过在机器学习中，在计算指数加权平均数的大部分时候，大家不在乎执行偏差修正。</p>
<h2 id="动量梯度下降法-Gradient-descent-with-Momentum"><a href="#动量梯度下降法-Gradient-descent-with-Momentum" class="headerlink" title="动量梯度下降法 Gradient descent with Momentum"></a>动量梯度下降法 Gradient descent with Momentum</h2><p>有一种算法叫做<strong>Momentum</strong>，或者叫做动量梯度下降法，运行速度几乎总是快于标准的梯度下降算法，基本的想法就是计算梯度的指数加权平均数，并利用该梯度更新你的权重。</p>
<p>计算动量：</p>
<script type="math/tex; mode=display">
v_{dW} = \beta v_{dW} + (1 - \beta) dW</script><script type="math/tex; mode=display">
v_{db} = \beta v_{db} + (1 - \beta) db</script><p>再更新参数：</p>
<script type="math/tex; mode=display">
W := W - \alpha v_{dW}</script><script type="math/tex; mode=display">
b := b - \alpha v_{db}</script><p>这样就可以减缓梯度下降的幅度。<em>它们能够最小化碗状函数，这些微分项，想象它们为从山上往下滚的一个球，提供了加速度，<strong>Momentum</strong>项相当于速度。</em></p>
<p>所以有两个超参数，学习率 $a$ 以及参数 $\beta$，$\beta$ 控制着指数加权平均数。$\beta$ 最常用的值是0.9，是很棒的鲁棒数。</p>
<p>有一个版本是 $v_{dW} = \beta v_{dW} + dW$，本质上没有区别。</p>
<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p><strong>root mean square prop</strong>算法，它也可以加速梯度下降，通过加快损失下降的方向，减缓无关方向，减少摆动。</p>
<script type="math/tex; mode=display">
S_{dW}= \beta S_{dW} + (1 -\beta) (dW)^{2}</script><script type="math/tex; mode=display">
S_{db}= \beta S_{db} + (1 - \beta)(db)^{2}</script><p>再更新参数：</p>
<script type="math/tex; mode=display">
W:= W -\alpha \dfrac{dW}{\sqrt{S_{dW}}}</script><script type="math/tex; mode=display">
b:=b -\alpha \dfrac{db}{\sqrt{S_{db}}}</script><p>为了确保数值稳定，在实际操练的时候，要在分母上加上一个很小很小的$\varepsilon$，$\varepsilon$是多少没关系，$10^{-8}$是个不错的选择.</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>把前面两个缝起来。</p>
<p>首先初始化：$v_{dW} = 0$，$S_{dW} =0$，$v_{db} = 0$，$S_{db} =0$。</p>
<script type="math/tex; mode=display">
v_{dW}= \beta_{1}v_{dW} + ( 1 - \beta_{1})dW</script><script type="math/tex; mode=display">
v_{db}= \beta_{1}v_{db} + ( 1 -\beta_{1} )db</script><script type="math/tex; mode=display">
S_{dW}=\beta_{2}S_{dW} + ( 1 - \beta_{2})(dW)^{2}</script><script type="math/tex; mode=display">
S_{db} =\beta_{2}S_{db} + ( 1 - \beta_{2} )(db)^{2}</script><p>一般使用<strong>Adam</strong>算法的时候，要计算偏差修正，$v_{dW}^{\text{corrected}}$，修正也就是在偏差修正之后：</p>
<script type="math/tex; mode=display">
v_{dW}^{\text{corrected}}= \dfrac{v_{dW}}{1 - \beta_{1}^{t}}</script><script type="math/tex; mode=display">
v_{db}^{\text{corrected}} =\dfrac{v_{db}}{1 -\beta_{1}^{t}}</script><script type="math/tex; mode=display">
S_{dW}^{\text{corrected}} =\dfrac{S_{dW}}{1 - \beta_{2}^{t}}</script><script type="math/tex; mode=display">
S_{db}^{\text{corrected}} =\dfrac{S_{db}}{1 - \beta_{2}^{t}}</script><p>最后更新权重：</p>
<script type="math/tex; mode=display">
W:= W - \dfrac{\alpha v_{dW}^{\text{corrected}}}{\sqrt{S_{dW}^{\text{corrected}}} +\varepsilon}</script><script type="math/tex; mode=display">
b:=b - \frac{\alpha v_{\text{db}}^{\text{corrected}}}{\sqrt{S_{\text{db}}^{\text{corrected}}} +\varepsilon}</script><p><strong>Adam</strong> 是一种极其常用的学习算法，被证明能有效适用于不同神经网络，适用于广泛的结构。</p>
<p>本算法中有很多超参数，超参数学习率$a$很重要，也经常需要调试，你可以尝试一系列值，然后看哪个有效。$\beta_{1}$常用的缺省值为0.9。超参数$\beta_{2}$，<strong>Adam</strong>论文作者，也就是<strong>Adam</strong>算法的发明者，推荐使用0.999。关于$\varepsilon$的选择其实没那么重要，<strong>Adam</strong>论文的作者建议$\varepsilon$为$10^{-8}$。</p>
<h2 id="学习率衰减-Learning-rate-decay"><a href="#学习率衰减-Learning-rate-decay" class="headerlink" title="学习率衰减 Learning rate decay"></a>学习率衰减 Learning rate decay</h2><p>慢慢减少$a$的本质在于，在学习初期，你能承受较大的步伐，但当开始收敛的时候，小一些的学习率能让你步伐小一些。</p>
<p>$a= \dfrac{1}{1 + decayrate \times \text{epoch}\text{-num}}a_{0}$，（<strong>decay-rate</strong>称为衰减率，<strong>epoch-num</strong>为代数，$\alpha_{0}$为初始学习率），注意这个衰减率是另一个需要调整的超参数。</p>
<p>人们用到的其它公式有$a =\dfrac{k}{\sqrt{\text{epoch-num}}}a_{0}$或者$a =\dfrac{k}{\sqrt{t}}a_{0}$（$t$为<strong>mini-batch</strong>的数字）。</p>
<h2 id="局部最优问题-Local-optima"><a href="#局部最优问题-Local-optima" class="headerlink" title="局部最优问题 Local optima"></a>局部最优问题 Local optima</h2><p>在深度学习研究早期，人们总是担心优化算法会困在极差的局部最优，不过随着深度学习理论不断发展，我们对局部最优的理解也发生了改变。</p>
<p>事实上，如果你要创建一个神经网络，通常梯度为零的点并不是这个图中的局部最优点，实际上成本函数的零梯度点，通常是鞍点。</p>
<p>意思就是一个最优点需要所有维度都是极小，在现在高维特征下很难遇到，所以碰到的所谓局部最优一般都不是最优，都可以跑出来。</p>
<h1 id="Lesson-2-3"><a href="#Lesson-2-3" class="headerlink" title="Lesson 2.3"></a>Lesson 2.3</h1><p>本章主要讲了超参数调试，归一化和深度学习框架。</p>
<h2 id="调试处理-Tuning-process"><a href="#调试处理-Tuning-process" class="headerlink" title="调试处理 Tuning process"></a>调试处理 Tuning process</h2><p>画格子取点。</p>
<p>超参数范围，对数尺。</p>
<p><strong>Pandas</strong>，小模型，每天逐渐变化超参数。</p>
<p><strong>Caviar</strong>，大模型，多个参数同时跑几天。</p>
<h2 id="归一化激活函数-Normalizing-activation"><a href="#归一化激活函数-Normalizing-activation" class="headerlink" title="归一化激活函数 Normalizing activation"></a>归一化激活函数 Normalizing activation</h2><p>就是 <strong>Batch 归一化</strong>，会使你的参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定。</p>
<p>实践中，经常做的是归一化$z^{[i]}$。</p>
<script type="math/tex; mode=display">
\mu = \dfrac{1}{m} \sum\limits_i z^{(i)}</script><script type="math/tex; mode=display">
\sigma^2 = \dfrac{1}{m} \sum\limits_i (z_i - \mu)^2</script><script type="math/tex; mode=display">
z_{norm}^{(i)} = \dfrac{z^{(i)} - \mu}{\sqrt{\sigma^2 + \varepsilon}}</script><p>所以现在我们已把这些$z$值标准化，化为含平均值0和标准单位方差，所以$z$的每一个分量都含有平均值0和方差1，但我们不想让隐藏单元总是含有平均值0和方差1，也许隐藏单元有了不同的分布会有意义，所以我们所要做的就是计算 $\tilde{z}^{(i)} = \gamma z_{norm}^{(i)} + \beta$，这里$\gamma$和$\beta$是你模型的学习参数，作用是可以随意设置${\tilde{z}}^{(i)}$的平均值。</p>
<p><em>应用Batch归一化了一些隐藏单元值中的平均值和方差，不过训练输入和这些隐藏单元值的一个区别是，你也许不想隐藏单元值必须是平均值0和方差1。它真正的作用是，使隐藏单元值的均值和方差标准化，即$z^{(i)}$有固定的均值和方差，均值和方差可以是0和1，也可以是其它值，它是由$\gamma$和$\beta$两参数控制的。</em></p>
<p>至于如何将 <em>BN</em> 层放进神经网络——tensorflow(</p>
<h2 id="Batch-Norm-为什么有用"><a href="#Batch-Norm-为什么有用" class="headerlink" title="Batch Norm 为什么有用"></a>Batch Norm 为什么有用</h2><p>通过归一化所有的输入特征值$x$，以获得类似范围的值，可以加速学习，不仅仅对于这里的输入值，还有隐藏单元的值。</p>
<p><strong>Batch</strong>归一化有效的第二个原因是，它可以使权重比你的网络更滞后或更深层。对于网络的泛化能力，浅层网络不一定能做的很好，所以我们尝试改变数据的分布，有个有点怪的名字“<strong>Covariate shift</strong>”。如果你已经学习了$x$到$y$ 的映射，如果$x$ 的分布改变了，那么你可能需要重新训练你的学习算法。这种做法同样适用于，如果真实函数由$x$ 到$y$ 映射保持不变。</p>
<p>按我的理解，就是前一层对于后一层的影响。当前一层的分布变化之后，后一层就要面临 <strong>Covariate shift</strong> 的问题，就会不稳定。而 Batch 归一化做的就是它减少了这些隐藏值分布变化的数量，使分布更稳定，神经网络的之后层就会有更坚实的基础。它限制了在前层的参数更新，会影响数值分布的程度，在后一层看到的这种情况，因此得到学习。</p>
<p><em>即使使输入分布改变了一些，它会改变得更少。它做的是当前层保持学习，当改变时，迫使后层适应的程度减小了，你可以这样想，它减弱了前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习，稍稍独立于其它层，这有助于加速整个网络的学习。</em></p>
<p>还有轻微的正则化的作用。因为均值和方差有一点小噪音，因为它只是由一小部分数据估计得出的。</p>
<h2 id="Softmax-回归"><a href="#Softmax-回归" class="headerlink" title="Softmax 回归"></a>Softmax 回归</h2><p>有一种<strong>logistic</strong>回归的一般形式，叫做<strong>Softmax</strong>回归，能让你在试图识别某一分类时做出预测，或者说是多种分类中的一个，不只是识别两个分类。</p>
<script type="math/tex; mode=display">
t=e^{z^{[l]}}</script><script type="math/tex; mode=display">
a_{i}^{[l]} = \dfrac{t_{i}}{\sum\limits_i t_{i}}</script><p>输出的 $a_i$ 就是第 $i$ 类的概率。</p>
<p>训练 softmax 分类器用的损失    函数一般是 $L(\hat{y},y ) = - \sum\limits_{j}y_{j}\log{\hat{y}_{j}}$。</p>
<h2 id="深度学习框架"><a href="#深度学习框架" class="headerlink" title="深度学习框架"></a>深度学习框架</h2><ul>
<li>Caffe/Caffe2</li>
<li>NCTK</li>
<li>DL4J</li>
<li>Keras</li>
<li>Lasagne</li>
<li>mxnet</li>
<li>PaddlePaddle</li>
<li>TensorFlow</li>
<li>Theano</li>
<li>Torch</li>
</ul>
<h1 id="Lesson-3-1"><a href="#Lesson-3-1" class="headerlink" title="Lesson 3.1"></a>Lesson 3.1</h1><p>本章主要介绍机器学习策略。很多事思想意识上的东西，可能很多照搬原话（</p>
<h2 id="正交化-Orthogonalization"><a href="#正交化-Orthogonalization" class="headerlink" title="正交化 Orthogonalization"></a>正交化 Orthogonalization</h2><p>意思大概就是把所有特征正交到几个相互独立的特征上。</p>
<blockquote>
<p>在机器学习中，如果你可以观察你的系统，然后说这一部分是错的，它在训练集上做的不好、在开发集上做的不好、它在测试集上做的不好，或者它在测试集上做的不错，但在现实世界中不好，这就很好。必须弄清楚到底是什么地方出问题了，然后我们刚好有对应的旋钮，或者一组对应的旋钮，刚好可以解决那个问题，那个限制了机器学习系统性能的问题。</p>
</blockquote>
<h2 id="单一数字评估指标-Single-number-evaluation-metric"><a href="#单一数字评估指标-Single-number-evaluation-metric" class="headerlink" title="单一数字评估指标 Single number evaluation metric"></a>单一数字评估指标 Single number evaluation metric</h2><p>无论是调整超参数，或者是尝试不同的学习算法，或者在搭建机器学习系统时尝试不同手段，如果你有一个单实数评估指标，进展会快得多，它可以快速告诉你，新尝试的手段比之前的手段好还是差。所以当团队开始进行机器学习项目时，推荐为问题设置一个单实数评估指标（F1 分数，平均值）。</p>
<p>有一个开发集，加上单实数评估指标，迭代速度肯定会很快，它可以加速改进机器学习算法的迭代过程。</p>
<h2 id="满足和优化指标-Satisficing-and-optimizing-metrics"><a href="#满足和优化指标-Satisficing-and-optimizing-metrics" class="headerlink" title="满足和优化指标 Satisficing and optimizing metrics"></a>满足和优化指标 Satisficing and optimizing metrics</h2><p>要把顾及到的所有事情组合成单实数评估指标有时并不容易，在那些情况里，有时候设立满足和优化指标是很重要的。</p>
<p>优化指标，想要准确度最大化，想做的尽可能准确。</p>
<p>满足指标，意思是它必须足够好，达到标准之后，不那么在乎这指标有多好。</p>
<p>通过定义优化和满足指标，就可以提供一个明确的方式，去选择“最好的”分类器。</p>
<h2 id="训练-开发-测试集划分-train-dev-test-distributions"><a href="#训练-开发-测试集划分-train-dev-test-distributions" class="headerlink" title="训练/开发/测试集划分 train/dev/test distributions"></a>训练/开发/测试集划分 train/dev/test distributions</h2><p>开发（<strong>dev</strong>）集也叫做开发集（<strong>development set</strong>），有时称为保留交叉验证集（<strong>hold out cross validation set</strong>）。然后，机器学习中的工作流程是，尝试很多思路，用训练集训练不同的模型，然后使用开发集来评估不同的思路，然后选择一个，然后不断迭代去改善开发集的性能，直到最后可以得到一个令你满意的成本，然后你再用测试集去评估。</p>
<p>让开发集和测试集来自同一分布，通过将所有数据随机洗牌。</p>
<p>大小 98:1:1.</p>
<p><em>处理机器学习问题时，应该把它切分成独立的步骤。一步是弄清楚如何定义一个指标来衡量你想做的事情的表现，然后我们可以分开考虑如何改善系统在这个指标上的表现。要把机器学习任务看成两个独立的步骤，用目标这个比喻，第一步就是设定目标。所以要定义你要瞄准的目标，这是完全独立的一步，这是你可以调节的一个旋钮。如何设立目标是一个完全独立的问题，把它看成是一个单独的旋钮，可以调试算法表现的旋钮，如何精确瞄准，如何命中目标，定义指标是第一步。</em></p>
<p><em>然后第二步要做别的事情，在逼近目标的时候，也许学习算法针对某个成本函数优化，要最小化训练集上的损失。可以做的其中一件事是，修改这个，为了引入权重，也许最后需要修改这个归一化常数。</em></p>
<p>如何定义$J$并不重要，关键在于正交化的思路。将定义指标看成一步，然后在定义了指标之后，你才能想如何优化系统来提高这个指标评分。比如改变你神经网络要优化的成本函数$J$。</p>
<blockquote>
<p>所以我的建议是，即使你无法定义出一个很完美的评估指标和开发集，你直接快速设立出来，然后使用它们来驱动你们团队的迭代速度。如果在这之后，你发现选的不好，你有更好的想法，那么完全可以马上改。对于大多数团队，我建议最好不要在没有评估指标和开发集时跑太久，因为那样可能会减慢你的团队迭代和改善算法的速度。</p>
</blockquote>
<h2 id="为什么是人的表现-Why-human-level-performance"><a href="#为什么是人的表现-Why-human-level-performance" class="headerlink" title="为什么是人的表现 Why human-level performance?"></a>为什么是人的表现 Why human-level performance?</h2><p>在过去的几年里，更多的机器学习团队一直在讨论如何比较机器学习系统和人类的表现。</p>
<blockquote>
<p>我认为有两个主要原因，首先是因为深度学习系统的进步，机器学习算法突然变得更好了。在许多机器学习的应用领域已经开始见到算法已经可以威胁到人类的表现了。其次，事实证明，当你试图让机器做人类能做的事情时，可以精心设计机器学习系统的工作流程，让工作流程效率更高，所以在这些场合，比较人类和机器是很自然的，或者你要让机器模仿人类的行为。</p>
</blockquote>
<p>当这个算法表现比人类更好时，进展和精确度的提升就变得更慢了。也许它还会越来越好，但是在超越人类水平之后，它还可以变得更好，但性能增速，准确度上升的速度这个斜率，会变得越来越平缓，我们都希望能达到理论最佳性能水平。随着时间的推移，当继续训练算法时，可能模型越来越大，数据越来越多，但是性能无法超过某个理论上限，这就是所谓的贝叶斯最优错误率（<strong>Bayes optimal error</strong>）。</p>
<p>所以贝叶斯最优错误率一般认为是理论上可能达到的最优错误率，就是说没有任何办法设计出一个$x$到$y$的函数，让它能够超过一定的准确度。</p>
<h2 id="可避免偏差-Avoidable-bias"><a href="#可避免偏差-Avoidable-bias" class="headerlink" title="可避免偏差 Avoidable bias"></a>可避免偏差 Avoidable bias</h2><p>在之前的课程关于偏差和方差的讨论中，我们主要假设有一些任务的贝叶斯错误率几乎为0。根据定义，人类水平错误率比贝叶斯错误率高一点，因为贝叶斯错误率是理论上限，但人类水平错误率离贝叶斯错误率不会太远。</p>
<p>对贝叶斯错误率的估计和训练错误率之间的差值称为可避免偏差。这个训练错误率和开发错误率之前的差值，就大概说明你的算法在方差问题上还有多少改善空间。</p>
<p><em>理解偏差和方差，那么在人类可以做得很好的任务中，你可以估计人类水平的错误率，你可以使用人类水平错误率来估计贝叶斯错误率。所以你到贝叶斯错误率估计值的差距，告诉你可避免偏差问题有多大，可避免偏差问题有多严重，而训练错误率和开发错误率之间的差值告诉你方差上的问题有多大，你的算法是否能够从训练集泛化推广到开发集。</em></p>
<p>要达到超越人类的表现往往不容易，但如果有足够多的数据，已经有很多深度学习系统，在单一监督学习问题上已经超越了人类的水平，所以这对在开发的应用是有意义的。</p>
<h1 id="Lesson-3-2"><a href="#Lesson-3-2" class="headerlink" title="Lesson 3.2"></a>Lesson 3.2</h1><p>本章还是机器学习策略。</p>
<h2 id="误差分析-Error-analysis"><a href="#误差分析-Error-analysis" class="headerlink" title="误差分析 Error analysis"></a>误差分析 Error analysis</h2><p>进行错误分析，应该找一组错误样本，可能在你的开发集里或者测试集里，观察错误标记的样本，看看假阳性（<strong>false positives</strong>）和假阴性（<strong>false negatives</strong>），统计属于不同错误类型的错误数量。在这个过程中，可能会得到启发，归纳出新的错误类型。</p>
<p>通过统计不同错误标记类型占总数的百分比，可以帮你发现哪些问题需要优先解决，或者给你构思新优化方向的灵感。</p>
<h2 id="清楚标记错误的数据-Cleaning-up-incorrectly-labeled-data"><a href="#清楚标记错误的数据-Cleaning-up-incorrectly-labeled-data" class="headerlink" title="清楚标记错误的数据 Cleaning up incorrectly labeled data"></a>清楚标记错误的数据 Cleaning up incorrectly labeled data</h2><p>首先，考虑训练集，事实证明，深度学习算法对于训练集中的随机错误是相当健壮的（<strong>robust</strong>）。标记出错的样本，只要这些错误样本离随机错误不太远，有时可能做标记的人没有注意或者不小心，按错键了，如果错误足够随机，那么放着这些错误不管可能也没问题，而不要花太多时间修复它们。</p>
<p>如果这些标记错误严重影响了在开发集上评估算法的能力，那么就应该去花时间修正错误的标签。但是，如果它们没有严重影响到用开发集评估成本偏差的能力，那么可能就不应该花宝贵的时间去处理。</p>
<ul>
<li>对开发集和测试集做同样的处理来确保他们保持同样的分布。</li>
<li>同时检验算法判断正确和错误的样本。</li>
<li>训练集和开发/测试集可能来自不同分布。</li>
</ul>
<blockquote>
<p>首先，深度学习研究人员有时会喜欢这样说：“我只是把数据提供给算法，我训练过了，效果拔群”。这话说出了很多深度学习错误的真相，更多时候，我们把数据喂给算法，然后训练它，并减少人工干预，减少使用人类的见解。但我认为，在构造实际系统时，通常需要更多的人工错误分析，更多的人类见解来架构这些系统，尽管深度学习的研究人员不愿意承认这点。</p>
<p>其次，不知道为什么，我看一些工程师和研究人员不愿意亲自去看这些样本，也许做这些事情很无聊，坐下来看100或几百个样本来统计错误数量，但我经常亲自这么做。当我带领一个机器学习团队时，我想知道它所犯的错误，我会亲自去看看这些数据，尝试和一部分错误作斗争。我想就因为花了这几分钟，或者几个小时去亲自统计数据，真的可以帮你找到需要优先处理的任务，我发现花时间亲自检查数据非常值得，所以我强烈建议你们这样做，如果你在搭建你的机器学习系统的话，然后你想确定应该优先尝试哪些想法，或者哪些方向。</p>
</blockquote>
<p><del>大佬之所以是大佬</del>。</p>
<h2 id="迁移学习-Transfer-learning"><a href="#迁移学习-Transfer-learning" class="headerlink" title="迁移学习 Transfer learning"></a>迁移学习 Transfer learning</h2><p>深度学习中，最强大的理念之一就是，有的时候神经网络可以从一个任务中习得知识，并将这些知识应用到另一个独立的任务中。所以例如，也许你已经训练好一个神经网络，能够识别像猫这样的对象，然后使用那些知识，或者部分习得的知识去帮助您更好地阅读x射线扫描图，这就是所谓的迁移学习。</p>
<p>具体来说，在第一阶段训练过程中，当进行图像识别任务训练时，可以训练神经网络的所有常用参数，所有的权重，所有的层，然后就得到了一个能够做图像识别预测的网络。在训练了这个神经网络后，要实现迁移学习，现在要做的是，把数据集换成新的$(x,y)$对，现在这些变成放射科图像，而$y$是想要预测的诊断，要做的是初始化最后一层的权重，在这个新数据集上重新训练网络。</p>
<p>经验规则是，如果有一个小数据集，就只训练输出层前的最后一层，或者也许是最后一两层。但是如果有很多数据，那么也许可以重新训练网络中的所有参数。如果重新训练神经网络中的所有参数，那么这个在图像识别数据的初期训练阶段，有时称为预训练（<strong>pre-training</strong>），因为在用图像识别数据去预先初始化，或者预训练神经网络的权重。然后，如果以后更新所有权重，有时这个过程叫微调（<strong>fine tuning</strong>）。</p>
<p>迁移学习起作用的场合是，在迁移来源问题中你有很多数据，但迁移目标问题你没有那么多数据。</p>
<ul>
<li>任务 A 和 B 有相同的输入 $x$。</li>
<li>A 有比 B 更多的数据。</li>
<li>A 的低级特征对于 B 的学习有帮助。</li>
</ul>
<h2 id="多任务学习-Multi-task-learning"><a href="#多任务学习-Multi-task-learning" class="headerlink" title="多任务学习 Multi-task learning"></a>多任务学习 Multi-task learning</h2><p>在迁移学习中，你的步骤是串行的，你从任务$A$里学习只是然后迁移到任务$B$。在多任务学习中，你是同时开始学习的，试图让单个神经网络同时做几件事情，然后希望这里每个任务都能帮到其他所有任务。</p>
<p>输出 $y^{(i)}$ 不再是一个标签，而是几个标签，表示不同的任务输出，网络的最后一层变成矩阵 $Y$。损失函数就是所有的输出损失求和。</p>
<p>多任务学习什么时候有意义：</p>
<ul>
<li><p>训练的一组任务可以共用低层次特征。</p>
</li>
<li><p>每个任务的数据量很接近。</p>
</li>
<li>可以训练一个足够大的神经网络，同时做好所有的工作。</li>
</ul>
<h2 id="端到端的深度学习-End-to-end-deep-learning"><a href="#端到端的深度学习-End-to-end-deep-learning" class="headerlink" title="端到端的深度学习 End-to-end deep learning"></a>端到端的深度学习 End-to-end deep learning</h2><p>终于知道所说的端到端是啥意思了。。。</p>
<p>简而言之，以前有一些数据处理系统或者学习系统，它们需要多个阶段的处理。那么端到端深度学习就是忽略所有这些不同的阶段，用单个神经网络代替它。</p>
<p>它表现可以很好，也可以简化系统架构，不需要搭建那么多手工设计的单独组件，但它也不是灵丹妙药，并不是每次都能成功。</p>
<p>优点：</p>
<ul>
<li><p>只通过数据，而不引入更多的人为概念。（Let the data speak）</p>
</li>
<li><p>更少的手工设计的组件。</p>
</li>
</ul>
<p>缺点：</p>
<ul>
<li>需要大量的数据。</li>
<li>排除了可能有用的手工设计的组件。</li>
</ul>
<h1 id="Lesson-5-1"><a href="#Lesson-5-1" class="headerlink" title="Lesson 5.1"></a>Lesson 5.1</h1><p>本节主要讲序列模型（Sequence Models）。</p>
<h2 id="数学符号-Notation"><a href="#数学符号-Notation" class="headerlink" title="数学符号 Notation"></a>数学符号 Notation</h2><p>$x^{<t>}$：序列的中间位置，$t$ 索引。</t></p>
<p>$T_x$：输入序列的长度。</p>
<p>$T_y$：输出序列的长度。</p>
<p><strong>one-hot</strong> 法表示字典中的每一个单词。</p>
<p><strong>Unknow Word</strong> 伪造单词，&lt;<strong>UNK</strong>&gt;。</p>
<h2 id="循环神经网络-Recurrent-Neural-Network"><a href="#循环神经网络-Recurrent-Neural-Network" class="headerlink" title="循环神经网络 Recurrent Neural Network"></a>循环神经网络 Recurrent Neural Network</h2><p>简单来说，在时间步 $t$ 中，模型不仅是用 $x^{<t>}$ 来预测 $\hat{y}^{<t>}$，而且还用了前一个时间步中的信息，也就是时间步 $t-1$ 的激活值会传递到 $t$。</t></t></p>
<p>有一个零时刻的初始激活值 $a^{<0>} = \mathbf{0}$。</0></p>
<p>循环神经网络是从左向右扫描数据，同时每个时间步的参数也是共享的。$W_{\text{ax}}$ 表示管理着从 $x^{<1>}$ 到隐藏层的连接的一系列参数，每个时间步使用的都是相同的参数 $W_{\text{ax}}$。而激活值也就是水平联系是由参数 $W_{aa}$ 决定的，同时每一个时间步都使用相同的参数 $W_{aa}$，同样的输出结果由 $W_{\text{ya}}$ 决定。</1></p>
<p><img data-src="rnn.png" alt></p>
<p>这个循环神经网络的一个缺点就是它只使用了这个序列中之前的信息来做出预测。</p>
<p>计算示例（FPP）：</p>
<p>$a^{&lt; t &gt;} = g_{1}(W_{aa}a^{&lt; t - 1 &gt;} + W_{ax}x^{&lt; t &gt;} + b_{a})$</p>
<p>$\hat y^{&lt; t &gt;} = g_{2}(W_a^{&lt; t &gt;} + b_{y})$</p>
<p>循环神经网络用的激活函数经常是<strong>tanh</strong>。</p>
<p>简化符号：</p>
<p>$a^{<t>} =g(W_{a}\left\lbrack a^{&lt; t-1 &gt;},x^{<t>} \right\rbrack +b_{a})$</t></t></p>
<p>$\hat y^{&lt; t &gt;} = g(W_{y}a^{&lt; t &gt;} +b_{y})$</p>
<p>所以我们定义$W_{a}$的方式是将矩阵$W_{aa}$和矩阵$W_{ax}$水平并列放置，$[ W_{aa}\vdots W_{ax}]=W_{a}$。</p>
<p>用这个符号（$\left\lbrack a^{&lt; t - 1 &gt;},x^{&lt; t &gt;}\right\rbrack$）的意思是将这两个向量堆在一起，用这个符号表示，即$\begin{bmatrix}a^{&lt; t-1 &gt;} \\ x^{&lt; t &gt;} \\\end{bmatrix}$。</p>
<p><img data-src="rnn-f.png" alt></p>
<h2 id="通过时间的反向传播-Backpropagation-through-time"><a href="#通过时间的反向传播-Backpropagation-through-time" class="headerlink" title="通过时间的反向传播 Backpropagation through time"></a>通过时间的反向传播 Backpropagation through time</h2><p><img data-src="rnn_cell_backprop.png" alt></p>
<h2 id="不同类型的循环神经网络"><a href="#不同类型的循环神经网络" class="headerlink" title="不同类型的循环神经网络"></a>不同类型的循环神经网络</h2><ul>
<li>多对多</li>
<li>多对一</li>
<li>一对一</li>
<li>一对多</li>
</ul>
<h2 id="语言模型和序列生成-Sequence-generation"><a href="#语言模型和序列生成-Sequence-generation" class="headerlink" title="语言模型和序列生成 Sequence generation"></a>语言模型和序列生成 Sequence generation</h2><p>所以语言模型所做的就是，预测某个特定的句子它出现的概率是多少。</p>
<p>为了使用<strong>RNN</strong>建立出这样的模型，首先需要一个训练集，包含一个很大的英文文本语料库（<strong>corpus</strong>）或者其它的语言，用于构建模型的语言的语料库。语料库是自然语言处理的一个专有名词，就是很长的或者说数量众多的英文句子组成的文本。</p>
<p>就是根据前面所给的单词，在下一个时间步预测出下一个单词的分布。</p>
<h2 id="新序列采样-Sampling-novel-sequences"><a href="#新序列采样-Sampling-novel-sequences" class="headerlink" title="新序列采样 Sampling novel sequences"></a>新序列采样 Sampling novel sequences</h2><p>就是把每一个时间步的输出传递到下一个时间步，直到 <strong>EOS</strong>。</p>
<h2 id="循环神经网络的梯度消失-Vanishing-gradients-with-RNNs"><a href="#循环神经网络的梯度消失-Vanishing-gradients-with-RNNs" class="headerlink" title="循环神经网络的梯度消失 Vanishing gradients with RNNs"></a>循环神经网络的梯度消失 Vanishing gradients with RNNs</h2><p>一个很深很深的网络，对这个网络从左到右做前向传播然后再反向传播。如果这是个很深的神经网络，从输出$\hat y$得到的梯度很难传播回去，很难影响靠前层的权重，很难影响前面层的计算。</p>
<p><strong>RNN</strong> 一个时间步的输出，基本上很难受到序列靠前的输入的影响，这是因为不管输出是什么，不管是对的，还是错的，这个区域都很难反向传播到序列的前面部分，也因此网络很难调整序列前面的计算。</p>
<h2 id="门控循环单元-Gated-Recurrent-Unit"><a href="#门控循环单元-Gated-Recurrent-Unit" class="headerlink" title="门控循环单元 Gated Recurrent Unit"></a>门控循环单元 Gated Recurrent Unit</h2><p><strong>GRU</strong> 能够改变 <strong>RNN</strong> 的隐藏层，使其可以更好地捕捉深层连接，并改善了梯度消失问题。</p>
<p>有个新的变量称为$c$，代表细胞（<strong>cell</strong>），即记忆细胞，记忆细胞的作用是提供了记忆的能力。实际上用的就是激活值 $c^{<t>} = a^{<t>}$。</t></t></p>
<p>在每个时间步，将用一个候选值重写记忆细胞，用 <strong>tanh</strong> 来激活。</p>
<script type="math/tex; mode=display">
\tilde{c}^{<t>} = \tanh{\left( W_c [c^{<t-1>}, x^{<t>}] + b_c \right)}</script><p>在<strong>GRU</strong>中真正重要的思想是有一个门，叫做$\Gamma_{u}$，这是个下标为 $u$ 的大写希腊字母 $\Gamma$，$u$ 代表更新（update），这是一个0到1之间的值。</p>
<script type="math/tex; mode=display">
\Gamma_{u}= \sigma(W_{u}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{u})</script><p>然后门决定是否要真的更新它。</p>
<script type="math/tex; mode=display">
c^{<t>} = \Gamma_{u} \times \tilde{c}^{<t>} +\left( 1- \Gamma_{u} \right)*c^{<t-1>}</script><p>因为$\Gamma_{u}$很接近0，可能是0.000001或者更小，这就不会有梯度消失的问题了。因为$\Gamma_{u}$很接近0，这就是说$c^{<t>}$几乎就等于$c^{<t-1>}$，而且$c^{<t>}$的值也很好地被维持了，即使经过很多很多的时间步。</t></t-1></t></p>
<p>对于完整的 <strong>GRU</strong>，还有一个门 $\Gamma_r$，$r$ 表示相关性（relevance），这个 $\Gamma_{r}$ 门告诉你计算出的下一个$c^{<t>}$的候选值${\tilde{c}}^{<t>}$跟$c^{<t-1>}$有多大的相关性。</t-1></t></t></p>
<script type="math/tex; mode=display">
\Gamma_{r}= \sigma(W_{r}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack + b_{r})</script><blockquote>
<p>这是多年来研究者们试验过很多很多不同可能的方法来设计这些单元，去尝试让神经网络有更深层的连接，去尝试产生更大范围的影响，还有解决梯度消失的问题，<strong>GRU</strong>就是其中一个研究者们最常使用的版本，也被发现在很多不同的问题上也是非常健壮和实用的。你可以尝试发明新版本的单元，只要你愿意。但是<strong>GRU</strong>是一个标准版本，也就是最常使用的。</p>
</blockquote>
<h2 id="长短期记忆-Long-short-term-memory"><a href="#长短期记忆-Long-short-term-memory" class="headerlink" title="长短期记忆 Long short term memory"></a>长短期记忆 Long short term memory</h2><script type="math/tex; mode=display">
\tilde{c}^{<t>} = \tanh{(W_c\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack) + b_c}</script><script type="math/tex; mode=display">
\Gamma_{u}= \sigma(W_{u}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{u})</script><script type="math/tex; mode=display">
\Gamma_{f}= \sigma(W_{f}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{f})</script><script type="math/tex; mode=display">
\Gamma_{o}= \sigma(W_{o}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{o})</script><script type="math/tex; mode=display">
c^{<t>} = \Gamma_u \times \tilde{c}^{<t>} + \Gamma_f \times c^{<t-1>}</script><script type="math/tex; mode=display">
a^{<t>} = \Gamma_o \times c^{<t>}</script><p>常用的版本可能是门值不仅取决于$a^{<t-1>}$和$x^{<t>}$，有时候也可以偷窥一下$c^{<t-1>}$的值（上图编号13所示），这叫做“窥视孔连接”（<strong>peephole connection</strong>）。</t-1></t></t-1></p>
<p>只要正确地设置了遗忘门和更新门，<strong>LSTM</strong>是相当容易把$c^{<0>}$的值一直往下传递到右边。这就是为什么<strong>LSTM</strong>和<strong>GRU</strong>非常擅长于长时间记忆某个值，对于存在记忆细胞中的某个值，即使经过很长很长的时间步。</0></p>
<p><img data-src="LSTM.png" alt></p>
<p><strong>GRU</strong>的优点是这是个更加简单的模型，所以更容易创建一个更大的网络，而且它只有两个门，在计算性上也运行得更快，然后它可以扩大模型的规模。</p>
<p>但是<strong>LSTM</strong>更加强大和灵活，因为它有三个门而不是两个。<strong>LSTM</strong>在历史进程上是个更优先的选择。</p>
<h2 id="双向循环神经网络-Bidirectional-RNN"><a href="#双向循环神经网络-Bidirectional-RNN" class="headerlink" title="双向循环神经网络 Bidirectional RNN"></a>双向循环神经网络 Bidirectional RNN</h2><p>就是正着跑一遍再反着跑一遍，是一个无环图（Acyclic graph）。</p>
<p><img data-src="brnn.png" alt></p>
<p>会有 $\hat y^{<t>} =g(W_{g}\left\lbrack \overrightarrow{a}^{&lt; t &gt;},\overleftarrow{a}^{&lt; t &gt;} \right\rbrack +b_{y})$。</t></p>
<p>通过这些改变，就可以用一个用<strong>RNN</strong>或<strong>GRU</strong>或<strong>LSTM</strong>构建的模型，并且能够预测任意位置，即使在句子的中间，因为模型能够考虑整个句子的信息。这个双向<strong>RNN</strong>网络模型的缺点就是需要完整的数据的序列，才能预测任意位置。</p>
<h2 id="深层循环神经网络-Deep-RNNs"><a href="#深层循环神经网络-Deep-RNNs" class="headerlink" title="深层循环神经网络 Deep RNNs"></a>深层循环神经网络 Deep RNNs</h2><p>要学习非常复杂的函数，通常我们会把<strong>RNN</strong>的多个层堆叠在一起构建更深的模型。</p>
<p>可以堆好几个隐藏层 $a^{[i]}$。</p>
<p>可以输出后再叠非循环层。</p>
<p><img data-src="drnn.png" alt></p>
<p>由于深层的<strong>RNN</strong>训练需要很多计算资源，需要很长的时间，尽管看起来没有多少循环层，这个也就是在时间上连接了三个深层的循环层，看不到多少深层的循环层，不像卷积神经网络一样有大量的隐含层。</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/08/02/scikit-learn/" rel="prev" title="scikit-learn">
      <i class="fa fa-chevron-left"></i> scikit-learn
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/08/10/mit-cv/" rel="next" title="mit机器学习cv">
      mit机器学习cv <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-1-1"><span class="nav-number">1.</span> <span class="nav-text">Lesson 1.1</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-1-2"><span class="nav-number">2.</span> <span class="nav-text">Lesson 1.2</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-1-3"><span class="nav-number">3.</span> <span class="nav-text">Lesson 1.3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-Back-Propagation"><span class="nav-number">3.1.</span> <span class="nav-text">反向传播 Back Propagation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-1-4"><span class="nav-number">4.</span> <span class="nav-text">Lesson 1.4</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%B1%82%E8%A1%A8%E7%A4%BA%EF%BC%9F"><span class="nav-number">4.1.</span> <span class="nav-text">为什么使用深层表示？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9D%97"><span class="nav-number">4.2.</span> <span class="nav-text">搭建神经网络块</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%92%8C%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-number">4.3.</span> <span class="nav-text">参数和超参数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-2-1"><span class="nav-number">5.</span> <span class="nav-text">Lesson 2.1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E5%B7%AE-%E5%81%8F%E5%B7%AE"><span class="nav-number">5.1.</span> <span class="nav-text">方差&#x2F;偏差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95"><span class="nav-number">5.2.</span> <span class="nav-text">基本方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Regularization%EF%BC%89"><span class="nav-number">5.3.</span> <span class="nav-text">正则化（Regularization）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dropout-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">5.4.</span> <span class="nav-text">dropout 正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-number">5.5.</span> <span class="nav-text">其他正则化方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E8%BE%93%E5%85%A5%EF%BC%88Normalizing%EF%BC%89"><span class="nav-number">5.6.</span> <span class="nav-text">归一化输入（Normalizing）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1-%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%EF%BC%88Vanishing-Exploding-gradients%EF%BC%89"><span class="nav-number">5.7.</span> <span class="nav-text">梯度消失&#x2F;梯度爆炸（Vanishing &#x2F; Exploding gradients）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">5.8.</span> <span class="nav-text">神经网络权重的初始化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%95%B0%E5%80%BC%E9%80%BC%E8%BF%91-Numerical-approximation-of-gradients"><span class="nav-number">5.9.</span> <span class="nav-text">梯度的数值逼近 Numerical approximation of gradients</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-number">5.10.</span> <span class="nav-text">梯度检验的注意事项</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-2-2"><span class="nav-number">6.</span> <span class="nav-text">Lesson 2.2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Mini-batch-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">6.1.</span> <span class="nav-text">Mini-batch 梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E6%95%B0-Exponentially-weighted-averages"><span class="nav-number">6.2.</span> <span class="nav-text">指数加权平均数 Exponentially weighted averages</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A8%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-Gradient-descent-with-Momentum"><span class="nav-number">6.3.</span> <span class="nav-text">动量梯度下降法 Gradient descent with Momentum</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RMSprop"><span class="nav-number">6.4.</span> <span class="nav-text">RMSprop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adam"><span class="nav-number">6.5.</span> <span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F-Learning-rate-decay"><span class="nav-number">6.6.</span> <span class="nav-text">学习率衰减 Learning rate decay</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E9%97%AE%E9%A2%98-Local-optima"><span class="nav-number">6.7.</span> <span class="nav-text">局部最优问题 Local optima</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-2-3"><span class="nav-number">7.</span> <span class="nav-text">Lesson 2.3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B0%83%E8%AF%95%E5%A4%84%E7%90%86-Tuning-process"><span class="nav-number">7.1.</span> <span class="nav-text">调试处理 Tuning process</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-Normalizing-activation"><span class="nav-number">7.2.</span> <span class="nav-text">归一化激活函数 Normalizing activation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-Norm-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E7%94%A8"><span class="nav-number">7.3.</span> <span class="nav-text">Batch Norm 为什么有用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Softmax-%E5%9B%9E%E5%BD%92"><span class="nav-number">7.4.</span> <span class="nav-text">Softmax 回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6"><span class="nav-number">7.5.</span> <span class="nav-text">深度学习框架</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-3-1"><span class="nav-number">8.</span> <span class="nav-text">Lesson 3.1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E4%BA%A4%E5%8C%96-Orthogonalization"><span class="nav-number">8.1.</span> <span class="nav-text">正交化 Orthogonalization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E4%B8%80%E6%95%B0%E5%AD%97%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87-Single-number-evaluation-metric"><span class="nav-number">8.2.</span> <span class="nav-text">单一数字评估指标 Single number evaluation metric</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BB%A1%E8%B6%B3%E5%92%8C%E4%BC%98%E5%8C%96%E6%8C%87%E6%A0%87-Satisficing-and-optimizing-metrics"><span class="nav-number">8.3.</span> <span class="nav-text">满足和优化指标 Satisficing and optimizing metrics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83-%E5%BC%80%E5%8F%91-%E6%B5%8B%E8%AF%95%E9%9B%86%E5%88%92%E5%88%86-train-dev-test-distributions"><span class="nav-number">8.4.</span> <span class="nav-text">训练&#x2F;开发&#x2F;测试集划分 train&#x2F;dev&#x2F;test distributions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%BA%E7%9A%84%E8%A1%A8%E7%8E%B0-Why-human-level-performance"><span class="nav-number">8.5.</span> <span class="nav-text">为什么是人的表现 Why human-level performance?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%AF%E9%81%BF%E5%85%8D%E5%81%8F%E5%B7%AE-Avoidable-bias"><span class="nav-number">8.6.</span> <span class="nav-text">可避免偏差 Avoidable bias</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-3-2"><span class="nav-number">9.</span> <span class="nav-text">Lesson 3.2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AF%E5%B7%AE%E5%88%86%E6%9E%90-Error-analysis"><span class="nav-number">9.1.</span> <span class="nav-text">误差分析 Error analysis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B8%85%E6%A5%9A%E6%A0%87%E8%AE%B0%E9%94%99%E8%AF%AF%E7%9A%84%E6%95%B0%E6%8D%AE-Cleaning-up-incorrectly-labeled-data"><span class="nav-number">9.2.</span> <span class="nav-text">清楚标记错误的数据 Cleaning up incorrectly labeled data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0-Transfer-learning"><span class="nav-number">9.3.</span> <span class="nav-text">迁移学习 Transfer learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0-Multi-task-learning"><span class="nav-number">9.4.</span> <span class="nav-text">多任务学习 Multi-task learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-End-to-end-deep-learning"><span class="nav-number">9.5.</span> <span class="nav-text">端到端的深度学习 End-to-end deep learning</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lesson-5-1"><span class="nav-number">10.</span> <span class="nav-text">Lesson 5.1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7-Notation"><span class="nav-number">10.1.</span> <span class="nav-text">数学符号 Notation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Recurrent-Neural-Network"><span class="nav-number">10.2.</span> <span class="nav-text">循环神经网络 Recurrent Neural Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%9A%E8%BF%87%E6%97%B6%E9%97%B4%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-Backpropagation-through-time"><span class="nav-number">10.3.</span> <span class="nav-text">通过时间的反向传播 Backpropagation through time</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">10.4.</span> <span class="nav-text">不同类型的循环神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%BA%8F%E5%88%97%E7%94%9F%E6%88%90-Sequence-generation"><span class="nav-number">10.5.</span> <span class="nav-text">语言模型和序列生成 Sequence generation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B0%E5%BA%8F%E5%88%97%E9%87%87%E6%A0%B7-Sampling-novel-sequences"><span class="nav-number">10.6.</span> <span class="nav-text">新序列采样 Sampling novel sequences</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1-Vanishing-gradients-with-RNNs"><span class="nav-number">10.7.</span> <span class="nav-text">循环神经网络的梯度消失 Vanishing gradients with RNNs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83-Gated-Recurrent-Unit"><span class="nav-number">10.8.</span> <span class="nav-text">门控循环单元 Gated Recurrent Unit</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86-Long-short-term-memory"><span class="nav-number">10.9.</span> <span class="nav-text">长短期记忆 Long short term memory</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Bidirectional-RNN"><span class="nav-number">10.10.</span> <span class="nav-text">双向循环神经网络 Bidirectional RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%B1%82%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Deep-RNNs"><span class="nav-number">10.11.</span> <span class="nav-text">深层循环神经网络 Deep RNNs</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hui hui"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Hui hui</p>
  <div class="site-description" itemprop="description">Be happy forever!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/songtianhui" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;songtianhui" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:191098194@smail.nju.edu.cn" title="E-Mail → mailto:191098194@smail.nju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hui hui</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>



        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


  

  <script async src="/js/cursor/fireworks.js"></script>


 
<script>
  var OriginTitile = document.title;
  var titleTime;
  document.addEventListener("visibilitychange", function() {
    if (document.hidden) {
      document.title = "(つェ⊂)" + OriginTitile;
      clearTimeout(titleTime);
    } else {
      document.title = "(*´∇｀*)" + OriginTitile;
      titleTime = setTimeout(function() {
        document.title = OriginTitile;
      }, 2000);
    }
  });
</script>


 
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/moment.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment-precise-range-plugin@1.3.0/moment-precise-range.min.js"></script>
<script>
  function timer() {
    var ages = moment.preciseDiff(moment(),moment(20210126,"YYYYMMDD"));
    ages = ages.replace(/years?/, "years");
    ages = ages.replace(/months?/, "moths");
    ages = ages.replace(/days?/, "days");
    ages = ages.replace(/hours?/, "hours");
    ages = ages.replace(/minutes?/, "minutes");
    ages = ages.replace(/seconds?/, "seconds");
    ages = ages.replace(/\d+/g, '<span style="color:#1890ff">$&</span>');
    div.innerHTML = `I have been here waiting for you for ${ages}`;
  }
  var div = document.createElement("div");
  //插入到copyright之后
  var copyright = document.querySelector(".copyright");
  document.querySelector(".footer-inner").insertBefore(div, copyright.nextSibling);
  timer();
  setInterval("timer()",1000)
</script>


 
<script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginrootpath":"live2dw/","pluginjspath":"lib/","pluginmodelpath":"assets/ relative)","scriptfrom":"local","tagmode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":180,"height":300},"mobile":{"show":true},"react":{"opacitydefault":0.7},"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
