<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2021美赛总结</title>
    <url>/2021/02/12/2021-mathmatical-modeling/</url>
    <content><![CDATA[<h1 id="契机"><a href="#契机" class="headerlink" title="契机"></a>契机</h1><p>原来是在表白墙上看到的招募信息，没有抱希望地去问了一下没想到真的成功了，然后在比赛前一天混进了有两个大佬的小队orz</p>
<a id="more"></a>
<p>其实在看到了招募信息后纠结了挺长一段时间的，比较担心别人不要我或者工作的时候我啥都不会给别人拖后腿挺丢人的。。。不过最后下定决心(腆着脸)去试一试，感觉就这么一个机会而且美赛算是认可度比较高的一项竞赛，大学就这么多年有机会还是要把握，最后果然能成而且表现还不算太差吧ww<br>感觉大学真的学会抓住机会很重要，有时候确实得需要去厚着脸皮，突破自己的舒适圈去争取一些事情，做一些“万一成了就是赚了，没成也不咋亏”的事</p>
<h1 id="建模中"><a href="#建模中" class="headerlink" title="建模中"></a>建模中</h1><p>真的开始工作了才知道自己真的差太多了，建模知识几乎一窍不通，python在两天前真的才刚在学<code>print(&quot;Hello, world!&quot;)</code>啊qwq<br>总结一些第一次参加建模比赛所学到的一点东西吧:</p>
<ul>
<li>python和matlab才是主力，cpp真的毫无卵用orz。所以目前至少一定要把python给学熟练了</li>
<li>python几个模块非常重要，python基础学好后一定专门找时间把它弄清楚了(numpy数据，pyplot/seaborn画图，sklearn, wordcloud…). 关键对于各种数据类型(dataframe等)和各种函数的接口的理解</li>
<li>数学建模基本上按照建模、代码、写作三个part来划分工作，可实际上也没有划分那么清楚，而我在这三个方面基本上没有任何工作力(latex排版稍微能帮上一点)，而且可能用到大量的类似机器学习、深度学习的知识，所以要学。。。</li>
<li>临场学习的能力真的也很重要，此次在工作中的任务基本都是靠从零开始快速学会，像python对excel/csv的数据处理以及可视化，tf-idf和词云等，也证实快速掌握一项能力是可行的(在积极主动地去学习的前提下), 大佬队员所说数学建模真的能大大提升你的skill set</li>
<li>小组的合作交流真的太重要的了www，这次的两个队员真的太nice了，既大佬又不嫌弃我还耐心地给我安排以及讲解一些工作，对于一些具体的工作还会拿出一些例子给我看，真的很感恩</li>
<li>python真的是世界上最好的语言！对py的理解和代码能力突飞猛进</li>
</ul>
<h1 id="提交"><a href="#提交" class="headerlink" title="提交"></a>提交</h1><p>这一趴真的太惊险刺激了，甚至到现在也没能放下心来orz<br><strong>总之一点不要把所有工作压倒ddl最后一刻，预留足够多的时间来做overview求求了！</strong> 本来就10:00 am的ddl结果9:55才算全部整理好开始下载提交结果遇到了队长网卡，pdf太大以及投递失败的问题，血压拉满到10:15才提交成功<br>关于pdf至今未能知道为啥它会这么大(47M)，估计应该是图片太多了吧。。。实在没办法可以找网站压缩，比如<a href="https://www.ilovepdf.com/zh-cn/compress_pdf">这个</a>还挺好用<br>然后它给我把47M压到了500k我也不知道怎么做到的。。。</p>
<h1 id="总之"><a href="#总之" class="headerlink" title="总之"></a>总之</h1><p>差得很远继续努力，技能树需要点亮很多很多，python和机器学习必须赶紧熟练掌握<br>然后在国赛的时候多学一些建模的知识，争取在下次比赛能够形成较为系统的意识<br><img data-src="mm.JPG" alt="conclusion" title="conclusion"></p>
]]></content>
      <tags>
        <tag>mathmatical-modeling</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown test</title>
    <url>/2021/01/26/Markdown-test/</url>
    <content><![CDATA[<h1 id="这是一级标题"><a href="#这是一级标题" class="headerlink" title="这是一级标题"></a>这是一级标题</h1><h2 id="这是二级标题"><a href="#这是二级标题" class="headerlink" title="这是二级标题"></a>这是二级标题</h2><h3 id="这是三级标题"><a href="#这是三级标题" class="headerlink" title="这是三级标题"></a>这是三级标题</h3><a id="more"></a>
<hr>
<p><strong>加粗</strong><br><em>斜体</em><br><strong><em>斜体加粗</em></strong><br><del>删除线</del></p>
<hr>
<blockquote>
<p>这是引用的内容</p>
<blockquote>
<p>这是引用的内容</p>
<blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>这是引用的内容</p>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
</blockquote>
<hr>
<p><img data-src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fc-ssl.duitang.com%2Fuploads%2Fitem%2F201709%2F13%2F20170913224334_BNTZG.thumb.400_0.jpeg&amp;refer=http%3A%2F%2Fc-ssl.duitang.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1615734130&amp;t=6ff751ad338c71d43fdc49f9a178a53c" alt="这是一张图片" title="This is a picture!"><br><a href="https://github.com/songtianhui/songtianhui.github.io" title="github repository">这是一个链接</a></p>
<p><img data-src="avatar.gif" alt="这也是一张图片3" title="This is also a picture!"></p>
<hr>
<ul>
<li>列表内容</li>
<li>列表内容<ul>
<li>列表内容</li>
</ul>
</li>
</ul>
<ol>
<li>列表内容</li>
<li>列表内容</li>
</ol>
<hr>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">这是</th>
<th style="text-align:right">一个</th>
<th style="text-align:center">表格</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">左对齐</td>
<td style="text-align:right">右对齐</td>
<td style="text-align:center">居中</td>
</tr>
</tbody>
</table>
</div>
<hr>
<p>Here is <code>code</code></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">&quot;Hello world!\n&quot;</span>);</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<script type="math/tex; mode=display">S_{n} = \sum\limits_{i=1}^{n} \dfrac{i(i-1)}{2}</script><p>行内公式$\kappa_{s} = \overline{\alpha} + \beta \times \pi, x = \dfrac{-b \pm \sqrt{b^{2} - 4ac}}{2a}$<br><del>好像没成功…Σ( ° △ °|||))︴</del><br><strong>成功了！</strong></p>
<hr>
<p><img data-src="https://box.nju.edu.cn/seafhttp/files/48a324b9-6900-408d-9da3-778b2d7b28e8/IMG_20210402_133755_0004.JPG" alt></p>
<h1 id="emoji"><a href="#emoji" class="headerlink" title="emoji"></a>emoji</h1><p><span class="github-emoji" alias="smile" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png?v8">&#x1f604;</span></p>
<p><span class="github-emoji" alias="dog" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f436.png?v8">&#x1f436;</span></p>
<p><span class="github-emoji" alias="joy" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f602.png?v8">&#x1f602;</span></p>
]]></content>
      <tags>
        <tag>This is tag</tag>
      </tags>
  </entry>
  <entry>
    <title>2021寒假计划</title>
    <url>/2021/01/28/Flags-for-winter-holiday/</url>
    <content><![CDATA[<h1 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h1><pre><code>先把这个blog基础设施搭建好，配置到一个最顺手好看的亚子。
</code></pre><h1 id="装系统"><a href="#装系统" class="headerlink" title="装系统"></a>装系统</h1><pre><code>用了一个学期虚拟机真的太累了，真是又慢又卡orz
准备装一个Ununtu比Debian用起来优雅多了，不过电脑磁盘不够了得买个移动硬盘。。。
</code></pre><a id="more"></a>
<h1 id="学习篇"><a href="#学习篇" class="headerlink" title="学习篇"></a>学习篇</h1><p>有想学很多东西但可能只能停留在想的阶段。。。</p>
<h2 id="Language"><a href="#Language" class="headerlink" title="Language"></a>Language</h2><pre><code>学个python吧，python是世界上最好的语言!
</code></pre><p><a href="https://www.liaoxuefeng.com/wiki/1016959663602400">廖雪峰python教程</a></p>
<h2 id="ML"><a href="#ML" class="headerlink" title="ML"></a>ML</h2><pre><code>看看吴恩达？机器学习好像很多地方都有用。
</code></pre><p><a href="https://www.bilibili.com/video/BV164411b7dx?from=search&amp;seid=11011536408088050441">吴恩达机器学习</a></p>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><pre><code>LeetCode做做题吧，算法比别人差太多了orz
</code></pre><h2 id="Mathematical-modeling"><a href="#Mathematical-modeling" class="headerlink" title="Mathematical modeling"></a>Mathematical modeling</h2><pre><code>那可能还要顺手学一下mathlab?
才知道有美赛这个玩意儿。。。现在着手可能晚了，也组不到人也没人要我这个啥都不会的菜鸡w
</code></pre><h2 id="Operating-System"><a href="#Operating-System" class="headerlink" title="Operating System"></a>Operating System</h2><pre><code>既然可能学不到什么新技能，不如预习预习下学期的os...这学期ics已经炸掉了，保住成绩也不错。
</code></pre><h2 id="Teaching"><a href="#Teaching" class="headerlink" title="Teaching"></a>Teaching</h2><pre><code>估计师说那还是要找我去上课的，既然没有动力学习把时间转换成money也行。
</code></pre><h2 id="Hobbies"><a href="#Hobbies" class="headerlink" title="Hobbies"></a>Hobbies</h2><pre><code>搞点课外兴趣，procreate画点画。可能再学点剪辑(?)
</code></pre><h1 id="谈恋爱"><a href="#谈恋爱" class="headerlink" title="谈恋爱"></a>谈恋爱</h1><p><strong>好！</strong><br><strong>我爱npy!</strong></p>
]]></content>
      <tags>
        <tag>flags</tag>
      </tags>
  </entry>
  <entry>
    <title>和芝麻的旅行日记——武汉篇</title>
    <url>/2021/04/11/Wuhan/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="32c54ca0169166e577062539e73ea2d5be0bbed87f10e958cebb7cb65a15ef15">8e8262f4c8b4badbe165aed5587f464d818e49d5ced7dee8633c4437270a54e2a3a48c49ce90a3b3da7d595176ee9742d8c1d58c91a87f8e519aeb6ef5f9722a99c9099dab34c11f9b79d5d59428135159d4ef979dbad40ef950fb46aa278f2f9b07d91ec32be45f80cba3a9fb69ab016afbf235884cb0acd2faaac2c9ae64908004049d103cf0f1eb5ecdc6ab95687d4f031adad7cdf6d47448e47f11eede9a0226ab1115b8e9195bea25593826fd27817e064b3e5724790a758738329fdef485f0db55b11136e0cb131e14276fb9e86e3e1e07d5cf5fbef1700d5899a1e57579b3fa026e5b33cd2b7b63cb63d02bb906a245aed342c4432fd966dd2223667b44dc9100437913af931a228738c9715a7f852068b4a5c270438a55099af69851f1dea0d50e50be7174967af58bd13401b8bc2e266f7dbe78ad5144ddaa66b80b5dfaeaab38c2709ddc6a23faa28c816c143ff0b78cd31b8bdab34fdf7a5aae9a8fe2cbd105a8ecec287b9a238b42a72505c5a2877eb6060636a49942bf1695d319fd926e7d64b57c3b4baa464e9c922477df7a21d9b56652d5f07ff2653b95ec94716f11e49e60f0c5029164a14f11eee1a12fae139e13f058f6166f5a8e72aabd422a03c9cc308eef88c3b4270f3b77fae570c9174ed3329e9155761a9b54575cd05079b9fc4c8e6c4465f1f7ef0e1c9c2212eefea9682d4bbc9a76341d16e8b87e608edb375f7117ca897005722e6be5cc94aab733838b894a2e4e50e4a1f3cba08f134c19ba1be8882fd497c956f81800aa5e06d3df598b481d2799727f0d6428d23b1771ccfcfb8cd0ec4521fe55627cec513f04845c7e7731fd2cd945cdb1894ebb59713dedc4745efcf6fb8d5a526bfe4ae6bd259c9357f3561450e45a046f43573b97fe6a1c7ab45c1580fc3fbdc4181b2d4bd2f8fb32fe89586ba3f1b8afea9136d0fed85b47e8d8579cb3eaa2f6bd7948d80091739451b941944b8ac5240a17523770797062a4f21776b0986e08dd37436f30e54dce545ac9c5bf86d8602356b0b9aec5ef978ea804f806accafbd1514f38e26b38e395e0681b896e5a26e8be4fb98c5e846c0f28a5e3a3c14bae0fa40d63948fbfc8a5e214e7844ff8164e8081d4839ef6cd127c8c6149258f7447cd1b960712ea5291b10329131117d8452161323c34cd3be9ba77b881f18a0231bbd51c24236a4785ba1cd7ad80cbc7b61d8cd7df1c9f197b981e9c631c2acf619e1a6de7c107ce1666ea304b3838afbb58ef91619d5599217c4fb380186d9ddb1db2d561f34743291579d38c8b99b39939933c0b6f15c93da6dfbb7e28198430cf5dff2703ac4199096c7407d1edee137db59d8715936b6a166794b415cfb6326faadf59e3e63b4df05fab1b7062342c442ca8effb97d9d5eef80744f79ab5d9d39f510161436debde700299ff075e0e99b8ff8561a8bd60aaceff4e2fc3cbac449afe72c040dc3b44dadd45948a0283ad5fbe66aea66a9a607753c57ddff4bdab17ef667126d873b88f97a047592796f3fb4d7ec397ea7e25d54b015be0141a7ee15eedb8fcf7b7cada20f9c00e5fe97eee6be24f0c9ddad2e11b92bded1b2bb3860a2055eda05f2cc7784689af84631a8a04cd74541ec0ec36d96ee7a021db850d1ae0841b740119c8d410497d84d234f824c0d5dc844c452e44b46c4e73d43cf643dcf33e17cd12b1f02c28bb16c050b9d1422a17f31f09472e372ff5f7de1bb20380931e65713643c1948c62f1a8f117ef80b64c341165fb4713211d6d44f59748f62fbff491641cbea0fa8df5fe189e3351950e5ddf054b83360b4f6d97b3539864e21f36eb1022bf44ff080f65639e93e990d70b4ca54e0b587baea781e0eae205c5ffea139b5757a8fba7826763d6520eedb15e17a58d8a83678c6f119acd02b4718fadd69786343f1c8088dfdc4b3d73e86f3922116d2eb35d195f653b2c7fecca6300f290983ee2782720453a79972c784116e9296bbf955936ddfd99bbd46b5f443b0582640b61836a02d1474fcf04d21cc6bffd174d476c2b580a5ab0397ac6036275cdad72f3adae9c1684c0fd44360316a8adc519de0c2805be1fc3cea0e950d73dd8ebaf1fd04740d1637879ba9829f7bfd767edc38743d1a1ffa4e8000ec6bbd8288a411c13b37ccee53879740fc3e21b283f511fb9a9bc1fe33007e3ee2f75bbf0a998b2df5fe03583a8cf89cd9d9ebd8c7d32b034191d588d741cfa24ace53fc577c8ae6fa46dd116de845f9727a621e0e0bebf83afe1d57d65929a9ad89d67ff87daa7b83a596316209ffd4d9c2c889e6a98695030345a50f560e6173ac09a3d4a5b4d51881d6cca7de6cf1dccc0bde5721b80d70322c2372b8cfd709f77c70728d7b74edf47e1aa655f96f27ab77d0a5db40f0a3b86243bb13dbf62b5d528451af52ce0307bd94ae5bdf8e14ce82f3f7e0427ea61ca40be87b4e02e1756008059afbd863643c6d4460f9262984585e3e1541e64df439bb004e45b10be75448b4f3153b703196add89a0cfd077da532d3b185ae48ec54cc78da8c2ac4d5f3e070f91ce16ad08dbceba061bf508c9f6a3487f3af12b221b5ebf8258937cd7f8ff4a7d689f8cfa1894397c0eda097e3f710a8dbc4fb2f083c3026087a98117fdb2c92cf593d35f848fb53e0523d11f2c301efa3dfc396e54fcf7277c860b69a2e4c4ee554c2d6d3ea66d301f2fbdd9b267528cc9578092a19d598e3255e7c0f41b9bf6f8d18640043e240e9fa4cb64d490a0c1d5f76e3c172ef469c8047fd104d22f92115be5e2657915a1ce737e4f1ad08a59b0b97e4120f69e5b6748a3869cf96cc629ee004752ae126d11b495e2ed746dc54755c53cb593d9ed0ac6c5efc3de622180e2b93975428199da755f06fa336d8b23bb8d9afb1d4c83498802db2a24641e83fcd32f827b4dea8ceffa83b014dfa730d039c0610b5f4f7613be0a20d82f451a8efef063a92bcadb9eb6cf70775e69b02a799566590d2ad56678c75101d53d707a7f80e54722b631bbe3a858f078f7f869dc46cde6504354d1a9b4f9e01f7cd66c2409caa9e94c866dcc8d1a4cb262dab7e2489ff424683315f97dffcee1f33c26197de2a70c6fb92e6f565d69800c5df05719eacbf52622f46b0d12aa7b5a76f3d496c422cc1bea31d15d237a01b2e2e4812bc4654e001b55f14e4cea02e545dec31514cd50b1dc5722d0a31ed11f59d0276d47fc9d827d6b9284977c93a9a6c8bbccfeb9f5d3a715548f2b216409fde269b928072e81b4d89db01471c8d39fcb6107654ce2236a3657b81379a340e5e492a4d52191b60a4a2958688fdb7e49b79cb4a36b7e77ec12aa8d183adca608b8c74205b55ac3638b10412dabd57bf1d355a25410d113aa7c3091dbc686a01d5f652644facd7dd0191762a435be4f3f558eac50d2945690bf2ccba3097d94763c2c10aaa14fec5acd0b422249e8ef4c3444afa08bda1f7a452f71e3b241d3f5d7c8295e18c6f6208abf280bf72532cf203026eb66babe1c3d27f5c445768212f1a57a98c40e990a4db303558c25c9a5df08a53b12bff9bc0a58d66ff039f463d308c524b6c15c1fa602ad62b8d4ce47c526e7f6ee32a607c230e1cd181d0e0cb294f74d1617d5706684e15063717b0e87c5877ea04145f0d000b4c065d8390564aa82bc08f838c29026cf33f007a8ce2a5755353ec45b1bee8c5b70d2e0ad654cd64c84fd44b1a22c06f71c513f170f8f0ba99cb611b2daadce4dc865c194ae6052a532ad476002db9957211170cdd69848185002a76a1fba3c2e0caf0100939ccbf1871d2be2f89a52414c06f941838afae9bdae34a3e56901849fab7096ea2dfd6038d5167db31ae6e076c046d16d50d5d844db206f2e7bdab9dae81c681f8dc837036ce374f211a3676a5a447eb54abcfd844e73588339d83d5826508998ea58241ff686edbc0caad6e07e5bc4de7ba23f6c1da6e21fa8102f34cb2137b15267984c9cc7fca795264163717e5b1f5460dfedb8cc76ef4de3b6c81908496375b19fb7282fd0e612328cd50e1d58b0df3c99b764aa8129d52ed8c647c80367390235bdc79c68844d2c8ef2ff949689ad86afe6671a8565f2d6fba223005818f6ece33f32a6ab3b5a36c47aced8009019d45c0cec0a877ca267c86bf819b19f4d27355da4613ee54537c874930bdfda5646be60c95d298044a8363d79fc9d0913fbcef3d309c5a63150dee5c15b12cfa9dc57b7895b5d17b4ebdbac28327f86d84baad2ca02e3422b36e3b4862182772591ec79470ba5906558832a10a2b6a2d109672a52562e816770964e0928af267e1e9c1fefad398b89a7bf82b02365b2304aeb90a8124519c16ce867c57a78cb602272796196d46b4da9c275e0075c303d2fbf63a268096ef7ec8e287617248c21c07f7df4c6b5e49e0ecd8e50c76c986ffe90a57b5a1da346e94ce69f607fc15eda73871ba3556724d4e073bc8ff17e4a9175b6e7d04bcedb169654599a1bc74fa69dcf2933a4c858d23e9ca434e3d0e1d61398b941c5e84ab5fee4bcc06c98cf0389170209524a613370efc0d317a04295ebac72c5a9f8c68bd455f61eefa97b9d4a0fc402923d61f84343491847d4c98b35f4b1a6833fd50255889695ea8aa7a0cff437d8b7534bc569d44e9052c027214778937d6c349dc9190474e12e346ef50dea97e642d1f8e2ece921c7b098bc45b222f3738a37bf4646bfcf3834c32cdc3dc3d6809542e58c8462bc7563a88debe22123c89bc3512bb59d22d191db5e22968e8c4c8984571636061ebb57d89821980c57789f0b4cb7bcc302d8adc3889dabbd53e9487c7073b770bd9298b6c2ecd12e5092f93eddd49713ebf2ba52b66f4f2e39fbf85535e7f3afdf3a301e3253bb193a4ed54b0ebb79d66680b1e57c0e8624e511642e2333d485ebb05e004d688bf0f31103829890c99af368bc6d9aca6f7628a11c2abf16b4b26ff61aab843b1585a19df3648f461ac8164ccd070e38680ab34518830e8697002955c22037509019b55972b350ee4da21a06688221f01b82c92dab2c010a9ef5d1ef671c81a0ecd7136a43ea0af123c841f3461f0c0f6773787aa73457f8e64331a5fa6b8c48e33b8ab5de308f0e670ffbd03466501f5a62f8587d6a72280e717ff5dea074cb16142b30dec0b76d69ec46e78aa2911fbcb85b7270f57280704e5fb13eee41558835d60e940e7d14ea649cc3d3f30d195a97d4e858758bbca3bce0d8bac0474719cd13762d8206eab4ed501ee6f1609a049c5fabc1ab150a0cae4474caded0dcd47044f64c85fda7b327887dc8e2684017e268afefb90ee3df5a0f7f5a1d0ace0ee979485a0c1c5e4a2a9cd29cff31657c25ea07a3c70090eb7e21bbca0d254035f4451fbb4cce72d87144bdc6081274b6a0ed738fc06523862b001692af3460420cca70c5c516ff75b708d3534f507853013158031c4627ecce8041dc852266b6db547cae1606888fb26c1b2f3e4600feb0cce8c420004107af163ecc216c2a177e624d4cd7a7382893c62f1e80377dab03e885f0644d4f6f959edd4ba21956b11ffe7f21a688d5c15192b898750ded54934ea32319b7cbef99d1d576e925a47deaa0847a4be3c06534eabb85d5035dba3bf1277cc45b0d972ff7d4dc68def0f7a20c87ff7118cc017670cfd5d42342c46a11f5089b5867ac3ed179ad3c1901b455da59d902403f824929e4140c33ebdef8e099692c617bafc29ab89311f88880f9d237f7340ebafa6081650b94984d0fe57d979de48697d2772d1a1a128750a7d5a45ff6df1108906fb3d88e877be6d346db4227c65d4824226dd1f3729f673309d9f2ff9b01c5564fd6253128fc7ef52aa35acd61e87e7d0cfdabd539fff51d2194279e269b8e3acbd53c0b26dd3a3e7c8faf9f0017016fe67902b124f20fb6bd9731614754ac5c3dd036b13554dda915d8c9ca9595a0e59ff2cd18e3dba1b48686314404e8a49a8bc4522c3f332a667b6eba3dc23af856c810bacd0291d1c2296670f105d812cae0296b15e44a99de8e13afc433b5b2e530501d67eb29a97e5f34ca472b98c235edb1740229e96f3256c6d3bc7f6990a592d7391276192fb50d351c0e282abe2a1ef66cdb6c6a755f2a70756aceaaea377f31c4c7eefdc74474cfe1a6aed60219c86d5d1bf13d5e577eaf2bc9a9e0771c1f3dab75d3a422c7438acddc810821a15bc811dbcc2847456469933e7593c403b1df4721044bc9993b8f8b5788157795e3c617b329cc9956a34575f40c58ff1f3d5b4a3adb2d0da6a434cc6a0728bdfc75a1fc4b0b44e5e6ebb9fd97fad09dcd5d89cd389168482233f751bcf4a4fe1f3aa572fb8f89d6f056471ec46e34604ea13305812fbb23b54141e89b6af121e0b1488af97e9f4b6b95a08e884636fad1e7f00bab6cf0a20ca51c586441164f700d29358ce8fc8d459c956f3c793b2ea2bb80b15b3280e729e747d7e743db9cbee4be76cf7be9e4efd4a961b5c6b964d0628600bfd8c58674d83da99cbc6b6aeb8f6ad68ff3ad476d13753e4a11aa34f692dce807ab0716f0848e934887976bf16924409be1f107ac412ae5a096a76ed3babd6d87d2e6d1bb5d980bcdc6dc1541c58f828eda55332ab642fc0b5e168ee6a5203fd6169cbbd6c9e1b3388e8f1120a7bebd12b0d25a8a580e4c5e6903e9acd117d019ef5d741983b2fcfceca92982d8732979a1b23cfe6bbeeeb4de322cf417221127e6deaaf2a3a594e36d9bc60bae3642028d6f69ed2f8e6ebe8aa3d47ebc6d2168615c809d46dc64d3af21ee7433d53ab47207dbcb60f3c28e9027f6f7ed6b989a7acf4992768654dfa06d24965a4bc345242e109d09151d0bdb2bacc3e6716b70291b2aa1e3453475708cd051e114ee5532cb58ab00bb11ab9869e212ecf6e58fa3fc7bfd5555a660d55ac8906b33a5ddc887c43f29213e8d0ed3edb0c2faa33a208e6b949ca1cd088b6dabcbd69fe30aae8cb531438b00861e5bc2756694b4c72beac2c943dfd2fc460d77dad8ad3eca9de53839d412f28b44c864a1371711826eb729eb67d7e10b3610735074ee72529e05777b3ecfc4f9fc436aa00f886241cae75151219dd39e8e15ddf8c02f4d3594d967c70c70be2dfa3a5734842443f5968d8b68060e92c8fee7e0b27425afd6768e44321a67ab747327fcec965fa9fe97a87cf8d5d46f8280374a61d977a7e9e74091b1af09d2cfb4f3c883e14365b2604fb5aedb834734f9f4fddb6b84c6827dd6f630a4e607f6e2b94626dc012746f4c62fcf96920513245ad7d69a239e77b8ded986b1b18c9dd785e447872f2895336135ea5a2c278e28eb9376010b139c6131285488a8e4f7b8ab2317474dff3759a594cae960f90b003aafe8cbe49332fefe86ed4c147e4fe671faf6bb197031f1fccbca15a2d9f9f7aff4353e28786de22929e8b1721abd33edaa5e487113f4519b86c55bf7cd4418689b16164bd4e979a46559be101da184b2c20b56be43e74598cef78b7b291edbd9fdaffe5be1a59f08e207605f485e81dd5be5d73ed6bf6c3d028895ec7e68a42200b63fda5cf86e8323136089bd8581bb7737c00c67e911bdfc34ce8ef420f5889219183f1e2c3660860eeb4935d6c41b634cc7ce19f777f5cd3901ed5a3587c897a9ebe649f92a93b715dea95fea1891f2866b8e50fca757d945fe101c407709fcfffac906bbacbcedc837e82e55e2556c0e8426dd7bcd2a19df4f40f3f12edab22dd5286fab57b9b89a4e6de88872a762f8317340d4b93f367f9a3d94fb5dd424861b7f886fc7d292faeded19635393d1b34671c361ee041e9120b02ba893228cc464567d3fa7b6c706141d909064e7c5e85f186a2deb838797829904a218f041238dca045083dafc9f3f5012b4c4596296f32ca853ac423f0a06180aa4899a1f5b9c385f2c6593b56334d23e8efc505fbc274e767324f0d4042ccbab026d839ba1d42052a6a697215c581c0cc8752ac09f3b7e6cd0b3f9db65c176dbdffa0c1edfb6ad88f231b4b606cd5a8fcaf9bfabc46b662d67e7dd1ad8dd177236fbac71777b3243a836f9b5ac17215e884a02169c2805685aee2c5511170068af9ec2cc891acee670805e0aebbaca6d2b83f60dccab0356582d56f9470260daaabf6c64201837e6fada402d424161f09546d562f7751e4f6b9f057e4517dd65508d7c1b0eb4047ce6be6c6bd7fcfa06c95e376bde744a16858ebfc20c12d3dd827ffb6ded6bfd3207677fa498d4dd74af6b1e830bbb77880a0e10ae7f33827d76500cf6dd0badf1f021a0b9630be42d9509cd1d689c1b575c5e91c5895ee50cf153f1b1f07a85238dfe707592bd635a5077ed5eb28d9901e5d535f78b029c961e78a7a14a263f7fd0662a051f906c723cec102f6cd3e97b27a65910a227dd955ac641aea9a93a10afab08374b0913ece3bfc38ec2279a8a03c539f6e36b3b530dd0148843c29ff704ba16acc5e679b9ad95fbc26339e5f778f94007fde87b3ca4b060e845d395381c875f24e1cc11e2fdd50f88c3f5d697e7996fe447e55e212b72d52928079549127b9fef659472f2efcc83e75c4931af9662b34fec8bd932991ed7f80fc09ae3b25706fe05c832c8d77697f1e6d11b7b12b96ea02b3a8c09042dc34a097603c08d1c94e99b235fb12a726572f91bacecd185be22ab417c322cf4b6dd54537ee39df029adf5f1215642b3c4e8daa1fc2da524e9d96bde9482937e723c0f9dd4da3d8c53842a4779b06a9a557498437c89d3197e0350fc5688f1f0d94314b630215f2a8745d221c916df1a172184cea7fff9782141a264b457b9f79994b6639826cc72d08ca84717969bd340b65dafc6e47758470638725cdcefb411445bbff6ff1c59382377256dcb2d4938c92304a5d406113fc170beae3444791578a78280e7d77d56655f7f533d22340d6c78fbe59807c9c442c992955393e381fa951503634541bb4111f57c74e174a7c965e6f8626c5df0485ae79ed326e50a64941ce8ed1002e0e299e4ef5a05338cd370157773f5146db746c2be3b777bd9e9cdf0b315a514679090643c10c9fbf0534fa4b91d53a04490fc23229443925b1ec2aadff292e70ba9a46aba72b49e1a5f1db4e29eaa8e029acb47dd35d366d7a59c2f08c47f1c21c72b9f1566514d8a6f4920efa18f2c264a55175287d03072067b0b5da5c08c3ce5095c056d0b8bd7f06dcc9662cde911c4a71cff9ea764d9b16e203e79183b2411867f0359644f696c828bed43770c93168d1041b62003573314c1fc3c39b57179523e5689df891cf4aa28f0363f930cf0ab18648636a22d94d0c6bf900ea2b4bcc092b0f49183ac0d512969c966f251c472ae071fe0761989818b6d87be02c37c8d864057a5124f12f87a079865ac3911fd1cdc36e1642f0ef97ecebafa49b0dc6caa51fe191e491902eb60a6eeaeb368faa8cd135aca8d5c79c72223dba9013b7675d965849be708950a135f5fea0f0976ed7556fa2e7953b61b0cc6ca1bf16ec9c230203feae2997254a9dc8ed3f8640d216b6ca545bee08e3bcb1caf501be09464e7ff001a9a001eee595a58ff71cfab52dee972c0ba2a8482881e4c2db16ec16dcb48274297586071a877660895b091fb125a970a841ba844c84ef91ae73b7efaae33a6a70289349df8aec565626128956b6e2b8dc5bb7d90997f0bee561a571c68abb11bf9425781039be6418454bba9c1d70d2b636cdeefe3c3674b789129f8e41ad2902180cc24238098b3f246b0ff32731984b108694e7cf84909d9b5ac438e261951ec202628a98220ea86ed824b6b0323b4ddc8ae5c8bfae21b9844c764009b7ffe9c99d43838344cf57dface9f135c769b677806eaa380c669588b9e91105158975fad3f724b83bec3a603b5a1afa31102218b6684ea23718d536a23fcd6549ef28bd482aa67dcc605b9a11d8e784e023162137563855cb92aae012792719a75a45ed697d98525c80d53a313ae648d246a333c4a726eafcf6a82613fb27cea287323c9b998047f9a8154d9b3898080a251f14300614f665d63e9e4ef63f9a9c76f8fdf9981b7827456718f41f2ae95b20e94e31aedddf874acd4079184f094bbd9d6376c653fcf6b8d059f025e29facd7daa1e7a1451d53b50c105bba2883484f9caea627b31238dce496fba3b3e2fed164081b05d245278a4b554bbf309287dfdb7173eff04addec6ccb1d8dd3347f6c5dbaea0890828a8f2e3fdd44c2a7abc34022aeb0c0904175813b193758c506f9e7675bc512ecc0c17bf1275ee9027d23ecc2c3d5e49c7e3258461874f7fc4d0b001e489bb4c3ccdd4fdd49d762a6b36482fc6a0f2ddae74c3b825cddceb774466e302375f56002d9eaf372cb62516f08a08f8344f2ab9e31dc84e1195553b660c8b567af21f94aa9c678b6c1c148952aeb1202ee8c03ee9a21a4e4edff5df346b5c1dc36d19cbf3343991193e3a4b61378267b4d8b6095dc4ebf4443a9ff501d02396184045d8d269b4b0bf02e63450c2500b02913e24e577aec28da1a7df9fe4c300051b8dbc122607c0880cba4a67a9c862bc2e222c92c2f9b0b199f687776e295fb974e552436efd05e824761e7f83e663dd994f38cf629c1992bfe6d76f3e03cde9dc35f10443b2dcb99fefa3eefc78d9f8d27365f75c12893d2b86dc47f08169588c6d8c2951389c4733363b713022793142f770fe96a0907bca56c2a5a8ea96912be8b18cb755a71a5f768979e13d63a1cfb35d325099c397497b9f38d8a1000dbd4920e8d0c452679fc257feb375a0c2cb0b156150ce6fb8c138d637b80b3afe51fbc692b69c3ddc735ddbed120cbed6d5f49799d1b954c9b430be1ecc654110db566da06115c452bc1c9cf49a2a0f825057556454a6e9d1116bc5730500c58ba6a2421f93000aba9e3da2419d79f9aa18d19b97ea7657f0ce736b117288ac9f02b4b7dfca13ce5689b48bee5a3ac94fd756b965cb84588fe664c63b7f1d818367b430a1a73e20a1b1b72413106505427480180e02757aaf75b33670c98ddeed342e7787ed527019cda2cba90c527bfea846659b91b39adbf0c3622fc10c5c8c1ec4971aa0b1e36b1808c78a57cbef152624111944d2d6bf1d89b63d718c9d8094c79caec756da695e2697488ecf4f791b93d4d062049e23fc56275ac9438fd07d4013936f61b6f18867356156a4093462f038d1c47fce0ba257ec520746d136af782b5ed2b1dc79df76aa87af9c92ad91f9f6cf0749e4e1b484910141c437936cd8c729f669e5ca2c258c4bbe0b2e00f7a70a6313725b7226b7bada0e8ed3234c9bcfb9c0f296e5bb5c450d40d332557b3c76326b7dda466f28489aeb141490325caf7904c452a7d0fedcb60f8158f72a2c8ba8639b457ab69c4e974bcc7e4de08e7b6e56b55f0a3783287049fa1876177d70295319b23fd1ad43ea3c63a1bee3a6f679408d9ddae2545c85d7b0a72937347f014bb0bbd3ccec73bb900113df4554f7438fb19144a7c43e1cff18e3d4504cdc12d7076c6904bc1875987724ca530b65012094b5ea70d197db2a37282cc116da1f326258164c0768b530d6be734beb88acbd68153189239f585a1f4b5215fc03c6f0000ad9f7a1ddf466ba4cc066ff4b2563fba5de534ebbb18e39924b22c78eade920f871c4b073306451efbdb06fd6855255b565bf12e4040b3a806c4588734b519f881be04d74fffb6bf3dc33771972dcd04312565b7a04ede6b688e6b04b7d4f3053093e2762ba3b2a5e40eb1d72b2521badf024d419d3edb1ea02c2d557da41b37db9a16555328a766060fe2e568ca217eff710fa2efa3384a9557fe368d823098cb56b74511fa6e346dbeeabf39c067cffe790ed31cf0cd681e9230e3a3de98b3bc49a48e97b739778599324a9586a7098b89d38de98deafdbaf8879268b75d38d23a3d83ee4371c7142d3d3a42fdadc2d05a7296ab0432e12bf7d0fe7153b2c5b14d4e4e9704dbce0dc57dbb9f7209527aafedca0c35823eae40a1acecd5962b38ff1f9d507646e2cd77cfe9f2a4780fcef5dac093ae30ffbfbaf5de100d62778f3b039af990711a25ee46649e12fb31c2e21544cf8ecaa20c401c2b288a6d68902f5cfe9ad49b3711da4b53e19efdb1c8c196c5fe500a6e7cb09f8cba29d887441d461387a526938b2e7f534de37cafd961bdb9dd4be7105fa43ccb9077873b91f0971bf30551b2d6640f8b98e03444cc3f44d3981091b05b85dfd5c05c9519a13b358b85a2f158e1d62ef49bc61a59ae02f4c57992c3f123f2d0ea3c3551b05be2ef182f101682fc3c8af279b01f1ea333ca293820d3a1f70fc09c42158a726584a1622e62d9cede5cc321236211e52737d54b14f15bec8cc0a74df9fb16d54c39a191319aa94bfddb6736f5872a88a9cbdaad2b0d0aeb2a3fabd9487feef8def938016f54a15f37cc04943f45e9d6e24daff547cc79417067aae679ef592c65f7de182c4ae6b3c57e6ed0d7059a1c03f72c147e8e57cc4d381b187bacee99410cab686ae691e28766df15263df451402c5a0d939c4d8f21e7aff1abeeda1deb8412a9ab71996218285d36205c318f29bc57d5825dd9dce37fe3a81427582ce275a5920abe64cab0c07fb5c785897fb54b9b66cee4da532fa4fa971da782f90acd4965d3b83cd2aefb1b8ca95ad54231ce22dca701aac28382deaec982e7ff10fd03740ebc88dc7a6ebf831b4c83953c6f20977e9cb02375a2bdbf560f902e3b797cefd57f32eea4216336ccab0cb58c858507370fd810172847182aa232e84d2f6640c2f1b1f19a45f1c6a341e680bf4865598dcd5b449fd531a8e579369e1d4d7aeffebef22e477a06db6c741d9c8e4604625b7197fe5113251690e49642efed329da25546c6b343b5741acb0561e49169fae41ce34bd6a4eca1e1b05d8415e93719bbad1a4911f688db849cf9e67392d800d0ecfa89219c1b2271613453b286fde055f8f892c7aa89d1047b481bc93265689aebc9c6e4ec4a540726f5ef93f2485f3342720c624020cde393db4674f071a708dadb73d3cc3673c2ee406646fc0beaae5099ad9a7cbd82d5ca7d20d679ba7331710dcd3ffeb235c7e40b607c5e1640031770862732f759f9b3a00e7f46b5118410e6e0ba98afdd67d445ea0da9db0c522f620bb0d78c04752536fe031028a271e33a5d499390daab1fa7351ba5887549b4e95d3ef8af5684a0a4d1588572d9bf3c5d2a035974642f6e458612f96669bae75b3b03955efcd7526a12d6a08df5ef5362c8682b52b83f3d78719fa08b1f58df394c4e260d5367554e884d79e0c3e0479469f4b99239bbb086a328582e4a5fbb85ae304c799855d86b3830172fcfc040e9b73a9f3dfc1038bf611bd8c4930247cef6c4d0fb1942dc1753592e1d06fe0eea322a4e86d93fd9ca34ef151db93b8069d05cdebdc0f6a47b745a11ece5edb2a80c8a41411cf4d7fba5ab9c34931b777ac49577e8073141b82f5adacefaa566f20f50aa980bf8861d4ed1c6ecedadbf7b9162ff77a5be560fdb4ad87ca933f14584ad7872ef083e46e68575b69958b857c73cc412ad970769a34b436b0e4086676ee31e1d39d9b394a4e8a64165a69e73b4c06f9de241e8507f1e3d86e57ecf71ee0023c7b680476871b54283f17143c51b9497c9748767e8629aeae9ab89ad1d71391a3865b7c5c6405e913a5b9744ba76b9ea477e17df16a142a78909f2fbc4a5103c171c1aa47a5eb63c9cc2f08d8028699019afc458c7dfbe90fe7f4afff25ecb7b23953c8e34e148e968287c3481674f2a4e870bec16a1e059168ca6ca12e20081a227c2f49fda4368f9d676820b782d0896089add98e878b03aae2277553a91116a1d9245036d5f307bbb0017b93899c71595f2039258dcaef741adfae66fad4414853a37bebc227f43ea1810a4cf29a24a5d42b6e03666799d6cf9020f6c43c30e38861e3ae28ca66f8f3abec175c1339cbbc4ef051175b4729b959dc5d84fee4cc4832dd20bddcd2aa9bd5cf6d3ed862be7f372785e87e7212f693168bda12c02995ffa5d97f25178bd13d785a8c8244486aa8bfccad7ccc3202eed2b7dec9cf26a4da5738257f0dbd89ac176ffa4b7c3026e3103bdcfb6d1db4e94c1903990a4a4fc4009373b7a4a47f59a9dca4e2dadd036e93c6876c8b8da4b709c97e8da5492d3058dccbc7959c27b84c868c42905bce92151d60fa5d3a2612098d0e2cbde1f9d61bf85200337abf3ce0a3970347f6e76bae26ef3607ecdddb6f5914cc3fa510aef6c4522d88df5e294f6b0cd6d4f504cba9c4eb34f8bb96111c9f204c3e3368fd46668afb9669ef0f3f121c1ae587a31c499d880851d48b2dcdbb7339657f5e0048397d20a293ee6ac2217c9ed91ea6bc67b3867fe03c4c31713f7e0fa949cbd7722b30f1ac4ec399920c0bc8bdd6197e4cc4c84afad8e67e503026f8cf4ad6d314b342caa83dc4f106d890a8d206b304206b23c5e8e1ae246fb941f20f721c86bf132fe2af64176570391dc31e535a38a1f1d5eacba9efea1c8b5631d473d6d66b4fa056fbd89416e178c8b08736d9fbce7f663f4b38f86b6428e8cdd182ee62fa9d93002a590dc53275ed846826eb53ea49944ad3cd5f5fdfaf9076d4d55ba1d0dd8fd1b868bf63ab7be20502b540640810a266160ee510b310ad760fa16b005ef79bc615f6f6fd9b6c0809035a634b962ad649f98247941e269d19be1b31460922702b0691c242eeda4fb5705b0fe075ff40560dbbdea1347fcb119c65f53eab5e22c5d558c74498991a67c5d451513d6dc81b504eb48d598ebe1fe7582a5fa89fbded0da03f702d73a6d9ff1e3fc1781b618e2bb7ae9d92266c7b4185df0ed041121e9b6d13e4b83e981e632c10216f02e519dd81be0df7ff8157805fecefc9c74c9a538a6a92f6a333e563b017bea2ac9a91a2f29fa5b7a61c4495263e2b28fc37d6114bf96bea5c4c20d1c89a4a035116fc88d2d02cca50761099425d025568cba32430ab6b404879447cb743c8e8b299fd741e169c7a8ebb9beb3484bf7010f6f695c518356e1b0c8e515aefb8a4c4a964d1b2dafd4d56e81e7317e79ac34d7dd337a33ec3e246ee3dc14294c9c19c0f7d81ad28f75707a7aff82d205cedf909a6ff486426b27aa846287499f9235c90467654d5fea5466293545a76f1609a7c17049d8c21c3c2a1af818b006ff55ab2b2dee1f9fc3ea835770c32954fa7c513e3ffe0f8a8a6185236ba9f4e42ff531fb7156e389ba67ec778d824480756bea4141b6e78c26d88be380419876fde1d377c0706b89feb46536abbfdeee259e0c8df0f8d33972ca739ce003e1c74f8988a5a3600152fecb8554665e4b56bd26c3c61862d8c5a2f246d9a376b971aba929b4deb67d1670bd435a4057a8c19b2fd9e67b9a140913d32463c21eaea70b4c274b7d1d05448c363ae8302793198f4fd2b7955b656331e7e842b18f36cc789f0fa859ab10e08719b0b3feeb20232e48b65180b90045f70bc11ead9973ab220ef6c0389ed0251b90660a4bbaf21ab1742c53cb596450ea783929d60216ea9789ed9fa5107426f1f0086b0e1bef2e0b0e136817f338a63a2c25de1742e8fa00fc7dffcf5532ad2139875b9a91d9dbf1e966bb53a09c6a438a2175e22c8ae44eb987f8925f9c09b3b87d55472acf2860534df7e30d70a4fee370f04114cbea8ce1fd6a86434948576a2093228cb9316e3f14d5a7353ba5ef15b3604774a963d360ba6eedfdaa912116214bf0e922d58036425206a383def71652e369b80d9d630b9e750604327dc4c560c70daa8853c80b0d39bfd26fe1a66ed99566101043dc43996c83b6f8e41ebefca5c070da2b90790fd7d821d3d0848bbdb25d863e1a6092845b31de18f12d841f051cdaa54d9c029fb1384847725fbe0ca597486ee3904a40d131c6e766d287d468ebd9ce7c8c5b3ca7e99e9f891c1abb79a4b0b06b4a0158d576efab1dc0b3ed269b78c0b20c6448f5a14b41c52cf20a176f1c737eb9c157093558463a0f6ec7de5f2ddf76a186cef7cb2f459c80ab2b71cdce4113c9a2e1ee5484812d33393086e5aae8c8c5b19e2b09bc879a9680bb0d923032433dd6822494110918211c780a68d0e33a6ae17fb64aab91d59ce2ae5efc65004d182135f222f57f2d9141adeb4facbabfc44b8610e8b12c7db414c0de7dca04f20086433bce59a54a8b26186286bbd9be20931af168e4bffceb5b728086032db3b8c986aa20e6241593d41503891b103f6173632c7b8be41557b9fc83428e917fd535e7dfece56c0be84af4acf7419c4f1f49b4a0e402ea4796234e623c5a9faad781bd53fb8d40da67343a0b0dae83f2f49f5f1bbd4b4bf991d34106a4b7fc5cd0830c5215730b0c441a00debd4a896f1e3b4a9860d73b79878d11bbfcd5783cccc7a2b1f7725b66eb1aaa2fc52eddf20a72d52de3c6ae5bb176e19174fa0b2e1107ef11633d56a42fc64cb6be21c376e24ab3c2182eae18588eafdba51a98400413d660bb198815a7027873a24964358d7e8fe6c1da282af9add9963ed191ee4f3893b4b7a87fc8e0b3387307f305ab4c512b78e8eb8c8af143cf84d056e9d7e5836dc00a1463fe85fc857c56f350e9c8ad38782d4f8626c6173c2e3debfb87dabdda7c6d9b7bee73bf8624c643f2eabb80350782b24113c777290f594b08e97857fc0c2394b72728bd19c9d6f5f0212fb5bd728afdc25681f2f7969a115a0e8fb7573d04e0ec5f2693344ecf185fe29649af2ee5b1a6f728498e0a94e28796d1657252b5c7a4b185f156477c347a5e9f745b9fd838ae4e5c3715bca374c47d298aebebcea6f1a1e95bd793d81f155eeef4ebddcf0e08c73ee6b581ffa9635e986e5be4616e06ed2dac7f84a1e283c4400432afd1053703cecafc23143fff89f49a0a0072d473a57908493f4a87593d2d612a0874bda289e2b30cc84598a594c05db120669c7948958a92d61fd6fe19d8bb23bf06cbded1a0d475218b358fe5b89159b85a3a00f681938b69b3364080151fa0034e46c068ec5eba55850e786bccee01b586d12f8c47a52532d405d734ad07fa398093f06fd059edbcac00d12693a6ff4e5d05cc43de847d2973c6aa91204cfe27862073842c5e06069902de735b9d2e229a860330cb74fc2c3811699d556d7c7d2fa67b162f7ba8fe97b42879258d88cd9f7cb822f5ff51ad88e472b59384505d46999ab08bb8960880a62eb20a405678813c86e2e523dd56b96f37cb33bd07a47b626d656422d015825191597f0ac10a97fc337612fb2be116d6b8c1a8665101d2c9331a3ff444219ff85b694cfd5367eb540d4e8de5610662d6602b3838154dfc50c50597f001854c446b656a789a0998403abde064376b720917056e0f892431286dcb3b819782bc6c2c86fdb2b6d8caee6654b6c0f2963f85dbe05c98a8aec653f574dfd1dc48f4373a1426b785f91e6670e9232059af566462c92339ea0e65b5037866e6103c863c0b916f6b8c1e64c2c18d09688d2bee8ab45c1ae64516880a386fdc8fd6abb34183b5421cd1bd6bc3a6bd18c42acdf00ecf9d54e7d7cf9baad6dd87d588708f0c25286c072eddef604095c502c06f93138da2c48e6f12a55c78b80d66c6b243f8ff52a23d004ce3f054a8ead6244769a12383e22ea7c6caeb69dac59e475137db25b485841f4950b402d6f8585b9d63c33934cfdfc3c25ad832c58d0d971ef6b0232d6d0ed6663f19ee7f05b516c96ce9a132b46ba7392413d5f9b0a3b2622c2b68d60066ab2e0de69cf9723287185d4b52dfa5c77165cf513d46195a640978f74c4035ad78e1b674b8839bbe0b2c3323c5e8a37226e18eac17f59d96a0d537afac7cf42a2a619c84763750f7d4094a766b87f1f339b4e7f208bd88b5a077103b4662aecf0c4d85068b0c457b34c9fa480e189deb4e14397262138e437e66936031f6bd067fc39ae58656f31fa92adc66c3c072ece0c2fda2774ef9c4647160f783e29292fd81587f4022b92a52508c9220152d6687408f45fe6dc981b61f360eefdbcf24b091ec76e76a7c4f022db43747fae04009c39e6659ace27f06503a7fcfd61bad2bb517d50fd4800a3ad28f6321ce9fbeeb8cea40b2f08b0d2febf60ad3183030395727975f2e703a9802b518dd7f3d515df7bab0bb99196b71ac5cec7e3c0e0b286d2f0ea6c0324044c115b9904dbf808c286f4247eb03677323f5247e7065ab546a8f03ba1cb3f6c21e4647a47971e6dada4345ab208cd7c5f0490a4d6849c9c93cfb2d43724d59a1fab773192805d3165c011d8801631cdef632ffb1b6dc54dc81e8df190b9a598e8a472f859073f3cc64f928aab6528ef978500e2724b0dfe5390ccee960d77bf8e1f29108ceb49d404dce0d026c44ff802149631d75834f760ad14013a04a9a3d172e89d92d6bc98a20979caf8d340250979030ce2fa4017fcb978563efb96980f9b78c19db86deb50828f56a2189a395af1471993fe16c0b28bef78b61d6db265da2d092a886c932328f99c8732ebc90b5bd9c7712d480680192e9df6867218d9cd16704f696641306defb15ead2fe60328d308e49c8ad8dd8988dc785e766dfb2446c7164f7aa9ea721eca550472d29887a109191ba2693fead76e444039976ac41b4675a99537c9986a264932c8a959acb293dac4895c0eb851656f175b5ea976e08192e014491d2360d23eb1e1c1c2891c17cfd32ffcde0f283540ed9fdf7273a38980b1cdc303b3631f430a2477f804098754bf88ca7fa570019024bb4171116755a72e502cd766ed7f76ba75bc4c4de9d0627124e4f9c283006c3f9927f94e149c3d6596f29c8681bfa74df4ed673aa20cafd4dd9a06cc2d333c2dcd91a91a81c7aff4b7a21bd9a2d7249853274fe14e21fb8e43386b00a24ce4fb92219537a7d82aa9ed7fc1dd085e36761a4e6822cbc7aefe6643a5427c8d82f917470af28fdb3969cc82156e4dd97e6d7250e36a89eba96fd1bf0d28fd068342e84527cae4b53f74eec76f7cbdf0e1b1fe26517c74b489ec0902f60a08af5223e61dea06e147208808ebabd917aedd6e1c540a2956920d2363ee90874f05b62d8d2516faaf8d7445c6646b5787691ce9e380806630119615564ea7332bfc6c59550f53cb0f5284bfc1a85fb0117184bcd43555503018cc2bbc89d7670aa94ec5033bcfa70739fa9dcb2d752f54fcc1d7129cd6c1685d69b4042e86246d6a16efb0322654fb44b831edcd06dacbc2905b31b76e722b024cbf02dfcdf7504fa98812bda8d217183a1c24d296094e678108d256b1228e684ef6f89c5410e58e216141f0a88b380e95003e8aca7fdc828135275251da3122c39b855e2f539f073c95675be0818653b429f1651ed26728a8902c016841c2b124a5c7de20f9f5ad0e2cb5feb0bd58e05461b3681a221be90ffc996ca0156913622175ec0bb0a5e9e41798a8424e57e72cce433770154a3283227ad7280b4cfea01762fe18cd63b341433f15c4cf3eecc5aef99a313a55da010fe052a894aa2483f3be3eae6dee2df4c36d505e1f4878de0d2a11dba2f53ea377f4b7edb89fc3d39440cbff9a42fc3ae105e63201ac3c3447666f57ef5ba07880f903ac49978e190172774f7e9a526316a349c6ab4747d2693d37604a4a6aed99f44012b92bd003a5fde42ac51a1dcaf81c1cb22e44f65c7bf1a2ef877b5caaa7c43e1dea921285c71367ee8a4f2bc061a4101b9f9f04c4960d7a1ecc1511eea4437612475156a2a31ea6f7362fb077b412ae908ddad943bec09476c53d2bb7ee0985e142d7f92125c217d586f94074856ee604f29d01a2eb959e3ccaa202e56fcee84f221ac01d84ebb49e5a6b9b179e43c35ef16f24fce6afa5c182fbd2e2a2d1d97c1aca22f84f4ca93e5bf604dd3eac71e9948a2cbf1203167e308d850c651306a3a04fbf9403735e37b97f6e7bc9d15e760de450f5a1e6d35d46e913102dc4916fa97ccdff3d7a755ffbe9218eb9b20aa9858f36fc141c653680644ea5e3083de33dcc658daf77e824c5c87f1d66c0af123b17704479d651a35ba646288eff62d05f1683ceec7bc82c185850233733543c976836f2140996afff64b15ec70cab3f80d071afb732642e37af006c96fe01589cb93247fa7852fd93b72eaccc5ad1e3932cb7978fae476c3363512e11f71ba7c5139d750266a31bad5339f45b2a1eb1b92a8d714f2eeeba58966939d72d33bfb9f75d0be8a476ca8c26639d1b2b2667b9c28647c50098368160a74546378ea864bfc371f56c3448be0b66cc3a8c8b43265707612cee828a8c9ad5fed4a503cc326ee177addccf98555988fa9bb7beecd93ef3ea23983f4ec6082ac56b4f7cbe6c732ee1f93347cd0cf788227eb12d982f49340a28db2907a080b5ba8621226c8a8bf1f27333a7b336e6b402f453ee84035318fb3a52c33a94c3b5802315175740f9949447f76051b96b6ff77902bc777f8c95d6c4f4f4819bb650dbfabe9e40e9c86bbc9e78fd803b573be1da8711c9e67fba40cd7021989f4a8bc317b94008324317fd1a244d89f0b448befb004ec0075bec81e90dea0fd71930a8b723a465e76522a1bc780d64f1f51ab1333016ec9c314d6dcdd5a76ef55eb0531240ad85b35f914f703d20cc73790fb1e7aa34570cb39952def901b2553f01ed3ff5f5cac73978e8611383c6e0db870cb27d7f17bc52ee4efceac01537fd7ddfe9e6e388c938cae57310a6e10c8475587c8b746e6fd629ccc0a2dcbf804c522c3b080df42d1138a8b7b921b8bec06f7424434b5f2bd2a7e6c6cbc33f7b9a215bbbf7660a164ca40334c07356e61026021ca95b34de6c6a775ef8c6bd22504ff555e9881bd78789531abb590ed4b1c6e92c9b2940e7a699c605b3f4c3ab639cec44d43b923ea0fceda80348e36184178ba94e03a31def2286910796565371d28242de8628bfb83bf2330e59588a779a870e5c2d2549500d09df2156f93f6e4827fbb8725c935130541de4b31dc242681105ebf54bf2bc26838c076e82a710dec3c053e97f5b9dbb3d5f087d244e00a6217bcb384c197a299457ba41b9680eb6723c1232e57f8efffb0e4e021da8c254f103d67fb842a7aeb51dcf192f8cfac850432470aa2af0eb059a9f4128a88920df99cfd674ffd00b4039e5435a02d4c6a5fcc708e83399f2a0a11359871dc890dd8e7d228423d561a1f5d814f66ba9d0b7b900a1bd004bdaba19882a245a6bf2c09976f83027fe94d6e7853cc9c77eac81f92d87156c352053385fe093434e291cc93e87fd637792272f8e683b94939fc95d455ccfb7094fe93966e628a041555e8f916b66d249ac0c3b62011d9f029139d92e1a2d0c99e84441dfd562f9a0aacc820442d9bd03c89cffe1e85493d78dd20cbf36bdb9d88e420673b1a1f189ce1eedfb6eafc469d07b6dce51166d836bf36e6933cadcad63e292d993fe8749f9981e913acc12946b69e4a33bc079a3e104e91d86c66b76b2c60700dbb8eb09e1f7e8870338ff7f1f6e0b7500bd35bbf9106bd4e3ad126dfb4fd06f0f9e0e7a6457b1de66b6117396a86adafc1854e95a03491bd0fed81fc62d312c01912e70703c801af737cd81a78cfa255304f0fef69cc92b33796587dc17dd952870f68927353bbb867df04b8d44943415330ccdd827198129819667a81aef2f86ccc64be8c77c402e1bbb3f67838bbe48daf61d39b27177391fc7fc0130539061f69055d8b906b03cb225c8fd01b41c6cf4c6035c0b6c6d763f73a9453c610a372ee17b42399a1f49fb632afd4d89b57283c110eddd9ec102bb6d30511843011a08748318a368835e5b59316579f74d9d74b68710e567795c66e672b9c86cdbb443af25e2f267754f8b353a45561d53ff1f204890334e179a3a9a6b5ba93b91e3fd1f7be2cfff3653641252dd60aa8f6257926bfa8474772507482afa37b4566c2765bde0cee3fb4988ed1d88afc865b2d2bc3ba1a3a49d9f758bcf9d3c69301309b2e5137129e3b53c53d8ffb505d9b29e66b776a10b6d4b31f1806d460345d2fb09d71631fc23eb9f696c6a2d6f7af6269991167a174933059c06e43b15b181b0a564d777d8d5eecc5cfabdc7ae24f64f9eb0c863e782019fa4c0195fa3b7d7ea06e74f69e8f930be0e6e35930de7d6c87b1cc6ad6d3fe0a845054c37369fa00af3df51ffc1411acbab206962a16c388357c2fa1b6f7944ac1791a78264412aa09a22254362b6be1b754a18f46bc13197e2326d2b9ef9ec35c84242bba3e59b5f6d7b0f90bd98e2df31cdf860e4be13c05295796ed5ddb22617f3cf88aaf3e31f6277a51758f03c13b5f33ec1e88478da2ca2388842e08d354caef6c3d874c2907994d84b26222d1aaeb79439699861a4da8a08327482d1393c92def44d7e6e1d761d444331d4570ca7b2a50b7eeab94dac8012b273f075af4c64d7ae3b65583867053ecb0745f587497797e18a2472bd485c495c9c6da00f1ee64886550b933ece7c043e1801f88e98222db51de92649502a7d8fdaa487d03eb3d44628be00aaa2255ba5d741ef198304654296e3a2aca54e2d8d489f5ae44c0fe6ddbfd1563defa380c65ec998d8be79f4e62ceeaf3c3ddff777559cfd9c726d67d6142c1d95842edec0cdddd09e7df39de1877321392b5c3aebeff21d7e63aa9e26c3ad6037d71ffec917cbe9a523dcf09bba1bb364ccdaee27ff662a789a0f1d62ddd24c5638ad7139b40b3822bf58ed24bdf3ff9646abf6336166bf2fff1974bda15be2652ab378b648252fcc8999f4559ba4a3ad964c453eb37ecfe714b25aa425eae44ae653b7908a6bbb789d6591d54a25964384fb06f477c60bda310b50ef62405c808b436366b237800ec483f6e8d6061e194380524d2861b7f3477ca5e3c27d083a3a6ae4b0b23f11adbaf395af5f01e8f893cc817a1af4c3a699184444f455ee7f4af0532bab25dffd024a8f6fe193960142d04fc018287c0e63641328cad282ad13e74f4094cea2a9412f3b9b3ef8dc350287ad82b84a02d376f770e2039a9577e3272b4779fe41738a37bd7f51cf09a059e28c3f831eb1c9580c039e2b97aaad81baf658e7b27caf567d8ac16f30cfaaeb4bd57addfd7ea22075ff725856ba4dc9f97216e5d228591354a48b99496ef53e102b94a9acbdf89da57750aa0e84b42db030ddf718df5d2c12e854c8d9285e659adf0df8e08fb57f75d3acdd51c41bf4ced7c372c4f62bf08c5d5e077c13c62e546a2df6cd2db9ad0ccc986380d10f4ba91777979a6bda1cf0da73af544021bc57a86c622007c7fdb1823a41675e2644fe9552f0799a27052c3a6bda7bf655341f7a0068fb0ccbe4ae386a11d3dad29819a366a3b952f7ab1b27fe556d3400848e25e29e6ba9b57d734893614b35d4a72b51e313e2765f75f1f8a6f9d64c707a1736e6024834a6f6175cea45d8ca56f0d603d172273fe737a6f1230705cc75018ca565915bb47d4fa04439fefa1526838404c1e5c6e5892080c98fa0ed36479bf5dc9b6f2e7c74c37f596dcb40f5c9c866feafe69ef211723f735db25df9febdd75c0d1e42eb18401874691740cc93cba8de9efc83b364ab7fd47d505db8f6e212f8efd53b5d2f0f2df2628d3bf0b9bff57c2cd1795611943ee4bd2d112a2e58beeab8cf04248ed6a0f65e0473ac74d13426457fac11e21995f6fb7ac7591d9f2b9ba6d54b22c3b407f98e537449945ed0ab7f5c05293d66e7d5176c0bff882131fcf73c4116a42879c46f2874a27135cc0a236aafa74354ffdb79ddb18d28092f0590d755fe0842021e95ee87fbfeae8b02f62196202f474dffaf96ca3bf317c797c1b2d2fc69f2f59cc8deacf6ad099aa06c5e8f497b0373fc9f8cbdde0d57bbb00901c705587569422501d7ce1c8006a1edfec546a6da2d1634843276134bd0d94af5d4a0649c41d25cfff9f44063199bcb394b2ecba4a8034b9a0f3aa1fe5d0cb2c0085f48c7e39477b14c54e9d784cc2380275ad00d110382a75f4381890cd883019c5571c5c3da16c59824b09264a3837e551e19f3ceffd97bc13ae63febef345c61169c194e6d343fdfe252403cbf2494d567a3e188e946f12e44a6c33f108db2d060493a768b1bc66527938d9e432d7ad1c60a67372f6a7feeed523499fa960e5e5862ae83f1822d80ed384ecde9326acef10c3385d9fb87ec1edb1725b3b419e307ba5e15554c89d8c8f387f9c07d4343e0a8e1fbe6dfa4bbdb36ba574057624f53221ccfa493ec6f37323157c1ce428abdca121ecdb05014a4a43f7548cc893528d9e7803658e42d6735852b9f3c8179c2e4f9d962fc987e61195c791eaf99eeaa8608369b53e74d8b33dfaca99ccc8a9581d9fefa8a3ed7acda631e4a08b70e3e5746127e5ae81693c619bc5ad9e755d03b1739e2a5a4edfe93e27cc2ab4b90d91b10fa90cc66b8b9f89bfe9eff5251594db2156d8c10263f7a8b597bafb89fa21ea62a2cae286344c376d790650d5be853ac756c9ae31bff5f5f9ad7bb80ef489aa1bbac478c02d12b096f3f9dca952accf71edd1773e851f693009754b692e8864e532ee94a3316c369f5479e5ed92f7c1937096a39481e976b6971a42405938810854d31da3a786a502e9713557d2e2acb1aa3dea6c53143c67c3a53e504cbc00a7cba152b495c77e0bca21af7e3737b79fa6bc6676651d39e2eab15bf2d388fd00a27827294a7356cd2d54916417c4355ed4d166a5f64a344dd7508f914103349a66338d4a699fc13ca68cce7c69a33901ebc5d9d8a8f7911a36c5a5d1df9d27470b22bec6006e9406d489eb526cc6263508cd8126c166058c91975a1e09ab4e06ad3ee36da2f974fd40f803d131120de7c2a6a05dbb706c5a48bccf6fab142b30b4d9c08107841c6ec6363b57921cbacd55066c159714143fe42f22d9f5c0d61df36b4d15e0b4daa1b1105dd3084803b20d47663c93c016a991a483e6200b2d321836fabdb752471c6b502ebca9c39c2a807efe0ecaca481e91a64d248e8a691fa323239c5c42894ca8f9b72da6a97879389be94ed6efbeea665f7f4a36f0a0effd5b62a4078806fb22ea5810488df7053c0024479752d219556289db81cc40466e4c86f84861bfe8439d4c564bad005573939495906ffa709df080cc3b087ce28451b08ba366cee02c784ab6cc1ee028f4125d69088e95b1c5cea339b3610bb614c04a29fd97e737073dbd43c6b4c36be72e7b589eb44340398fd4d20e63076b0ca6dd09e3a9117e1388d5226763fc3aaed2c7017692ec8880f8b25d257ad992edddb2aec15e3e37264d4820bbc4a089d78fafc42336e713e1ecf301f701fbc6628ed3f1fbf76034c77ae08ca5650aa5582996adc020c299899b285288ce7574e94b0678d26b1653b26939ff53a4ddb86cbfdad565af37523fbcca886c84fb70d34f3e312071202c098c41b5658d850253f5a41fef6e7b57b9526287226da44b361f249f486b6859159ea4ffb4b7c3aa1d9fab6b476267a887e301b56108db2cafc1a6e22a4a84443213c0c6196eef12a86036745676f2f8b299c76ad17a9e5350c3ae72dc00b9d72a14cdf87babda3e8e7d5f586243fa6cf8175c51ce9c273be139502d0a862dac4951225af4b4f36eb63a044d285b9fe00b01c0a97e12d632846efab012a17655c659ad9d228f74e723d95ad5f8c5b088382a7ced950cad8c3e23c2fc776a4a0969434e262dc27f7bde76cc2fd224950a8c89ee8e25b64bd746d67420183a797231e7dcb765eb25be3313d86921537b574402256b63869860389d7b54aeb79104ac0c95083b30c85086d3111a854809a004fa783b611c86632d171640268db84e04ffaa627a3b8ebca86a6ac4c61d8f1cd90f5b70dc46813875d4b2547205496eede486d1e16275e9bcc730cdb1a183d0957a0bf4fd0a66c06b1d750b0356edefe62f43f6baa4d02a92cb23ef82cec0881fab664f1e076bc3e585a0442ec32d0f4778dabc4265918801bb98b5b71112b6089ffdf088d4dae7c2ed29c37a3bfedb381cc1c2ca79be2c534555e0265a44fbcfe08a57a2256a383b851432cf55268884134d2f409951c953cb6d023685cf21a6aa6b034312207a09c68300632b7f3baf7f9b0cf49476a7e315031f158dd7f0ec9088c67ead7acbe1e93211a9db136f8594e01781e889bbaefebfd7c3d6e94d0670990af67352b8ada3c8aabb0ff10db1c4094ee04cbac7efa5701731fd1f75ab574c1f57c0a84dcaf02fb838886c7434f6d43386771bf3414dcd5ecfdedd14d5641fcc7b98e641c45547a85e70a3c655aa348127074ad696f66a0f3bc</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>traveling</category>
      </categories>
  </entry>
  <entry>
    <title>高级算法课程笔记-Lecture1</title>
    <url>/2021/09/07/advanced-algorithm/</url>
    <content><![CDATA[<p>课程主页：<a href="http://tcs.nju.edu.cn/wiki/">http://tcs.nju.edu.cn/wiki/</a></p>
<a id="more"></a>
<p>本章是 etone 老师通过 min/max-cut 来引入高级算法的概念。</p>
<p><strong>图的切（Graph Cut）</strong>：有无向图 $G(V, E)$，子集 $C \subseteq E$ 是 $G$ 的一个切如果在去掉 $C$ 中所有边后 $G$ 不连通（disconnected）了。</p>
<p>显然对于 $V$ 的一个二分 $\{S, T\}$，$E(S, T)=  \{ uv \in E | u \in S, v \in T\}$ 是 $G$ 的一个切。</p>
<h1 id="最小割-Min-Cut"><a href="#最小割-Min-Cut" class="headerlink" title="最小割 Min-Cut"></a>最小割 Min-Cut</h1><p>我们可以推广一下问题到输入可以是 <strong>multi-graphs</strong>，也即可以有重边，不能自环。</p>
<h2 id="最大流-Max-flow"><a href="#最大流-Max-flow" class="headerlink" title="最大流 Max flow"></a>最大流 Max flow</h2><p>一个朴素的想法，也是一个可行的确定性算法就是 <strong>最大流最小割定理</strong>，一个图的 s-t 最大流也就是它的最小割。</p>
<ul>
<li>注意到这个算法本质上是固定 s 和 t 的，也就是可能不是全局最优，不过我们只要遍历点跑最大流就可以了。</li>
<li>时间复杂度：$(n-1) \times \text{max flow time} = \tilde{O}(mn)$。<ul>
<li>$\tilde{O}$ 表示忽略多项式的对数系数。</li>
<li>实际是 $O(mn + n^2 \log{n})$</li>
</ul>
</li>
</ul>
<h2 id="卡格尔算法-Karger’s-Contraction-Algorithm"><a href="#卡格尔算法-Karger’s-Contraction-Algorithm" class="headerlink" title="卡格尔算法 Karger’s Contraction Algorithm"></a>卡格尔算法 Karger’s Contraction Algorithm</h2><p>一个图的割太多了（$2^{\Omega(n)}$ 量级），但可以证明最小割最多只有 $O(n^2)$，我们希望生成随机的切集中在最小割上</p>
<p><strong>收缩（contraction）</strong>：<code>contract(G, e)​</code> 将边 $e = uv$ 的两个端点合并（merge）。<em>在合适的数据结构下可以 $O(n)$ 实现。</em></p>
<figure class="highlight plain"><figcaption><span>Karger's Algorithm</span></figcaption><table><tr><td class="code"><pre><span class="line">Input: G(V,E)</span><br><span class="line"></span><br><span class="line">while |V| &gt; 2 do:</span><br><span class="line">	pick random uv in E</span><br><span class="line">	G &#x3D; contract(G, uv)</span><br><span class="line">return C &#x3D; E &#x2F;&#x2F; the edges between the only two vertices</span><br></pre></td></tr></table></figure>
<p>显然 $O(n)$ 的 <code>contract</code> 操作下该算法的复杂度是 $O(n^2)$。</p>
<h3 id="准确性的分析"><a href="#准确性的分析" class="headerlink" title="准确性的分析"></a>准确性的分析</h3><p>因为是一个随机算法，所以输出的切是一个随机变量。</p>
<p>可证如下定理：</p>
<div class="note info">
            <p>对于任意 $n$ 个顶点的 multigraph，Karger 算法的 $Pr[\text{a min-cut is returned}] \geq \dfrac{2}{n(n - 1)}$。</p>
          </div>
<p><del>第一次发现 note 的用法，感觉好好看</del></p>
<p>下面来证：</p>
<ul>
<li>记 $e_1, e_2, …, e_{n-2}$ 为随机选择的收缩的边的序列。</li>
<li>记 $G_1 = G$ 为初始图，$G_{i + 1} = contract(G_i, e_i), i = 1, 2,…, n-2$。</li>
</ul>
<p>显然可知：如果 $C$ 是 $G$ 的一个最小切且 $e \notin C$，则 $C$ 也是 <code>G&#39; = contract(G, e)​</code> 的一个最小割。所以有：</p>
<p>$\begin{aligned} p_C &amp;= Pr[C \text{ is returned}] \\ &amp;= Pr[e_i \notin C \text{ for all } i = 1, 2, …,n-2] \\ &amp;= \prod\limits_{i = 1}^{n-2} Pr[e_i \notin C | \forall j &lt; i, e_i \notin C] \end{aligned}$</p>
<p>$\forall j &lt; i, e_i \notin C$ 意味着 $C$ 也是 $G_i$ 的一个最小切。</p>
<p>观察到一个最小切的边数 $|C|$ 一定是小于等于任意点的度数（degree），否则可以直接把这个点切开。</p>
<p>所以更加小于等于度数的平均数，即 $|C| \leq \dfrac{2 |E|}{|V|}$。（所有点的度数和为 $2|E|$）</p>
<p>所以我们可以计算</p>
<p>$\begin{aligned} Pr[e_i \notin C | \forall j &lt; i, e_i \notin C]  &amp;= 1 - \dfrac{|C|}{|E_i|} \text{（从剩下的点中均匀随机选取）} \\ &amp; \geq 1 - \dfrac{2}{|V_i|} \\ &amp;= 1 - \dfrac{2}{n - i + 1} \end{aligned}$</p>
<p>$\begin{aligned} p_C &amp; \geq \prod\limits_{i = 1}^{n-2} (1 - \dfrac{2}{n - i + 1}) \\ &amp;= \prod\limits_{k = 3}^{n} \dfrac{k - 2}{k} \\ &amp;= \dfrac{2}{n(n-1)} \end{aligned}$</p>
<span class="label success">得证。</span>
<p>乍一看这个算法有啥吊用，准确率也太低了。</p>
<p>不过随机算法的精髓就在于有了准确性的下界，就可以通过 repeat 来提升准确性（好像是蒙特卡罗还是拉斯维加斯来着）。</p>
<p>Karger 算法主要就是将解空间的大小由指数级降低到了平方级。</p>
<p>我们可以运行算法 $t = \dfrac{n(n-1)\ln{n}}{2}$ 次并返回最小的解，该解是最小切的概率为：</p>
<p>$\begin{aligned} \ &amp;1 - Pr[\text{所有 } t \text{ 次都失败}] \\ = &amp;1 - (Pr[\text{一次失败}])^t \\ \geq &amp; 1 - \left( 1 - \dfrac{2}{n(n-1)} \right)^{\frac{n(n-1)\ln{n}}{2}} \\ \geq &amp; 1 - \dfrac{1}{n}  \end{aligned}$</p>
<p>运行一次算法的时间是 $O(n^2)$，也就是说我们可以在 $O(n^4\log{n})$ 的时间复杂度下高概率地（w.h.p）找到最小切。</p>
<h3 id="一个推论"><a href="#一个推论" class="headerlink" title="一个推论"></a>一个推论</h3><div class="note info">
            <p>对任意的 $n$ 阶图 $G(V, E)$，最小割的个数不会超过 $\dfrac{n(n-1)}{2}$。</p>
          </div>
<p>这是从概率方法上推到的：</p>
<p>设有 $M$ 个不同的最小切，显然它们是独立的。</p>
<p>$\because 1 \geq Pr[\text{a min-cut is returned}] \geq M \times \dfrac{2}{n(n-1)}$</p>
<p>$\therefore M \leq \dfrac{n(n-1)}{2}$</p>
<span class="label success">得证。</span>
<h2 id="快速最小切-Fast-Min-Cut"><a href="#快速最小切-Fast-Min-Cut" class="headerlink" title="快速最小切 Fast Min-Cut"></a>快速最小切 Fast Min-Cut</h2><p>我们知道 $p_C \geq \prod\limits_{i = 1}^{n-2} (1 - \dfrac{2}{n - i + 1})$，也就是这个概率是随 $i$ 增大递减的，也就是说当图越来越小也就越来越难成功。</p>
<p>我们就可以思考一个新的方法，先把图收缩的一个较小的程度，然后再递归地找这个更小的图上的最小切。</p>
<p>先定义一个随机收缩的函数，把图收缩到只剩 $t$ 个点：</p>
<figure class="highlight plain"><figcaption><span>random_contract(G, t)</span></figcaption><table><tr><td class="code"><pre><span class="line">Input: G(V, E), integer t &gt;&#x3D; 2</span><br><span class="line"></span><br><span class="line">while |V| &gt; t:</span><br><span class="line">	choose an edge uv in E uniformly at random</span><br><span class="line">	G &#x3D; contract(G, uv)</span><br><span class="line">return G</span><br></pre></td></tr></table></figure>
<p>然后定义我们的快速最小切：</p>
<figure class="highlight plain"><figcaption><span>fast_cut(G)</span></figcaption><table><tr><td class="code"><pre><span class="line">Input: G(V, E)</span><br><span class="line"></span><br><span class="line">if |V| &lt;&#x3D; 6:</span><br><span class="line">	return a mincut by brute force</span><br><span class="line">else:</span><br><span class="line">	t &#x3D; [1 + |V| &#x2F; sqrt(2)]</span><br><span class="line">	G1 &#x3D; random_contract(G, t)</span><br><span class="line">	G2 &#x3D; random_contract(G, t)</span><br><span class="line">	return min&#123;fast_cut(G1), fast_cut(G2)&#125;</span><br></pre></td></tr></table></figure>
<p>一样的我们可以算出 <code>random_contract</code> 成功的概率：</p>
<p>$\begin{aligned} &amp; Pr[C \text{ survives all contractions in random_contract}] \\ = &amp; \prod\limits_{i=1}^{n - t} Pr[C \text{ survives the i-th contraction } | C \text{ survives the first (i−1)-th contractions}] \\ \geq &amp; \prod\limits_{i=1}^{n - t} \left( 1 - \dfrac{2}{n - i +1} \right) \\  = &amp; \dfrac{t(t - 1)}{n(n - 1)}  \end{aligned}$</p>
<p>当 $t = \lceil 1 + n / \sqrt{2} \rceil $ 时，这个概率至少是 $1/2$。</p>
<p>定义两个事件：</p>
<ul>
<li>A：$C$ survives <code>random_contract</code> 中所有的 contractions。</li>
<li>B：<code>random_contract</code> 后最小切的大小保持不变。</li>
</ul>
<p>显然 $A \to B$，所以 $Pr[B] \geq Pr[A] \geq \frac{1}{2}$。</p>
<p>我们用 $p(n) $ 来表示一个 $n$ 阶图的 <code>fast_cut</code> 的下界：</p>
<p>$\begin{aligned} p(n) &amp; = Pr[\text{fast_cut returns a min-cut}] \\ &amp;= Pr[\text{a min-cut is returned by fast_cut(G1) and fast_cut(G2)}] \\ &amp; \geq 1 - (1 - Pr[B \wedge \text{ fast_cut(G1) return a min-cut in G1}])^2 \\ &amp; \geq 1 - (1 - Pr[A \wedge \text{ fast_cut(G1) return a min-cut in G1}])^2 \\ &amp; = 1 - (1 - Pr[A] Pr[\text{fast_cut(G1) return a min-cut in G1 | A}])^2 \\ &amp; \geq 1 - \left( 1 - \dfrac{1}{2} p(\lceil 1 + n / \sqrt{2} \rceil) \right)^2  \end{aligned}$</p>
<p>base case 是 $p(n) = 1$ 当 $n \leq 6$。</p>
<p>归纳易证 $p(n) = \Omega(\frac{1}{\log{n}})$。</p>
<p>时间复杂度上由那个算法我们有 $T(n) = 2T(\lceil 1 + n / \sqrt{2} \rceil) + O(n^2)$，易证 $T(n) = O(n^2 \log{n})$。</p>
<p>最终我们能总结如下定理：</p>
<div class="note info">
            <p>对任意的 $n$ 阶图，<code>fast_cut</code> 算法可以在 $O(n^2 \log{n})$ 的时间复杂度下以 $\Omega(\frac{1}{\log{n}})$ 的概率返回一个最小切。</p>
          </div>
<p><code>fast_cut</code> 比原始的 <code>random_contract</code> 时间长，概率高。</p>
<p>我们可以运行它 $(\log{n})^2$ 次，在 $O(n^2 \log^3{n})$ 的时间复杂度下以 $1 - O(1/n)$ 的概率返回最小切。</p>
<p>回忆前面的确定算法的时间复杂度是 $O(mn + n^2 \log{n})$，也就是说在稠密图上这个随机算法会表现地更好一些。</p>
<p><em>最终 Karger 继续改进了这个算法并获得了一个关于边数的接近线性时间的随机算法不过超出了本课程的讨论范围。</em></p>
<h1 id="最大割-Max-cut"><a href="#最大割-Max-cut" class="headerlink" title="最大割 Max-cut"></a>最大割 Max-cut</h1><p>问题就转变成了一个 <strong>NP-hard</strong> 的问题，如果我们相信 $P \neq NP$，就无法在多项式时间用确定性算法解决这个问题。</p>
<h2 id="贪心-Greedy"><a href="#贪心-Greedy" class="headerlink" title="贪心 Greedy"></a>贪心 Greedy</h2><figure class="highlight plain"><figcaption><span>greedy_cut</span></figcaption><table><tr><td class="code"><pre><span class="line">Input: G(V, E)</span><br><span class="line"></span><br><span class="line">S &#x3D; T &#x3D; emptyset</span><br><span class="line">for i &#x3D; 1, 2, ..., n:</span><br><span class="line">	v_i joins one of S,T to maximize the current |E(S,T)|</span><br></pre></td></tr></table></figure>
<p>然后讲了到<strong>近似比（approximation ratio）</strong>的概念，<del>复习问求。</del></p>
<p>暂时先不假证明（懒）地给出定理：</p>
<div class="note info">
            <p><code>greedy_cut</code> 是一个 0.5-近似的最大切算法。</p>
          </div>
<p>这并不是多项式时间最好的最大切算法，最好的算法 <a href="http://www-math.mit.edu/~goemans/PAPERS/maxcut-jacm.pdf">Goemans-Williamson Algorithm</a> 可以达到近似比 $\alpha^{\star} = \frac{2}{\pi}\inf_{x\in [-1,1]}{\frac{\arccos{(x)}}{1-x}}$，并被证明不存在近似比更好的多项式时间近似算法。</p>
<h2 id="通过条件期望去随机化-Derandomization"><a href="#通过条件期望去随机化-Derandomization" class="headerlink" title="通过条件期望去随机化 Derandomization"></a>通过条件期望去随机化 Derandomization</h2><p>对于任意顶点 $v \in V$，$X_{v} \in \{0, 1\}$ 是一个均匀随机 bit 来表示该点加入 $S$ 或 $T$。</p>
<p>这个随机的 cut 的大小可以被表示为：</p>
<script type="math/tex; mode=display">
|E(S, T)| = \sum\limits_{uv\in E} I[X_u \neq X_v]。</script><p>由于期望的线性（linearity），</p>
<script type="math/tex; mode=display">
\mathbb{E}[|E(S, T)|]=\sum\limits_{u v \in E} \mathbb{E}\left[I\left[X_{u} \neq X_{v}\right]\right]=\sum\limits_{u v \in E} \operatorname{Pr}\left[X_{u} \neq X_{v}\right]=\dfrac{|E|}{2}。</script><p>$|E|$ 就是最大切的 $OPT_G$ 的一个上界，所以我们有：</p>
<script type="math/tex; mode=display">
\mathbb{E}[|E(S,T)|] \geq \dfrac{OPT_G}{2}。</script><p>所以我们生成的随机的切的平均值大于等于 $\frac{OPT_G}{2}$，由于<strong>平均原理（averaging principle）</strong>，至少有一个二分生成的切是大于等于 $\frac{OPT_G}{2}$ 的。然后我们希望有算法来找到这个解。</p>
<p>我们想一个个地固定 $X_{v_{i}}$ 的值来构造一个二分 $\{\hat{S}, \hat{T}\}$ 使得：</p>
<script type="math/tex; mode=display">
|E(\hat{S}, \hat{T})| \geq \mathbb{E}[|E(S, T)|] \geq \dfrac{O P T_{G}}{2}。</script><p>我们从第一个点 $v_1$ 开始，通过全期望公式：</p>
<script type="math/tex; mode=display">
\mathbb{E}[E(S, T)]=\dfrac{1}{2} \mathbb{E}\left[E(S, T) \mid X_{v_{1}}=0\right]+\dfrac{1}{2} \mathbb{E}\left[E(S, T) \mid X_{v_{1}}=1\right]。</script><p>至少存在一个赋值 $x_1 \in \{0, 1\}$ 使得：</p>
<script type="math/tex; mode=display">
\mathbb{E}\left[E(S, T) \mid X_{v_{1}}=x_{1}\right] \geq \mathbb{E}[E(S, T)]。</script><p>我们可以继续运用这个方法：</p>
<script type="math/tex; mode=display">
\mathbb{E}\left[E(S, T) \mid X_{v_{1}}=x_{1}, \ldots, X_{v_{i-1}}=x_{i-1}\right]= \frac{1}{2} \mathbb{E}\left[E(S, T) \mid X_{v_{1}}=x_{1}, \ldots, X_{v_{i-1}}=x_{i-1}, X_{v_{i}}=0\right]
+\frac{1}{2} \mathbb{E}\left[E(S, T) \mid X_{v_{1}}=x_{1}, \ldots, X_{v_{i-1}}=x_{i-1}, X_{v_{i}}=1\right]</script><p>至少存在一个赋值 $x_i \in \{0, 1\}$ 使得：</p>
<script type="math/tex; mode=display">
\mathbb{E}\left[E(S, T) \mid X_{v_{1}}=x_{1}, \ldots, X_{v_{i}}=x_{i}\right] \geq \mathbb{E}\left[E(S, T) \mid X_{v_{1}}=x_{1}, \ldots, X_{v_{i-1}}=x_{i-1}\right]</script><p>所以我么就可以这要找到一个序列 $x_1, x_2, …, x_n \in \{0, 1\}$ 来形成一个单调的路径：</p>
<script type="math/tex; mode=display">
\mathbb{E}[E(S, T)] \leq \cdots \leq \mathbb{E}\left[E(S, T) \mid X_{v_{1}}=x_{1}, \ldots, X_{v_{i-1}}=x_{i-1}\right] \leq \mathbb{E}\left[E(S, T) \mid X_{v_{1}}=x_{1}, \ldots, X_{v_{i}}=x_{i}\right] \leq \cdots \leq \mathbb{E}\left[E(S, T) \mid X_{v_{1}}=x_{1}, \ldots, X_{v_{n}}=x_{n}\right]</script><p>最终一个二分 $(\hat{S}, \hat{T})$ 被这个序列确定下来，并且 $E(\hat{S}, \hat{T}) \geq \frac{OPT_G}{2}$。</p>
<p>我们可以有如下算法：</p>
<figure class="highlight plain"><figcaption><span>monotone_path</span></figcaption><table><tr><td class="code"><pre><span class="line">Input: G(V,E)</span><br><span class="line"></span><br><span class="line">S &#x3D; T &#x3D; emptyset</span><br><span class="line">for i &#x3D; 1, 2, ..., n:</span><br><span class="line">	v_i joins one of S,T to maximize the average size of cut conditioning on the choices made so far by the vertices (v_1, v_2, ..., v_i)</span><br></pre></td></tr></table></figure>
<p>所以，上面的贪心算法实际上是通过平均情况的去随机化得来的。</p>
<h2 id="通过-pairwise-独立的去随机化"><a href="#通过-pairwise-独立的去随机化" class="headerlink" title="通过 pairwise 独立的去随机化"></a>通过 pairwise 独立的去随机化</h2>]]></content>
      <categories>
        <category>课程</category>
        <category>Advanced Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>问题求解笔记-近似算法</title>
    <url>/2021/05/18/approximation-algorithm/</url>
    <content><![CDATA[<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><a id="more"></a>
<ul>
<li>$U = (\Sigma_I, \Sigma_O, L, L_I,\mathcal{M}, cost, goal)$是一个优化问题，$A$是相应的一个算法。对于任意的 $x \in L_1$，定义<strong>A对x的相对误差</strong> $\varepsilon_{\boldsymbol{A}}(\boldsymbol{x})$如下：</li>
</ul>
<script type="math/tex; mode=display">
\varepsilon_{\boldsymbol{A}}(\boldsymbol{x})=\frac{\left|cost(A(x))-Opt_{U}(x)\right|}{Op t_{U}(x)}</script><ul>
<li><p>定义算法 $A$的<strong>相对误差（relative error）</strong>$\varepsilon_{\boldsymbol{A}}(\boldsymbol{n})$如下：</p>
<script type="math/tex; mode=display">
\varepsilon_{A}(n)=\max \left\{\varepsilon_{A}(x) \mid x \in L_{I} \cap\left(\Sigma_{I}\right)^{n}\right\}</script></li>
<li><p>定义$A$对$x$的<strong>近似比率（approximation ratio）</strong>$\boldsymbol{R}_{\boldsymbol{A}}(\boldsymbol{x})$如下：</p>
<script type="math/tex; mode=display">
\boldsymbol{R}_{\boldsymbol{A}}(\boldsymbol{x})=\max \left\{\frac{cost(A(x))}{Opt_{U}(x)}, \frac{Opt_{U}(x)}{cost(A(x))}\right\}</script></li>
<li><p>定义算法<strong>$A$的近似比率$\boldsymbol{R}_{\boldsymbol{A}}(\boldsymbol{n})$</strong> 如下：</p>
<script type="math/tex; mode=display">
\boldsymbol{R}_{\boldsymbol{A}}(\boldsymbol{n})=\max \left\{R_{A}(x) \mid x \in L_{I} \cap\left(\Sigma_{I}\right)^{n}\right\}</script></li>
<li><p>对任意的实数 $\delta &gt; 1$，如果 $\forall x \in L_1,R_A(x) \leq \delta$，则称 $A $为 <strong>$\delta$-近似算法（approximation algorithm）</strong>。</p>
</li>
<li><p>对任意的实数 $\delta &gt; 1$，如果 $\forall n \in \mathbb{N},R_A(n) \leq f(n)$，则称 $A$为 <strong>$f(n)$-近似算法</strong>。</p>
</li>
<li><p>一个例子——生产调度问题</p>
<ul>
<li>GMS</li>
</ul>
</li>
</ul>
<ul>
<li><p>$U = (\Sigma_I, \Sigma_O, L, L_I,\mathcal{M}, cost, goal)$是一个优化问题，$A$是相应的一个算法。如果对任意的输入对$(x, \varepsilon) \in L_1 \times \mathbb{R}^+$，$A$能够在至多 $\varepsilon$的相对误差内计算出一个可行解 $A(x)$，并且 $Time_A(x, \varepsilon^{-1})$可以被 $|x|$的多项式函数约束，则 $A$称为 $U$的<strong>多项式时间近似近似方案（PTAS）</strong>。</p>
<ul>
<li>如果 $Time_A(x, \varepsilon^{-1})$可以同时被 $|x|$和 $\varepsilon^{-1}$的多项式函数约束，则称为<strong>完全多项式时间近似方案（FPTAS）</strong>。</li>
<li>PTAS：时间复杂度的渐进增长速度，界于输入规模。FPTAS：不仅界于输入规模，还界于精度的提升。</li>
<li><em>所以 FPTAS 应该是对 NP-hard 问题最优解了。</em></li>
</ul>
</li>
</ul>
<p>$Time_A(x, \varepsilon^{-1})$<em>中的-1次幂，是一种观察上的变化，当</em> $\varepsilon$<em>误差越小，</em> $\varepsilon^{-1}$<em>越大，我们在说时间渐进复杂度时，通常以观察“增长性”为常理，所以用了</em> $\varepsilon^{-1}$<em>取代</em> $\varepsilon$。</p>
<hr>
<h1 id="优化问题的分类"><a href="#优化问题的分类" class="headerlink" title="优化问题的分类"></a>优化问题的分类</h1><p>（建立在 $P \neq NP$）我们将 NPO 问题分为五类：</p>
<ul>
<li>NPO(I)：所有存在 FPTAS 的NPO优化问题。<ul>
<li>有最好的近似算法方案的NPO问题。人类能在其中起到的作用！我们可以在这一类问题中，采用时间换空间的策略，满足我们的需求，多计算，高精度！</li>
</ul>
</li>
<li>NPO(II)：所有存在 PTAS 的NPO优化问题。<ul>
<li>人类能在其中起到的作用！我们可以在这一类问题中，采用空间换时间的策略，满足我们的需求，低精度，高速度。</li>
</ul>
</li>
<li>NPO(III)：所有的优化问题 $U$满足<ul>
<li>存在 $\delta &gt; 1$的多项式时间 $\delta$-近似算法。<em>当我们放低精度（误差可以大到某个程度）到某个幅度时，确定有近似解。</em></li>
<li>没有 $d &lt; \delta$的多项式时间 $d-$近似算法，即没有 PTAS。<em>在这个误差率以下，没有多项式</em> $d$<em>近似算法。</em></li>
</ul>
</li>
<li>NPO(IV)：所有的优化问题 $U$满足<ul>
<li>存在多项式界的函数 $f : \mathbb{N} \to \mathbb{R}^+$，对 $U$有多项式时间 $f(n)$-近似算法。</li>
<li>不存在 $\delta \in \mathbb{R}^+$的多项式时间 $\delta$-近似算法。</li>
<li><em>只能用多项式函数进行近似解的误差界、限分析，不存在绝对的</em> $\delta$<em>近似。在P不等于NP的前提下，只有</em> $f(n)$<em>近似可以等价于不存在</em> $\delta$<em>近似</em>。</li>
</ul>
</li>
<li>NPO(V)：所有的优化问题满足，如果存在一个多项式时间 $f(n)$-近似算法，则 $f(n)$没有 polylogarithmic函数界。<ul>
<li>存在一个多项式时间近似算法，但是其误差的增长速度“不慢”（不被多项式界）。</li>
</ul>
</li>
</ul>
<hr>
<h1 id="近似的稳定性"><a href="#近似的稳定性" class="headerlink" title="近似的稳定性"></a>近似的稳定性</h1><p>稳定性用来衡量<em>衡量算法在同一个问题的不同实例上的表现，精度和时间开销的关联表现。</em></p>
<ul>
<li>$U = (\Sigma_I, \Sigma_O, L, L_I,\mathcal{M}, cost, goal)$和 $\overline{U} = (\Sigma_I, \Sigma_O, L, L,\mathcal{M}, cost, goal)$是两个优化问题，$L_1 \subset L$。$\overline{U}$关于 $L_1$的<strong>距离函数（distance function）</strong> 是任意一个函数 $h_L : L \to \mathbb{R}^{\geq 0}$满足<ul>
<li>$\forall x \in L_I, h_L(x) = 0$</li>
<li>$h$是多项式可计算的</li>
<li>将 $U$问题（一个可能是PTAS问题）上的近似算法，扩展到一个更大但“距离不远”的 $\overline{U}$问题上。</li>
</ul>
</li>
<li>$\textbf{Ball}_{r,h}(L_I) = \{ w \in L | h(w) \leq r\}$</li>
<li>$A$是 $\overline{U}$的一个算法，且 $A$是 $U$的一个 $\varepsilon$-近似算法，对某个 $\varepsilon \in \mathbb{R}^{&gt;1}$。$p$是一个正实数，若对任意实数 $0 &lt; r \leq p$，存在 $\delta_{r, \varepsilon} \in \mathbb{R}^{&gt;1}$，使得 $A$是 $U_r = \{\Sigma_I, \Sigma_O, L, Ball_{r,h}(L_I),\mathcal{M}, cost, goal\}$一个 $\delta_{r, \varepsilon}$-近似算法，则 $A$称为对于$h$是 <strong>$p$-稳定（$p$-stable）</strong>。</li>
<li>$A$是<strong>稳定的（stable）</strong>，如果对任意 $p \in \mathbb{R}^+$，$A$是 $p$-稳定的。</li>
</ul>
<ul>
<li>$A = \{ A_\varepsilon\}_{\varepsilon &gt; 0}$是 $U$的一个 PTAS，如果对任意的 $r &gt; 0$，任意 $\varepsilon &gt; 0$，$A_\varepsilon $是 $U_r$的一个 $\delta_{r, \varepsilon}$-近似算法，则称PTAS $A$关于 $h$是<strong>稳定的</strong>。</li>
<li>PTAS $A$是<strong>超稳定（superstable）</strong>的，如果 $\delta_{r, \varepsilon} \leq f(\varepsilon) \cdot g(r)$，其中<ul>
<li>$f,g$的定义域分别是 $\mathbb{R^{\geq 0}}$和 $\mathbb{R}^{+}$，</li>
<li>$\lim\limits_{\varepsilon \to 0}f(\varepsilon) = 0$。</li>
</ul>
</li>
<li><em>观察到，若 $A$是优化问题 $U$的超稳定PTSA，则它也是任意 $U_r$的PTSA。</em></li>
</ul>
<hr>
<h1 id="对偶近似算法（Dual-Approximation-Algorithm）"><a href="#对偶近似算法（Dual-Approximation-Algorithm）" class="headerlink" title="对偶近似算法（Dual Approximation Algorithm）"></a>对偶近似算法（Dual Approximation Algorithm）</h1><ul>
<li>$U = (\Sigma_I, \Sigma_O, L, L_I,\mathcal{M}, cost, goal)$是一个优化问题。$U$的一个<strong>限制距离函数（constraint distance function）</strong> 是一个函数 $h : L_I \times \Sigma_O^\star \to \mathbb{R}^{\geq 0}$满足<ul>
<li>$\forall S \in \mathcal{M}(x), h(x, S) = 0$，</li>
<li>$\forall S \in \mathcal{M}(x), h(x, S) &gt; 0$，</li>
<li>$h$是多项式时间可计算的。</li>
</ul>
</li>
<li>对任意的 $\varepsilon \in \mathbb{R}^+$，任意 $x \in L_I$，$\mathcal{M}_{\varepsilon}^h(x) = \{S \in \Sigma_O^\star\ | h(x, S) \leq \varepsilon\}$是 $\mathcal{M(x)}$对 $h$的 $\varepsilon\textbf{-ball}$。</li>
<li>$U$的一个优化算法 $A$称为 <strong>$h$-dual</strong> $\varepsilon$-近似算法，如果对任意的 $x \in L_I$，<ul>
<li>$A(x) \in \mathcal{M}_\varepsilon^{h}(x)$，</li>
<li>$cost(A(x)) \geq Opt_U(x)$当 $goal = maximum$，$cost(A(x)) \leq Opt_U(x) $当 $goal = minimum$。</li>
</ul>
</li>
<li>一个算法 $A$称为 $U$的 <strong>$h$-dual 多项式时间近似方案（h-dual PTAS）</strong>，如果<ul>
<li>$\forall (x, \varepsilon) \in L_I \times \mathbb{R}^+, A(x, \varepsilon) \in \mathcal{M}_{\varepsilon}^h(x)$，</li>
<li>$cost(A(x)) \geq Opt_U(x)$当 $goal = maximum$，$cost(A(x)) \leq Opt_U(x) $当 $goal = minimum$，</li>
<li>$Time_A(x, \varepsilon^{-1})$被一个 $|x|$的多项式函数界定。</li>
</ul>
</li>
<li>同上，同时被 $|x|,\varepsilon^{-1}$的多项式函数界定，叫 $h$-dual FPTAS。 </li>
</ul>
]]></content>
      <categories>
        <category>课程</category>
        <category>problem-solving</category>
      </categories>
  </entry>
  <entry>
    <title>Python常用内建模块</title>
    <url>/2021/02/25/build-in-module/</url>
    <content><![CDATA[<p>参考: <a href="https://www.liaoxuefeng.com/wiki/1016959663602400">廖雪峰python教程</a><br>学习完了python基本语法，更重要的是对python各种库的熟练运用，本章主要是对python内建库的讲解。<br><a id="more"></a></p>
<h1 id="datetime"><a href="#datetime" class="headerlink" title="datetime"></a>datetime</h1><p>datetime是Python处理日期和时间的标准库。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>now = datetime.now() <span class="comment"># 获取当前datetime</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(now)</span><br><span class="line"><span class="number">2021</span>-02-<span class="number">25</span> <span class="number">14</span>:<span class="number">18</span>:<span class="number">54.205754</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="built_in">type</span>(now))</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">datetime</span>.<span class="title">datetime</span>&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>注意到<code>datetime</code>是模块，<code>datetime</code>模块还包含一个<code>datetime</code>类，通过<code>from datetime import datetime</code>导入的才是<code>datetime</code>这个类。</li>
<li>如果仅导入<code>import datetime</code>，则必须引用全名<code>datetime.datetime</code>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dt = datetime(<span class="number">2015</span>, <span class="number">4</span>, <span class="number">19</span>, <span class="number">12</span>, <span class="number">20</span>) <span class="comment"># 用指定日期时间创建datetime</span></span><br></pre></td></tr></table></figure>
<h2 id="datatime转timestamp"><a href="#datatime转timestamp" class="headerlink" title="datatime转timestamp"></a>datatime转timestamp</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dt.timestamp()</span><br></pre></td></tr></table></figure>
<ul>
<li>注意Python的timestamp是一个浮点数，整数位表示秒。</li>
<li>timestamp的值与时区毫无关系，因为timestamp一旦确定，其UTC时间就确定了，转换到任意时区的时间也是完全确定的，这就是为什么计算机存储的当前时间是以timestamp表示的，因为全球各地的计算机在任意时刻的timestamp都是完全相同的（假定时间已校准）。</li>
</ul>
<h2 id="timestamp转datetime"><a href="#timestamp转datetime" class="headerlink" title="timestamp转datetime"></a>timestamp转datetime</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = <span class="number">1429417200.0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(datetime.fromtimestamp(t)) <span class="comment"># 本地时间（当前操作系统的时区）</span></span><br><span class="line"><span class="number">2015</span>-04-<span class="number">19</span> <span class="number">12</span>:<span class="number">20</span>:<span class="number">00</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(datetime.utcfromtimestamp(t)) <span class="comment"># UTC时间</span></span><br><span class="line"><span class="number">2015</span>-04-<span class="number">19</span> 04:<span class="number">20</span>:<span class="number">00</span></span><br></pre></td></tr></table></figure>
<h2 id="str转datetime"><a href="#str转datetime" class="headerlink" title="str转datetime"></a>str转datetime</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>cday = datetime.strptime(<span class="string">&#x27;2015-6-1 18:19:59&#x27;</span>, <span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(cday)</span><br><span class="line"><span class="number">2015</span>-06-01 <span class="number">18</span>:<span class="number">19</span>:<span class="number">59</span></span><br></pre></td></tr></table></figure>
<ul>
<li>字符串<code>%Y-%m-%d %H:%M:%S</code>规定了日期和时间部分的格式。详细的说明请参考<a href="https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior">Python文档</a>。</li>
<li>转换后的datetime是没有时区信息的。</li>
</ul>
<h2 id="datetime转str"><a href="#datetime转str" class="headerlink" title="datetime转str"></a>datetime转str</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>now = datetime.now()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(now.strftime(<span class="string">&#x27;%a, %b %d %H:%M&#x27;</span>))</span><br><span class="line">Thu, Feb <span class="number">25</span> <span class="number">14</span>:<span class="number">18</span></span><br></pre></td></tr></table></figure>
<h2 id="datetime加减"><a href="#datetime加减" class="headerlink" title="datetime加减"></a>datetime加减</h2><p>对日期和时间进行加减实际上就是把datetime往后或往前计算，得到新的datetime。加减可以直接用+和-运算符，不过需要导入timedelta这个类。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime, timedelta</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>now = datetime.now()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>now + timedelta(days=<span class="number">2</span>, hours=<span class="number">12</span>)</span><br><span class="line">datetime.datetime(<span class="number">2021</span>, <span class="number">2</span>, <span class="number">28</span>, <span class="number">2</span>, <span class="number">18</span>, <span class="number">54</span>, <span class="number">205754</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="时区转换"><a href="#时区转换" class="headerlink" title="时区转换"></a>时区转换</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 拿到UTC时间，并强制设置时区为UTC+0:00:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>utc_dt = datetime.utcnow().replace(tzinfo=timezone.utc)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(utc_dt)</span><br><span class="line"><span class="number">2015</span>-05-<span class="number">18</span> 09:05:<span class="number">12.377316</span>+<span class="number">00</span>:<span class="number">00</span></span><br><span class="line"><span class="comment"># astimezone()将转换时区为北京时间:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bj_dt = utc_dt.astimezone(timezone(timedelta(hours=<span class="number">8</span>)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(bj_dt)</span><br><span class="line"><span class="number">2015</span>-05-<span class="number">18</span> <span class="number">17</span>:05:<span class="number">12.377316</span>+08:<span class="number">00</span></span><br><span class="line"><span class="comment"># astimezone()将转换时区为东京时间:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokyo_dt = utc_dt.astimezone(timezone(timedelta(hours=<span class="number">9</span>)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(tokyo_dt)</span><br><span class="line"><span class="number">2015</span>-05-<span class="number">18</span> <span class="number">18</span>:05:<span class="number">12.377316</span>+09:<span class="number">00</span></span><br><span class="line"><span class="comment"># astimezone()将bj_dt转换时区为东京时间:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokyo_dt2 = bj_dt.astimezone(timezone(timedelta(hours=<span class="number">9</span>)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(tokyo_dt2)</span><br><span class="line"><span class="number">2015</span>-05-<span class="number">18</span> <span class="number">18</span>:05:<span class="number">12.377316</span>+09:<span class="number">00</span></span><br></pre></td></tr></table></figure>
<h1 id="collections"><a href="#collections" class="headerlink" title="collections"></a>collections</h1><p>collections是Python内建的一个集合模块，提供了许多有用的集合类。</p>
<h2 id="namedtuple"><a href="#namedtuple" class="headerlink" title="namedtuple"></a>namedtuple</h2><p><code>namedtuple</code>是一个函数，它用来创建一个自定义的tuple对象，并且规定了tuple元素的个数，并可以用属性而不是索引来引用tuple的某个元素。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Point = namedtuple(<span class="string">&#x27;Point&#x27;</span>, [<span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p = Point(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>p.x</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<h2 id="deque"><a href="#deque" class="headerlink" title="deque"></a>deque</h2><p>deque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈。</p>
<ul>
<li>支持<code>appendleft()</code>, <code>popleft()</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>q = deque([<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>q.append(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>q.appendleft(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>q</span><br><span class="line">deque([<span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;x&#x27;</span>])</span><br></pre></td></tr></table></figure>
<h2 id="defaultdict"><a href="#defaultdict" class="headerlink" title="defaultdict"></a>defaultdict</h2><p>使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict.</p>
<ul>
<li>默认值是调用函数返回的，而函数在创建defaultdict对象时传入。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dd = defaultdict(<span class="keyword">lambda</span>: <span class="string">&#x27;N/A&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dd[<span class="string">&#x27;key1&#x27;</span>] = <span class="string">&#x27;abc&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dd[<span class="string">&#x27;key1&#x27;</span>] <span class="comment"># key1存在</span></span><br><span class="line"><span class="string">&#x27;abc&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dd[<span class="string">&#x27;key2&#x27;</span>] <span class="comment"># key2不存在，返回默认值</span></span><br><span class="line"><span class="string">&#x27;N/A&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="OrderedDict"><a href="#OrderedDict" class="headerlink" title="OrderedDict"></a>OrderedDict</h2><p>使用dict时，Key是无序的。如果要保持Key的顺序，可以用<code>OrderedDict</code>.</p>
<ul>
<li>按照插入的顺序排列，不是Key本身排序。</li>
</ul>
<h2 id="ChainMap"><a href="#ChainMap" class="headerlink" title="ChainMap"></a>ChainMap</h2><p><code>ChainMap</code>可以把一组dict串起来并组成一个逻辑上的dict.</p>
<h2 id="Counter"><a href="#Counter" class="headerlink" title="Counter"></a>Counter</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = Counter()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> ch <span class="keyword">in</span> <span class="string">&#x27;programming&#x27;</span>:</span><br><span class="line"><span class="meta">... </span>    c[ch] = c[ch] + <span class="number">1</span></span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c</span><br><span class="line">Counter(&#123;<span class="string">&#x27;g&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;r&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;a&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;i&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;o&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;n&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;p&#x27;</span>: <span class="number">1</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.update(<span class="string">&#x27;hello&#x27;</span>) <span class="comment"># 也可以一次性update</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c</span><br><span class="line">Counter(&#123;<span class="string">&#x27;r&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;o&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;g&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;l&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;p&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;i&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;n&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;h&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;e&#x27;</span>: <span class="number">1</span>&#125;)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>language</tag>
      </tags>
  </entry>
  <entry>
    <title>问题求解笔记-代数编码</title>
    <url>/2021/04/25/codeing/</url>
    <content><![CDATA[<h1 id="检错和纠错码"><a href="#检错和纠错码" class="headerlink" title="检错和纠错码"></a>检错和纠错码</h1><h2 id="最大似然编码（maximum-likelihood-decoding​）"><a href="#最大似然编码（maximum-likelihood-decoding​）" class="headerlink" title="最大似然编码（maximum-likelihood decoding​）"></a>最大似然编码（maximum-likelihood decoding​）</h2><p>所有的纠错建立在这个基础上，按最大概率出错位纠错。</p>
<p><strong>二进制对称信道</strong>（binary symmetric channel）</p>
<a id="more"></a>
<h2 id="分块码-（Block-Codes​）"><a href="#分块码-（Block-Codes​）" class="headerlink" title="分块码 （Block Codes​）"></a>分块码 （Block Codes​）</h2><p>($n,m$)-<strong>分块码</strong>：被编码的信息可以被分成多块 $m$ 位二进制数，每块会被编码成 $n$ 位二进制数。</p>
<ul>
<li>编码函数$E:\mathbb{Z}_2^m \rightarrow \mathbb{Z}_2^n$</li>
<li><p>解码函数 $D: \mathbb{Z}_2^n \rightarrow \mathbb{Z}_2^m$</p>
</li>
<li><p>编码字(codeword)：$E$ 的像中的一个元素</p>
</li>
</ul>
<p><strong>汉明距离</strong>（Hamming distance）：$d(x,y)$，$x$ 和 $y$ 不相同的位数。</p>
<p><strong>最小距离</strong>：$d_{min}$，所有不同有效编码字 $x,y$ 的距离最小的。</p>
<p><strong>权重</strong>：$w(x)$，$x$ 中 $1$ 的位数。</p>
<p>一些性质：</p>
<ul>
<li>$w(x) = d(x, 0)$</li>
<li>$d(x,y) \geq 0$</li>
<li>$d(x, y ) = 0 \Leftrightarrow x = y$</li>
<li>$d(x, y) = d(y,x)$</li>
<li>$d(x,y) \leq d(x,z) + d(z,y)$</li>
</ul>
<p><strong>定理 8.13.</strong> $C$ 是 $d_{min} = 2n+1$ 的编码，则 $C$ 可以纠 $n$ 位错，可以检测 $2n$ 位错。</p>
<p><em>所以码字空间分布越均匀，最小码字距离越大，查纠错能力越强。</em></p>
<hr>
<h1 id="线性码（Linear-Codes）"><a href="#线性码（Linear-Codes）" class="headerlink" title="线性码（Linear Codes）"></a>线性码（Linear Codes）</h1><p><strong>群码</strong>（group code）：$\mathbb{Z}_2^n$ 的子群的编码。</p>
<p><strong>引理 8.17.</strong> $x,y$ 为 $n$ 位码字，则 $w(x  +y) = d(x,y)$。</p>
<p><strong>定理 8.18.</strong> 群码 $C$ 的 $d_{min}$ 是所有非零码字的最小的权重。</p>
<p><em>码字的运算，还是码字，群封闭性。</em></p>
<h2 id="线性码"><a href="#线性码" class="headerlink" title="线性码"></a>线性码</h2><p><strong>内积</strong>：</p>
<p> $\begin{aligned} \textbf{x}\cdot \textbf{y}  &amp;= \textbf{x}^t \textbf{y} \\ &amp;= \begin{pmatrix} x_1 &amp; x_2 &amp;\cdots &amp;x_n \end{pmatrix} \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n  \end{pmatrix} \\ &amp;= x_1y_1 + x_2y_2 + \cdots + x_ny_n \end{aligned}$</p>
<p>记 $\mathbb{M}_{m\times n}(\mathbb{Z}_2)$ 为所有 $m$ 行 $n$ 列且元素属于 $\mathbb{Z}_2$ 的矩阵的集合。</p>
<p>$H \in \mathbb{M}_{m\times n}(\mathbb{Z}_2)$  的<strong>空域</strong>（null space我自己瞎起的名字）：$\{\textbf{x} \in \mathbb{Z}_2^n : H \textbf{x} = 0 \}$；记为 Null$(H)$。</p>
<p><strong>定理 8.21.</strong> $H \in \mathbb{M}_{m\times n}(\mathbb{Z}_2)$ 的 $null ~space$ 一定是群码。</p>
<p><strong>线性码</strong>：通过某个 $H \in \mathbb{M}_{m\times n}(\mathbb{Z}_2)$ 的 $null ~space$ 生成的编码。</p>
<p><em>对于 $n$ 为编码，我们需要找到一个好的矩阵 $H$，使得我们的编码空间（$H$ 的 null space）的 $d_{min}$ 尽可能大。</em></p>
<hr>
<h1 id="奇偶校验和生成矩阵（Parity-Check-and-Generator-Matrices）"><a href="#奇偶校验和生成矩阵（Parity-Check-and-Generator-Matrices）" class="headerlink" title="奇偶校验和生成矩阵（Parity-Check and Generator Matrices）"></a>奇偶校验和生成矩阵（Parity-Check and Generator Matrices）</h1><p><strong>规范奇偶校验矩阵</strong>（canonical parity-check matrix）：$H \in \mathbb{M}_{m\times n}(\mathbb{Z}_2)$ 且 $n &gt; m$，且最后 $m$ 列形成的 $m \times m$ 子矩阵是单位阵 $I_m$，即 $H = \left( A | I_m \right)$。</p>
<p>每个规范奇偶校验阵 $H$ 都有一个 $n \times (n - m)$ 阶标准<strong>生成阵</strong>（standard generator matrix）: $G = \left( \dfrac{I_{n-m}}{A} \right)$。</p>
<p><strong>定理 8.25.</strong> $H$ 是一个规范奇偶校验阵，则 Null$(H)$ 包含了满足 前 $n-m$ 位随便取，但后 $m$ 位由 $H\textbf{x} = \textbf{0}$ 决定的 所有向量 $\textbf{x} \in \mathbb{Z}_2^n$。后 $m$ 位每个作为某个前 $n-m$ 位的偶校验位。所以，$H$ 可以给出一个 $(n, n-m)$ -分块码。</p>
<ul>
<li>前 $n-m$ 位叫<strong>信息位</strong>（information bit），后 $m$ 位叫校验位</li>
</ul>
<p><strong>定理 8.26.</strong>  $G$ 是一个 $n \times k$ 阶标准生成阵，则 $C = \{ \textbf{y} : G\textbf{x} = \textbf{y}, x \in \mathbb{Z}_2^n \}$ 是一个 $(n,k)$-分块码。</p>
<p><strong>引理 8.27.</strong> $H = (A | I_m)$ 是一个 $m \times n$ 规范奇偶校验阵, $G = \left( \dfrac{I_{n - m}}{A} \right)$ 是对应的 $n \times (n - m)$ 阶标准生成阵，则 $HG = 0$。</p>
<p><strong>定理8.28.</strong>  $H = (A | I_m)$ 是一个 $m \times n$ 规范奇偶校验阵, $G = \left( \dfrac{I_{n - m}}{A} \right)$ 是对应的 $n \times (n - m)$ 阶标准生成阵，$C$ 是由 $G$ 生成的编码。则 $\textbf{y} \in C \Leftrightarrow H\textbf{y} = 0$，也就是说，$C$ 是有规范奇偶校验矩阵的线性码。</p>
<p><em>偶校验矩阵的来处：在H的右侧 $m \star m$设置单位矩阵，就是要用相应位对A中相应行上的非0位进行偶校验.。</em></p>
<p><em>生成矩阵的来处：当给定任意的A，可以使用相应的偶校验矩阵来通过解线性方程的方式得到群码，但是不如构造G矩阵，对待传输数据直接进行计算得到相应的群码和编码对应。效率更高。</em></p>
<p><em>可证码字能够覆盖所有待编码位串。后m位可以完成对待编码位串中非0位的偶校验！</em></p>
<p><em>如何完成偶校验，取决于A矩阵和待编码信息。</em></p>
<p><strong>定理 8.31.</strong> $H$ 为一个 $m \times n$ 矩阵，$H$ 的空域是一个单检错码 当且仅当 $H$ 没有列是全零。（$e_i$ 不在空域中，$d_{min} &gt; 1$）</p>
<p><strong>定理 8.34.</strong> $H$ 为一个 $m \times n$ 矩阵，$H$ 的空域是一个单纠错码 当且仅当 $H$ 没有列是全零并且没有两列是相同的。（$e_i + e_j$ 不在空域中，$d_{min} \geq 3$）</p>
<p>所以一个 $m \times n$ 的规范奇偶校验阵，要检一位错、纠一位错，除了 $\textbf{0}, \textbf{e_i}$，剩余 $2^m - (1 + m)$ 个为信息列。</p>
<hr>
<h1 id="高效解码"><a href="#高效解码" class="headerlink" title="高效解码"></a>高效解码</h1><p><strong>x</strong>的<strong>像</strong>（syndrome）：$H\textbf{x}$</p>
<p><strong>命题 8.36.</strong> 矩阵 $H$ 决定了一套线性码，$\textbf{x} = \textbf{c} + \textbf{e} $ 为接收到的$n$-位串，$\textbf{c}$ 为正确码字，$\textbf{e}$ 为错误，则 $H\textbf{x} = H\textbf{e}$。</p>
<ul>
<li>这个命题说明接受到的信息由错误决定而不是由正确码字决定。</li>
</ul>
<p><strong>定理 8.37.</strong> $H \in \mathbb{M}_{m\times n}(\mathbb{Z}_2)$ 并且假设 $H$ 的编码是一个单检错码，$\textbf{r}$ 是接收到的 $n$-位码，如果 $H\textbf{r} = \textbf{0}$，则没有错；否则，如果 $H\textbf{r} $ 等于 $H$ 的第 $i$ 列，则第 $i$ 位出错。</p>
<h2 id="陪集解码（Coset-Decoding）"><a href="#陪集解码（Coset-Decoding）" class="headerlink" title="陪集解码（Coset Decoding）"></a>陪集解码（Coset Decoding）</h2><p>把所有陪集表示（$\textbf{e} +     C$）列表列出来，查表解码纠错。</p>
<p>按最大似然，总是取权最小的出错 $\textbf{e}$ ，叫做 <strong>coset leader</strong>。</p>
]]></content>
      <categories>
        <category>课程</category>
        <category>problem-solving</category>
      </categories>
  </entry>
  <entry>
    <title>问题求解笔记-密码算法</title>
    <url>/2021/04/18/cryptography/</url>
    <content><![CDATA[<h1 id="公钥加密系统"><a href="#公钥加密系统" class="headerlink" title="公钥加密系统"></a>公钥加密系统</h1><p>一个公钥加密系统中，每个参与者都拥有一把<strong>公钥</strong>$P$和<strong>私钥</strong>$S$，密钥需要保密，公钥可以公之于众。</p>
<p>$\mathcal{D}$表示允许信息的集合，要求公钥与密钥指定一种从$\mathcal{D}$到自身的一一对应的函数。<br>公钥函数$P()$和密钥函数$S()$都是$\mathcal{D}$的排列，互为反函数，即对任意的 $M \in \mathcal{D}$，有</p>
<script type="math/tex; mode=display">M = S(P(M)) \text{ 且 } M = P(S(M))$$。

我们期望的就是除了密钥持有者以外，没有人能够高效地计算出 $S()$。

加密方法是加密的第一个“秘密”。但是加密方法不代表加密函数。

加密方法，再加上秘钥，才能唯一确定加密函数。

## 发送密文

- Bob取得Alice的公钥$P_A$。
- Bob计算出相应与$M$的密文$C=P_A(M)$，并把$C$发送给Alice。
- 当Alice收到密文$C$后，运用自己的密钥$S_A$恢复原始信息$M$。



## 数字签名

- Alice运用密钥$S_A$和等式$\sigma=S_A)M’$计算出信息$M’$的数字签名$\sigma$。
- Alice把消息/签名对$(M’,\sigma)$发给Bob。
- Bob收到$(M’,\sigma)$时，通过验证等式$M’=P_A(\sigma)$来证实消息的确是来自Alice。

任何人都可以把数字签名翻译出来，但只有密钥持有者可以生成数字签名。

用自己独有的“特性”：字迹、指纹、私章等，对自己发出的内容，如信函等，进行加工并送给接收方；接收方从中能够从加工信息中识别出发送方的独有特征并认定；

双发产生纠纷时，发送方提供自有特征证据、接收方提供解析出的“特征证据”，进行比对，否认或者不得不承认发送信息确实出自自身；

发送方的独有特征：私钥；

与加密的区别：提供签名方的不可抵赖 +  验证签名方的不可伪造。



# RSA加密系统

获得公私钥的过程：

1. 随机选择两个大素数 $p, q, p \neq q$。
2. 计算 $n = pq$。
3. 选取一个与 $\phi(n) = (p - 1)(q - 1)$ 互质的小奇数 $e$。
4. 对模 $\phi(n)$，计算 $e$ 的乘法逆元 $d$。（$e, \phi(n)$ 互质保证逆元存在且唯一，用exgcd）
5. $P = (e, n)$作**RSA公钥**。
6. $S = (e, n)$作**RSA私钥**。

原理在于：如果知道了 $n$ 不知道 $p,q$，由于大数的素数分解的复杂性很难获得 $\phi(n)$，所以即使知道 $e$ 也很难得到 $d$。

加密 $M \in \mathbb{Z}_n$：

$$P(M) = M^{e} \text{  mod } n</script><p>解密 $C \in \mathbb{Z}_n$：</p>
<script type="math/tex; mode=display">S(C) = C^d \text{  mod } n</script><p>（可证 $M^{ed} \equiv M \quad (\text{mod } n )$）</p>
]]></content>
      <categories>
        <category>课程</category>
        <category>problem-solving</category>
      </categories>
  </entry>
  <entry>
    <title>cuda 安装教程</title>
    <url>/2021/07/19/cuda/</url>
    <content><![CDATA[<p>一直是个电脑配置盲，显卡啥的都不太懂，<del>大概学了个假cs</del>。正在学习ML，DL，要用到pytorch, tenserflow啥的，需要用到 Nvidia cuda，正好安装一下，也捋一捋配置。</p>
<hr>
<h1 id="我的电脑配置"><a href="#我的电脑配置" class="headerlink" title="我的电脑配置"></a>我的电脑配置</h1><ul>
<li>电脑型号：HUAWEI xpro</li>
<li>系统：Ubuntu 20.04.2 LTS <ul>
<li>主要工作系统是 Linux，所以本篇基本是 Ubuntu 上的 Cuda 安装教程。</li>
</ul>
</li>
<li>处理器：Intel® Core™ i5-8265U CPU @ 1.60GHz × 8 </li>
</ul>
<a id="more"></a>
<h2 id="显卡"><a href="#显卡" class="headerlink" title="显卡"></a>显卡</h2><p>最主要就是看我们的显卡了，首先扫扫盲看看显卡是个什么东西。</p>
<p>参考<a href="https://www.zhihu.com/question/28422454">一篇知乎回答</a>。</p>
<ul>
<li><strong>显卡（Vedio Card , Graphics Card）</strong>，也叫显示适配器，主机里的数据要显示在屏幕上就需要显卡。因此，显卡是电脑进行数模信号转换的设备，承担输出显示图形的任务。具体来说，<strong>显卡接在电脑主板上，它将电脑的数字信号转换成模拟信号让显示器显示出来</strong>。<ul>
<li>集成显卡，是指显卡集成在主板上，不能随意更换。而独立显卡是作为一个独立的器件插在主板的AGP接口上的，可以随时更换升级。集成显卡使用物理内存，而独立显卡有自己的显存。</li>
</ul>
</li>
<li><strong>VGA(Video Graphics Array)</strong>，是视频图形阵列，就是显示输出接口，能够输出一个分辨率下的色彩rgb数据。</li>
<li><strong>GPU(Graphics Processing Unit)</strong>，这个就是显卡上的一块芯片，就像 CPU 是主板上的一块芯片。关于GPU的作用。。。这个其实ics和os里都讲过(，指路<a href="http://jyywiki.cn/OS/2021/slides/13.slides#/4">jyy的slides</a>。就是一个用于画图的硬件，进行大量的显存操作的像素计算（矩阵运算），能够加速图形渲染显示。当人们发现它的强大算力之后就有了。。。</li>
<li><strong>CUDA(Compute Unified Device Architecture)</strong>，通用并行计算架构，是一种运算平台。它包含CUDA指令集架构以及GPU内部的并行计算引擎。你只要使用一种类似于C语言的<strong>CUDA C语言</strong>，就可以开发CUDA程序，从而可以更加方便的利用GPU强大的计算能力，而不是像以前那样先将计算任务包装成图形渲染任务，再交由GPU处理。（炼丹）</li>
<li><strong>cuDNN</strong>，NVIDIA cuDNN是用于深度神经网络的GPU加速库。它强调性能、易用性和低内存开销。NVIDIA cuDNN可以集成到更高级别的机器学习框架中，如谷歌的Tensorflow。</li>
</ul>
<p>大概就这些了，理清楚显卡是干啥的，看一下我的显卡。</p>
<p><code>$ neofetch</code> 一下：</p>
<p><img data-src="fetch.png" alt="neofetch"></p>
<p>看到了两个 GPU:</p>
<ul>
<li>NVIDIA GeForce MX250</li>
<li>Intel UHD Graphics 620</li>
<li>等等为什么有两个？！<span class="github-emoji" alias="no_mouth" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f636.png?v8">&#x1f636;</span></li>
</ul>
<p>上网搜了一下，是可以有双显卡的，一个集显一个独显。不做调整的话默认开机使用用的是核显，使用到一定量，会自动切换到独立显卡。不过也有些因为驱动问题，导致一直默认核显。（啊我好像没装驱动！<span class="github-emoji" alias="joy" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f602.png?v8">&#x1f602;</span>）</p>
<h3 id="Intel-UHD-Graphics-620"><a href="#Intel-UHD-Graphics-620" class="headerlink" title="Intel UHD Graphics 620"></a>Intel UHD Graphics 620</h3><p><code>$ lspci | grep -i UHD</code> 看一下：</p>
<p><img data-src="UHD.png" alt="Intel 显卡"></p>
<p>这就是那个集成显卡，是集成在 i5 的那个 CPU 里的。UHD不适用于硬核游戏。相反，它提供了用于管理笔记本电脑的屏幕和一些轻便游戏的基本性能。</p>
<p>它是 VGA compatible controller。</p>
<h3 id="NVIDIA-GeForce-MX250"><a href="#NVIDIA-GeForce-MX250" class="headerlink" title="NVIDIA GeForce MX250"></a>NVIDIA GeForce MX250</h3><p>这就是那个独立显卡，经典 Nvidia 的，<code>$ lspci | grep -i Nvidia</code> 看一下：</p>
<p><img data-src="Nvidia.png" alt="Nvidia 显卡"></p>
<p>查了一下这个显卡也不咋地，只能说比集显好一点。功耗低性能差，游戏绘图设计就别考虑了。<strong>彳亍</strong>。</p>
<p>然后我意外地发现这个 Nvidia 的好像在这没装驱动，也就是说一直没用上。。。</p>
<h1 id="装驱动"><a href="#装驱动" class="headerlink" title="装驱动"></a>装驱动</h1><p>发现连显卡驱动都没有赶紧装一下<span class="github-emoji" alias="disappointed_relieved" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f625.png?v8">&#x1f625;</span></p>
<p><a href="https://phoenixnap.com/kb/install-nvidia-drivers-ubuntu">找了一篇教程</a></p>
<p><a href="https://www.cyberciti.biz/faq/ubuntu-linux-install-nvidia-driver-latest-proprietary-driver/#verification">另一篇教程</a></p>
<p>直接GUI图形化安装吧。</p>
<ol>
<li>直接在软件搜索里搜 <code>driver</code> ，点那个绿的就跳出来了。</li>
</ol>
<p><img data-src="find_friver.png" alt="找driver"></p>
<ol>
<li>这里有好多种驱动，选一种安装。</li>
</ol>
<p><img data-src="install_driver.png" alt="安装驱动"></p>
<p>还好问了一下IT侠这几个的区别，前几个是nv做的闭源驱动，最后一个是开源的，建议上网搜一下看自己的卡用哪个驱动好。</p>
<p>这上面显示460是recommended，那就用它了。</p>
<p><img data-src="drivers.png" alt="选这个"></p>
<p>安装成功之后，重启。</p>
<p><code>$ nvidia-smi</code> 检查一下，出现这个界面就成功了。</p>
<p><img data-src="driver_succ.png" alt="驱动安装成功"></p>
<p><code>$ nvidia-setting</code> 打开驱动设置。</p>
<p><code>$ sudo prime-select nvidia/intel</code> 可以切换显卡。</p>
<p>（有趣的是过了几天我的 <code>$ nvidia-smi</code> 突然打不开了，显示什么 <code>communicate failed</code>，解决方法是在 <code>intel</code> 和 <code>nvidia</code> 之间切换一下再重启就可以了。）</p>
<hr>
<h1 id="装-Cuda"><a href="#装-Cuda" class="headerlink" title="装 Cuda"></a>装 Cuda</h1><p>终于一切准别就绪，开始安装cuda。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/">官方文档</a></p>
<ul>
<li>GPU 一定要是支持 cuda 的。</li>
<li>还需要 <code>gcc</code>，应该都有吧。</li>
<li>可选安装一个 GDS。（我也不知道这是啥）</li>
<li><p>首先还要下一个 <strong>CUDA Toolkit</strong>，在<a href="https://developer.nvidia.com/cuda-downloads">这</a>下，进去选择自己的设备型号按指示就可以了。</p>
<ul>
<li>遇到一个问题就是我们已经安装了 Nvidia Driver，所以在后面 sh xxx.run 时有一个安装 driver 的选项就不要选了，否则会失败。</li>
<li>这也说明在 Cuda Toolkit 中会自带一个驱动从而不一定要先装显卡驱动？</li>
<li>出现这个就是成功辣！<img data-src="toolkit_succ.png" alt="安装成功！"></li>
</ul>
</li>
<li><p>最后设置一下环境变量，<code>$ sudo vim ~/.zshrc</code> 中添加：</p>
<ul>
<li><p><code>export PATH=/usr/local/cuda-11.4/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</code> （注意这里的cuda-xx.x文件夹要看下载的是什么版本）</p>
</li>
<li><p><code>export LD_LIBRARY_PATH=/usr/local/cuda-11.4/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</code></p>
</li>
</ul>
</li>
</ul>
<p>到这里其实就差不多完成了，运行一下 <code>$ nvcc -V</code> 如下显示就可以了。</p>
<p><img data-src="nvcc.png" alt="nvcc"></p>
<p>然后在 <code>/usr/local/cuda-11.4/samples</code> 中有一些测试用例，可以自己玩一玩。</p>
<h2 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h2><p>我在之后回看的时候发现了一些问题，在知乎上找到一篇<a href="https://zhuanlan.zhihu.com/p/91334380">很好的文章</a>讲清楚了基本所有涉及到的概念。</p>
<ul>
<li><p>一个是在上上那张图里的 warning 中说没有配上 driver，并且至少要 470 的驱动。</p>
<ul>
<li>但是我的 sample 中的样例都可以跑。。。所以暂时不严谨地认为 cuda 还是能用的（，驱动和cuda版本不兼容的问题暂时还没有暴露出来。</li>
<li>不过我不知道跑 sample 是否需要 drive 的正确参与，如果不一定的话还是说明只有一个没有兼容上驱动的 cuda壳子。</li>
<li>如果到时候出现了版本兼容的问题，考虑的解决方法是将 cuda 和驱动全卸了按照版本对应重装。这就提到了我在上面装 cuda 的那个步骤中，其实运行 <code>.run</code> 文件的时候就可以在那里装驱动了，它版本应该是帮我配好了的。</li>
<li>其实电脑上是可以装上多个驱动和多个 cuda 的。不过多个驱动好像 ubuntu 上会崩，yls就这么翻车了（。多个 cuda 可以通过在 <code>$PATH</code> 中放那个 <code>...\cuda\bin</code> 路径而不是带版本号的 <code>...\cudax.x\bin</code>路径，然后通过 cuda 的符号链接来控制多个版本。不过一是为了避免复杂性二是我这电脑磁盘真不够了qwq，会考虑全卸了重装。</li>
</ul>
</li>
<li><p>另一个问题其实是在装 <code>pytorch</code> 时遇到的，在 <code>conda install</code> 时会装一个也叫 <code>cudatoolkit</code> 的包，搞不懂这个和 Nvidia 官网下的有啥区别。并且 conda 装的 <code>pytorch</code> 和 <code>cudatoolkit</code> 版本都是11.1的，不知道和我那个大 cuda 又有什么区别。</p>
<ul>
<li>这个其实就在我上面提到的那个文章以及它引用的<a href="https://www.cnblogs.com/yhjoker/p/10972795.html">另一篇</a>中有详细的阐释。总之就是 Nvidia 官方下的那个大 cuda 是个完整的工具包；而像 pytorch 这样的框架，只要用到 cuda 中的动态链接库。所以 conda 在下 <code>pytorch</code> 时 ，就会装一个 “mini cuda toolkit”（其实是和大 cuda 有重叠的），只要有和这个小 cuda 兼容的驱动，pytorch 就能跑。</li>
<li>所以其实如果我们还没装大 cuda，<code>torch.cuda.is_avaliable()</code> 应该也是 <code>True</code>，<code>pytorch</code> 没有大开发需求的话基本可以正常用的。这也因为小 cuda 的版本是 11.1，应该是和我们 460 的驱动兼容的。</li>
</ul>
</li>
</ul>
<p>总之，大 cuda （用大小来外号这两个cuda好像也是个好方法<span class="github-emoji" alias="joy" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f602.png?v8">&#x1f602;</span>）和 driver 好像就是没兼容上的，不过我们有兼容好的小 cuda，跑 <code>pytorch</code> 是基本没问题的。出了问题再说（挖坑）。</p>
<h2 id="卸载"><a href="#卸载" class="headerlink" title="卸载"></a>卸载</h2><p>因为上述问题要重装所以再总结一下卸载流程。</p>
<p>cuda 有自带的卸载脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo /usr/<span class="built_in">local</span>/cuda/bin/cuda-uninstaller</span></span><br></pre></td></tr></table></figure>
<hr>
<h1 id="装-cuDNN"><a href="#装-cuDNN" class="headerlink" title="装 cuDNN"></a>装 cuDNN</h1><p><a href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html">官方文档</a></p>
<p>首先需要一个nvidia的账号，自己注册一个。</p>
<p>首先要在<a href="https://developer.nvidia.com/rdp/cudnn-download">这</a>下载cuDNN，完成一个调查表，点击同意协议就可以出现了，选择自己的版本的下载。</p>
<p>下载这几个：</p>
<p><img data-src="cudnn.png" alt="cudnn list"></p>
<ul>
<li>将 <code>cuDNN Library for Linux</code> 解压，复制到 cuda 环境中。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tar -xzvf cudnn-x.x-linux-x64-v8.x.x.x.tgz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo cp cuda/include/cudnn*.h /usr/<span class="built_in">local</span>/cuda/include</span> </span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo cp -P cuda/lib64/libcudnn* /usr/<span class="built_in">local</span>/cuda/lib64</span> </span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo chmod a+r /usr/<span class="built_in">local</span>/cuda/include/cudnn*.h /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn*</span></span><br></pre></td></tr></table></figure>
<ul>
<li>将 <code>.deb</code> 文件安装。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo dpkg -i libcudnn8_x.x.x-1+cudax.x_amd64.deb	<span class="comment"># runtime library</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo dpkg -i libcudnn8-dev_8.x.x.x-1+cudax.x_amd64.deb	<span class="comment"># developer librarty</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo dpkg -i libcudnn8-samples_8.x.x.x-1+cudax.x_amd64.deb	<span class="comment"># samples and doc</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li>检查一下。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cp -r /usr/src/cudnn_samples_v8/ <span class="variable">$HOME</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span>  <span class="variable">$HOME</span>/cudnn_samples_v8/mnistCUDNN</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> make clean &amp;&amp; make</span></span><br></pre></td></tr></table></figure>
<p>我遇到一个报错：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; WARNING - FreeImage is not <span class="built_in">set</span> up correctly. Please ensure FreeImage is <span class="built_in">set</span> up correctly. &lt;&lt;&lt;</span></span><br></pre></td></tr></table></figure>
<p>检查一下是一个包没有安装，装一下就行。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install libfreeimage3 libfreeimage-dev</span></span><br></pre></td></tr></table></figure>
<p>编译完之后运行一下可执行文件，出现 <code>Test passed!</code> 就说明 cuDNN 已经正确安装辣！:happy:</p>
<h2 id="卸载-1"><a href="#卸载-1" class="headerlink" title="卸载"></a>卸载</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> rm /usr/<span class="built_in">local</span>/cuda/include/cudnn.h</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> rm /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn*</span></span><br></pre></td></tr></table></figure>
<p>然后还要把几个 <code>.deb</code> 包点开卸载，我看网上的教程都没有这一步，我觉得还是必要的因为版本不一样。</p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title>数字图像处理课程笔记</title>
    <url>/2021/09/08/digital-image-processing/</url>
    <content><![CDATA[<h1 id="Ch1-概述"><a href="#Ch1-概述" class="headerlink" title="Ch1 概述"></a>Ch1 概述</h1><ul>
<li><p>数字图像是指由被称作像素的小块区域组成的二维矩阵。</p>
<ul>
<li>将物理图像行列划分后，每个小块区域称为像素（pixel）。</li>
<li>每个像素包括两个属性：位置和亮度 （或色彩）。</li>
</ul>
</li>
<li><p>数字图像的两个基本问题</p>
<ul>
<li>像素数目</li>
<li>灰度值的度量</li>
<li>本质问题就是物理信号离散化，数字化</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="图像获取"><a href="#图像获取" class="headerlink" title="图像获取"></a>图像获取</h2><p>图像获取就是图像数字化的过程，包括扫描、<strong>采样（成像技术）</strong>和<strong>量化（模数转化技术）</strong>。</p>
<ul>
<li><p>图像采样：用有限的样本数目去近似无限的现实物理信号（有限近似无限）。</p>
</li>
<li><p>图像量化，用离散计算机表示去近似连续的现实物理信号（离散近似连续）。</p>
<ul>
<li>取样点数和量化级数的选取<ul>
<li>条件：图像有 $M\times N$个像素，每个像素有 $Q$ 个灰度级别。</li>
<li>取值规则：$M, N, Q$ 通常取 2 的整数次幂。若 $Q = 2^8 = 256$，称为 8bit 量化，或称256灰度级。</li>
<li>取值范围：由于存在量化误差，原则上 b 越大重建图像失真越小。对于人眼应用 b 取 5-8；而对于卫星图片等图像分析应用b取8-12。</li>
</ul>
</li>
<li>$M\times N$ 必须满足奈奎斯特取样定理，否则会因取样点数不够产生混淆失真。</li>
</ul>
</li>
</ul>
<ul>
<li>最佳量化<ul>
<li>使量化误差最小的量化方法为最佳量化。</li>
<li>使用均方误差测度讨论最佳量化。</li>
</ul>
</li>
</ul>
<p>设 $Z$ 和 $q$ 分别代表像素幅度和其量化值，$p(Z)$ 为像素幅度概率密度函数；$Z$ 的取值范围在 $H_1$ 和 $H_2$ 之间，量化总层数为 $K$，$\delta^2$ 表示量化器量化的均方误差，则</p>
<script type="math/tex; mode=display">
\delta^2 = \sum\limits_{k=1}^{K} \int_{Z_k}^{Z_{k+1}} (Z - q_k)^2p(Z) dZ。</script><p>当量化层数足够大，每个判决层的 $p(Z)$ 可以近似为均匀分布，则</p>
<script type="math/tex; mode=display">
\begin{aligned}
\delta^{2} &=\sum_{k=1}^{K} \int_{Z_{k}}^{Z_{k+1}}\left(Z-q_{k}\right)^{2} p(Z) d Z \\
&=\sum_{k=1}^{K} p(Z) \int_{Z_{k}}^{Z_{k+1}}\left(Z-q_{k}\right)^{2} d Z \\
&=\frac{1}{3} \sum_{k=1}^{K} p(Z)\left[\left(Z_{k+1}-q_{k}\right)^{3}-\left(Z_{k}-q_{k}\right)^{3}\right]
\end{aligned}</script><p>对 $q_k$ 求导并令等于 $0$：</p>
<script type="math/tex; mode=display">
0=\int_{Z_{k}}^{Z_{k+1}} p(Z)\left(-2 Z+2 q_{k}\right) d Z</script><p>于是</p>
<script type="math/tex; mode=display">
q_{k}=\dfrac{\int_{Z_{k}}^{Z_{k+1}} Z p(Z) d Z}{\int_{Z_{k}}^{Z_{k+1}} p(Z) d Z}</script><p>若 $p(Z)$ 为常数，则 $q_k = \frac{1}{2}(Z_k + Z_{k+1})$，此时量化误差为 $\dfrac{(H_2 - H_1)^2}{12K^2}$。</p>
<ul>
<li>图像获取设备的主要应用<ul>
<li>基于图像采集卡或图像卡将模拟制式的视频信号采集到计算机。</li>
<li>摄象机本身带有数字化部件可以直接将数字图像通过计算机端口（如并口、USB接口）或标准设备（如磁盘驱动器）传送进计算机。</li>
</ul>
</li>
</ul>
<h2 id="图像显示"><a href="#图像显示" class="headerlink" title="图像显示"></a>图像显示</h2><ul>
<li>图像显示和打印的基本原理<ul>
<li>显示分辨率：每英寸荧光点或打印墨点的数目，单位是dpi。</li>
<li>电视机平均分辨率0.76mm。SVGA显示器平均分辨率0.28mm。</li>
<li>打印分辨率需要在300dpi—600dpi之上。</li>
</ul>
</li>
<li>点阵位图格式<ul>
<li>用文件中的几个字节表示表示像素点的位置和颜色。</li>
<li>图像分辨率：每英寸像素点的数目，单位ppi。</li>
</ul>
</li>
<li>显示与文件的关系<ul>
<li>若图像分辨率比显示分辨率高，则显示出的图像比实际图像大。</li>
<li>若图像分辨率比显示分辨率低，则显示出的图像比实际图像小。</li>
</ul>
</li>
</ul>
<h2 id="图像表示"><a href="#图像表示" class="headerlink" title="图像表示"></a>图像表示</h2><ul>
<li>1位二值图像</li>
<li><p>8位灰度图像</p>
<ul>
<li>位平面。<ul>
<li>数字图像每一个像素点的灰度值均可以表示成二进制数，这样从二进制每一位的微观角度出发，能够将图片分解为若干个位平面。</li>
<li>图像的重要信息主要包含在高位位平面中，从低位位平面的显示效果中不可识别原始图像信息。</li>
</ul>
</li>
<li>抖动技术（dithering）<ul>
<li>基本策略是平衡灰度分辨率到空间分辨率。基本原理是当人眼从较远距离观察图像时，将会平均局部小区域像素的灰度值，并记录之。</li>
<li>很明显采用抖动矩阵无法完整的表达图像信息，甚至造成打印的图像失真。<em>解决方案：Floyd-Steinberg算法</em>。</li>
</ul>
</li>
</ul>
</li>
<li><p>24位彩色图像</p>
<ul>
<li><p>RGB模式。(相加混色模型，用于发光物体)</p>
</li>
<li><p>CMYK模式。（相减混色模型，用于不发光物体）</p>
<ul>
<li>三次色（青色Cyan，洋红Magenta和黄色Yellow）K为真正黑色。</li>
</ul>
</li>
<li><p>HSB模型。</p>
<ul>
<li>色相（Hue），饱和度（Saturation），亮度（Brightness）。</li>
<li><em>在绘画艺术上，饱和度的概念可描述如下：是一种纯色加上白色染料后的结果，此时色彩的饱和度发生变化而色彩的亮度不发生变化。</em></li>
<li><em>同样在绘画艺术上，亮度的概念可描述如下：是一种纯色加上黑色染料后的结果，此时色彩的亮度发生变化而色彩的饱和度不发生变化。</em></li>
</ul>
</li>
</ul>
</li>
<li>8位索引图像<ul>
<li>调色板（palette）：图像的色彩索引表。</li>
<li>若图像的色彩数远小于全色彩数时，当每个像素的值用色彩索引表的位置表示就可以大大节约存储空间。</li>
</ul>
</li>
</ul>
<h1 id="Ch2-二值图像与像素关系"><a href="#Ch2-二值图像与像素关系" class="headerlink" title="Ch2 二值图像与像素关系"></a>Ch2 二值图像与像素关系</h1><h2 id="邻域关系"><a href="#邻域关系" class="headerlink" title="邻域关系"></a>邻域关系</h2><ul>
<li>4邻域 $N_4(p)$<ul>
<li>邻域距离：$&lt;=1$。就是上下左右4个点。</li>
</ul>
</li>
<li>对角线邻域 $N_D(p)$<ul>
<li>四个对角。</li>
</ul>
</li>
<li>8邻域 $N_8(4) = N_4(p) + N_D(p)$<ul>
<li>邻域距离：$&lt;2$。就是周围一圈8个点。</li>
</ul>
</li>
</ul>
<h2 id="连接关系"><a href="#连接关系" class="headerlink" title="连接关系"></a>连接关系</h2><p>可以用来分离物体，鉴定边界。</p>
<p>在二值图上通过邻域进行定义：</p>
<ul>
<li>4连接<ul>
<li>$p \in N_4(q)$</li>
</ul>
</li>
<li>8连接<ul>
<li>$p \in N_8(q)$</li>
</ul>
</li>
<li>m连接<ul>
<li>$p \in N_D(q) \wedge (N_4(P) \cap N_4(q) = \emptyset)$</li>
</ul>
</li>
</ul>
<h2 id="邻接关系"><a href="#邻接关系" class="headerlink" title="邻接关系"></a>邻接关系</h2><ul>
<li><p>有连接关系 $(4,8,m)$ 的像素 $p,q$ 是邻接关系。</p>
</li>
<li><p>两个子图像 $S_1, S_2$，如果存在 $S_1$ 中的某像素和 $S_2$ 中的某像素是邻接关系，则两子图是邻接关系。</p>
</li>
<li><p>从 $p $ 到 $q$ 的一条<strong>路径</strong>是值一个像素序列 $p_1, .., p_n$，$p_i$ 与 $p_{i+1}$ 邻接。</p>
<ul>
<li>像素 $p,q$ 之间存在一条路径，则称 $p,q$ <strong>联通</strong>。</li>
</ul>
</li>
<li><p>图像 $S$ 中的任何一个像素 $p$，与 $p$ 联通的所有像素的集合构成一个<strong>联通分量</strong>，联通分量中所有像素两两联通。</p>
<ul>
<li>4连接：4联通分量</li>
<li>8/m连接：2联通分量</li>
</ul>
</li>
<li><p><strong>对象分割</strong>：标记联通分量。</p>
<ul>
<li>算法就是遍历扫描，没有邻接标记，有邻接标记为邻接标记并记录等价类。</li>
</ul>
</li>
</ul>
<h2 id="距离"><a href="#距离" class="headerlink" title="距离"></a>距离</h2><ul>
<li>正定性：$D(p,q) \geq 0$，等号当且仅当 $p = q$。</li>
<li>对称性：$D(p,q) = D(q,p)$。</li>
<li>三角不等式：$D(p,q) \geq D(p,z) + D(q,z)$。</li>
</ul>
<p>常见距离：</p>
<ul>
<li>Minkowski 距离<ul>
<li>$D(\boldsymbol{p}, \boldsymbol{q})=\left(\sum_{i=1}^{n}\left|p_{i}-q_{i}\right|^{k}\right)^{\frac{1}{k}}$</li>
</ul>
</li>
</ul>
<h2 id="骨架抽取"><a href="#骨架抽取" class="headerlink" title="骨架抽取"></a>骨架抽取</h2><ul>
<li><p>计算距离图像：</p>
<ul>
<li>初始化距离为无穷大。</li>
<li>边缘点：8邻域有背景点，距离为0。</li>
<li>非边缘点，距离为8邻域中最小距离+1。</li>
<li>反复扫描图像，重复上述步骤。</li>
</ul>
</li>
<li><p>计算骨架图像：</p>
<ul>
<li>最大值在骨架上。</li>
<li>以最小梯度扩展。</li>
</ul>
</li>
</ul>
<h1 id="Ch3-灰度直方图与点运算"><a href="#Ch3-灰度直方图与点运算" class="headerlink" title="Ch3 灰度直方图与点运算"></a>Ch3 灰度直方图与点运算</h1><h2 id="灰度直方图-Histogram"><a href="#灰度直方图-Histogram" class="headerlink" title="灰度直方图 Histogram"></a>灰度直方图 Histogram</h2><p>定义：灰度级的函数，描述的是图像中每种灰度级像素的个数，反映图像中每种灰度出现的频率。横坐标是灰度级，纵坐标是灰度级出现的频率。</p>
<p>$\mathrm{H}(\mathrm{D})=\lim\limits_{\Delta \mathrm{D} \rightarrow 0} \dfrac{\mathrm{A}(\mathrm{D})-\mathrm{A}(\mathrm{D}+\Delta \mathrm{D})}{\mathrm{D}-(\mathrm{D}+\Delta \mathrm{D})}=\lim\limits_{\Delta \mathrm{D} \rightarrow 0} \dfrac{\mathrm{A}(\mathrm{D})-\mathrm{A}(\mathrm{D}+\Delta \mathrm{D})}{-\Delta \mathrm{D}}=-\dfrac{\mathrm{d}}{\mathrm{dD}} \mathrm{A}(\mathrm{D})$</p>
<p>其中 $A(D)$ 为阈值面积函数：为一幅连续图像中被具有灰度级 $D$ 的所有轮廓线所包围的面积。</p>
<p>对于离散函数，固定 $\Delta D$ 为 $1$，则 $H(D) = A(D+1) - A(D)$。</p>
<p><em>高维色彩直方图</em>。</p>
<h2 id="性质与计算"><a href="#性质与计算" class="headerlink" title="性质与计算"></a>性质与计算</h2><ul>
<li>不表示图像的空间信息。</li>
<li><p>任一特定图像都有唯一直方图，但反之并不成立。</p>
</li>
<li><p>若一幅图像包含一个灰度均匀一致，且背景与物体对比度很强，假设物体的边界由灰度级 $D_1$ 定义的轮廓线，则</p>
<ul>
<li>$\int_{D_{1}}^{\infty} H(D) d D = \text{物体的面积}$</li>
</ul>
</li>
<li><p>直方图的可加性：一副图像由若干个不相交的区域构成，则整幅图像的直方图是这若干个区域直方图之和。</p>
</li>
</ul>
<h2 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h2><ul>
<li>数字化参数<ul>
<li>一般一幅数字图像应该利用全部或几乎全部可能的灰度级。</li>
<li>对直方图做快速检查。</li>
</ul>
</li>
<li>边界阈值选择<ul>
<li>使用轮廓线确定简单物体边界的方法，称为<strong>阈值化</strong>。</li>
<li>对物体与背景有较强对比的景物的分割特别有用。</li>
<li>双峰直方图。</li>
</ul>
</li>
<li>摄影中的常见名词<ul>
<li>过曝（Overexposed）</li>
<li>正常曝光（Average）</li>
<li>曝光不足（Underexposed）</li>
</ul>
</li>
<li>图像增强<ul>
<li>图像增强技术不需要考虑图像降质的原因，只将图像中感兴趣的特征有选择地突出，将不需要的特征进行衰减。</li>
<li>图像增强没有一个的统一理论，如何评价图像增强的结果好坏也没有统一的标准。</li>
<li>目的是改善图像视觉效果，便于观察和分析；便于人工或机器对图像的进一步处理。</li>
<li>有空间域法：点处理（图象灰度变换、直方图均衡、伪彩色处理等）；<em>频率域法：高、低通滤波、同态滤波等。</em><ul>
<li><strong>空间域</strong>图像增强技术指在空间域中，通过线性或非线性变换来增强构成图像的像素。</li>
<li>点处理：作用于单个像素的空间域处理方法，包括图像灰度变换、直方图处理、伪彩色处理等技术。</li>
<li>模板 处理：作用于像素邻域的处理方法，包括空域平滑、空域锐化等技术。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="点运算-Point-Operation"><a href="#点运算-Point-Operation" class="headerlink" title="点运算 Point Operation"></a>点运算 Point Operation</h2><p>定义：对于一幅输入图像，将产生一幅输出图像，输出图像的每个像素点的灰度值由输入像素点决定。</p>
<ul>
<li><p>点运算由灰度变换函数（gray-scale transformation, GST）确定。</p>
</li>
<li><p>令 $r$ 和 $s$ 所定义的变量分别是 $f(x, y)$ 和 $g(x,y)$ 在任意点 $(x,y)$ 的灰度级，则 $T$ 为灰度变换函数</p>
<ul>
<li>$s = T(r)$</li>
<li>与局部（邻域）运算的区别：输入像素-输出像素一一对应。</li>
<li>与几何运算的差别，不改变图像的空间关系。</li>
</ul>
</li>
<li>运用<ul>
<li>光度学标定（photometric calibration），希望数字图像的灰度能够真实反映图像的物理特性。</li>
<li>对比度增强（contrast enhancement）或对比度扩展（contrast stretching）。</li>
<li>显示标定（display calibration）<ul>
<li>显示设备不能线性地将灰度值转换为光强度，因此点运算和显示非线性组合可以保持显示图像时的线性关系。</li>
</ul>
</li>
</ul>
</li>
<li>点运算种类 $0 \leq r \leq 1$，$0$ 黑 $1$ 白<ul>
<li>线性点运算 $s = a \times r + b$。</li>
<li>非线性点运算。</li>
<li>图像反转 $s = L - r - 1$。<ul>
<li>反转变换适用于增强嵌入于图像暗色区域的白色或灰色细节, 特别是当黑色面积占主导地位时。</li>
</ul>
</li>
<li>对数变换 $s = c \log{(1 + r)}$<ul>
<li>对数变换使一窄带低灰度输入图像值映射为一宽带输出值，这种变换可以用来扩展被压缩的高值图像中的暗像素。</li>
</ul>
</li>
<li>幂次变换 $s = c r^{\gamma}$</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>课程</category>
        <category>Digital Image Processing</category>
      </categories>
  </entry>
  <entry>
    <title>高级算法课程笔记-指纹</title>
    <url>/2021/09/09/fingerprinting/</url>
    <content><![CDATA[<h1 id="多项式判等-Polynomial-Identity-Testing"><a href="#多项式判等-Polynomial-Identity-Testing" class="headerlink" title="多项式判等 Polynomial Identity Testing"></a>多项式判等 Polynomial Identity Testing</h1><p><strong>PIT问题</strong>：</p>
<p>输入两个 $d$ 阶多项式 $f, g \in \mathbb{F}[x]$，输出他们是否相等 $f \equiv g$。</p>
<p>这里 $\mathbb{F}[x]$ 指 $x$ 在域（field）$\mathbb{F}$ 上的多项式环（ring）。</p>
<p>我们可以很自然地将这个问题转化为判断一个 $d$ 阶多项式（$f - g$）是否恒为 0。</p>
<p>如果 $f$ 被显式地给出了，我们很容易地可以通过遍历它的系数来判断。为了使问题更加 nontrivial，假设 $f$ 是一个黑盒（black box/ oracle），看不到系数，只能通过输入 $x$ 得到 $f(x)$。</p>
<a id="more"></a>
<ul>
<li>一个直接的确定性方法就是带入 $d+1$ 个不同的数计算 $f(x_1), f(x_2), …,f(x_{d+1})$ 来计算是否全为零，若全为零则根据<strong>代数基本定理</strong>，能保证 $d$ 阶多项式 $f$ 一定恒为零。</li>
</ul>
<div class="note info">
            <p>一个非零 $d$ 阶单变量多项式一定至多有 $d$ 个根。</p>
          </div>
<p>然后自然地我们有如下算法：</p>
<ol>
<li>suppose we have a finite subset $S \subseteq \mathbb{F}$</li>
<li>pick $r \in S$ uniformly at random</li>
<li>if $f(r) = 0$ return ‘yes’ else ‘no’</li>
</ol>
<p>我们很容易可以得出：</p>
<ul>
<li>如果 $f \equiv 0$，算法永远返回 ‘yes’。</li>
<li>如果 $f \not \equiv 0$，算法可能错误地返回 ‘yes’（假阳性），但这只当 $r$ 是 $f$ 的根时发生。根据代数基本定理，$f$ 最多 $d$ 个根，所以算法出错的可能性有界：<ul>
<li>$Pr[f(r) = 0] \leq \dfrac{d}{|S|}$。</li>
<li>我们可以固定 $|S| = 2d$，这样出错的概率最多就是 $1/2$。然后通过独立重复 $\log_{2}{\frac{1}{\delta}}$ 次来将这个概率减小到 $\delta$。</li>
</ul>
</li>
</ul>
<h2 id="Equality-的通信复杂性"><a href="#Equality-的通信复杂性" class="headerlink" title="Equality 的通信复杂性"></a>Equality 的通信复杂性</h2><p> <strong>通信复杂性</strong>（<a href="http://en.wikipedia.org/wiki/Communication_complexity">communication complexity</a>）是姚期智先生（对就是那个图灵奖）提出来的多个实体的计算模型。简单来说就是有两个端，分别有两个输入 $a$ 和 $b$，我们要计算一个函数 $f(a,b)$，不考虑计算的复杂性，我们只关心<strong>通信协议（communication protocol）</strong>，也就是两个端传输的信息的比特。</p>
<p><del>这个其实在问求4讲过不过我一直没搞明白。</del></p>
<p>在这里我们讨论的函数是 $\mathrm{EQ}: \{0, 1\}^n \times \{0, 1\}^n \to \{0, 1\}$，对任意 $a, b \in \{0, 1\}^n$ 有：</p>
<p>$\mathrm{EQ}(a, b)= \begin{cases}1 &amp; \text { if } a=b \\ 0 &amp; \text { otherwise }\end{cases}$</p>
<ul>
<li>最简单的想法就是把整个 $a$ 或 $b$ 传过去，通信复杂度是 $n$ 个 bit。<ul>
<li>可证（yao 证出来的）这是最好的确定性通信协议，也就是要确定地保证完全相等，至少需要 n bit的传输。</li>
<li>这个证明其实远比它看起来要 nontrivial。</li>
</ul>
</li>
</ul>
<p>我们可以考虑将 $a,b$ 看成 $n$ 阶多项式 $f,g$，然后运用我们刚才的 <strong>PIT</strong> 来解决。</p>
<p>需要传输 $r, f(r)$，$r \in [2n]$ 所以需要 $O(\log{n})$ 个bit，而 $f(r)$ 实际上是 $f(r) = \sum\limits_{i = 0}^{n-1} a_i r^i = O(r^n) = O(n^n)$ 是一个指数量级，需要 $O(n\log{n})$ 个bit。这比直接传整个数据库过去还多，<del>屁用没有</del>。</p>
<ul>
<li>所以根本原因在于我们选的<strong>域</strong>太大了，是和 $n$ 正比的，能不能找到小一点的，最好是一个常数大小的域。<ul>
<li>很快地我们就能够想到在模意义下取数，也就是 $\mathbb{Z}_p = \{0, 1, …, p-1\}$。</li>
</ul>
</li>
</ul>
<ul>
<li>我们只需要随机取一个质数 $p \in [n^2, 2n^2]$（可证一定能找到），$f,g \in \mathbb{Z}_p[x]$，然后随机取 $r \in [p]$。<ul>
<li>这样所需要传输的数据 $r, f(r) &lt; p$，只需要 $O(\log{p}) = O(\log{n})$ 的通信复杂度。</li>
<li>单边错概率是 $\dfrac{n}{p} = O(\dfrac{1}{n})$，w.h.p。</li>
</ul>
</li>
</ul>
<h2 id="Schwartz-Zippel-定理"><a href="#Schwartz-Zippel-定理" class="headerlink" title="Schwartz-Zippel 定理"></a>Schwartz-Zippel 定理</h2><p>接下来我们再来看<strong>PIT问题</strong>的更一般的形式，也就是推广到多元：</p>
<p>对于两个 $n$ 元 $d$ 阶多项式 $f, g \in \mathbb{F}[x_1, …, x_n]$，判断它们是否相等 $f \equiv g$。</p>
<p>仍然我们可以将问题转化成多项式判零：</p>
<p>$f\left(x_{1}, \ldots, x_{n}\right)=\sum\limits_{i_{1}, \ldots, i_{n} \geq 0} a_{i_{1}, i_{2}, \ldots, i_{n}} x_{1}^{i_{1}} x_{2}^{i_{2} \ldots} x_{n}^{i_{n}} \equiv 0?$</p>
<p>这里注意一个多元多项式的阶（degree）是指一项中所有变量的幂次的和的最大值，所以进一步的：</p>
<p>$f\left(x_{1}, \ldots, x_{n}\right)=\sum\limits_{i_{1}, \ldots i_{n} \geq 0 \atop i_{1}+\cdots+i_{n} \leq d} a_{i_{1}, i_{2}, \ldots, i_{n}} x_{1}^{i_{1}} x_{2}^{i_{2} \ldots} x_{n}^{i_{n}}$</p>
<p>如果 $f$ 是显式地给出的，我们可以遍历所有系数，共最多 $\left(\begin{array}{c} n+d \\ d \end{array}\right) \leq(n+d)^{d}$ 个来检查是否为零（这里其实用到了一点组合知识）。</p>
<p>一样的 $f$ 会作为一个黑盒，给定 $\vec{x} \in \mathbb{F}^{n}$，返回 $f(\vec{x})$。</p>
<p>或者作为乘积形式（product form）。比如范德蒙行列式（Vandermonde determinant）：</p>
<script type="math/tex; mode=display">
M=\left[\begin{array}{ccccc}
1 & x_{1} & x_{1}^{2} & \ldots & x_{1}^{n-1} \\
1 & x_{2} & x_{2}^{2} & \ldots & x_{2}^{n-1} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n} & x_{n}^{2} & \ldots & x_{n}^{n-1}
\end{array}\right] \quad f(\vec{x})=\operatorname{det}(M)=\prod\limits_{j<i}\left(x_{i}-x_{j}\right)</script><p>乘积形式的特点就在计算一个点处的函数值非常方便，而展开算系数就会极其复杂。</p>
<p>如果能存在一个多项式时间的确定性算法解 PIT，那意味着两个本质性问题：$\mathbf{NEXP} \neq \mathbf{P/poly}$ 或者 $\mathbf{\sharp P} \neq \mathbf{FP}$，当然我已经不懂了。</p>
<p>所以依然和上面类似的我们可以有一个随机算法，在 $S \subseteq \mathbb{F}$ 中随机取点，0返回 yes 否则 no。</p>
<p>而它的正确性，也就是假阳性，也就是取到它的根的概率由如下 <strong>Schwartz-Zippel 定理</strong> 给出：</p>
<div class="note info">
            <p>对于 $d$ 阶多项式 $f \in \mathbb{F}[x_1, x_2, …, x_n]$，如果 $f \not \equiv 0$，那么对于任何有限集 $S \in \mathbb{F}$ 和随机均匀独立选取的 $r_1, r_2, …, r_n \in S$，有</p><p>$Pr[f(r_1, r_2, …, r_n) = 0] \leq \dfrac{d}{|S|}$。</p>
          </div>
<p>这个定理也说明了 $d$ 阶 $n$ 元多项式在任意 $S^n$ 中的根的个数至多 $d \times |S|^{n-1}$ 个。</p>
<p>原证明较复杂，Dana Moshkovitz 在之后给出了一个很简洁优雅的证明，<del>我有空来写一遍</del>。</p>
]]></content>
      <categories>
        <category>课程</category>
        <category>Advanced Algorithm</category>
      </categories>
  </entry>
  <entry>
    <title>问题求解笔记-问题的形式化描述</title>
    <url>/2021/04/28/formalization/</url>
    <content><![CDATA[<p>庆祝终于脱离了常规算法的学习，进入了NP问题的坑。（</p>
<p>然后庆祝我们终于遇到了JH这本超级无敌晦涩难懂的教科书，真的是 <strong>太tmd难读了</strong>！！</p>
<p>在此附上助教对于该书的评价：</p>
<a id="more"></a>
<p><img data-src="chathistory.JPG" alt="最棒教科书" title="最棒教科书"></p>
<p>所以在整理笔记的同时，也是一个对内容汉化的过程，希望能有助于理解。。。<span class="github-emoji" alias="cold_sweat" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f630.png?v8">&#x1f630;</span></p>
<hr>
<h1 id="一些定义"><a href="#一些定义" class="headerlink" title="一些定义"></a>一些定义</h1><p><strong>字母表（alphabet）</strong>：一个非空有限集 $\Sigma$ 。</p>
<p><strong>符号（symbol）</strong>：字母表 $\Sigma$ 中的一个元素。</p>
<p><strong>字（word）</strong>：字母表中元素组成的一个序列。<strong>$\lambda$</strong>：空字。所有字的集合：$\Sigma^{\star}$</p>
<p><strong>字$w$长度$|w|$</strong>：该字中元素的个数。井$_a(w)$ ：字 $w$ 中符号 $a$ 出现的次数。</p>
<p>$\Sigma^n =  \{ x \in \Sigma | |x| = n\}$</p>
<p><strong>$u,v$ 的连接（concatenation）</strong>：$u,v$ 连起来，记作$uv$。</p>
<p><strong>前缀（prefix）、后缀（suffix）</strong>：前面/后面一截。</p>
<p><strong>子字（subword）</strong>：$z， w = uzv$。</p>
<p><strong>语言（language）</strong>：$L \subseteq \Sigma^{\star}$。它的补：$L^C = \Sigma^{\star} - L$。</p>
<p><strong>语言的连接</strong>：$L_1L_2 = L_1 \circ L_2 = \{ uv \in (\Sigma_1 \cup \Sigma_2)^{\star} | u \in L_1 ~ and ~ v \in L_2\}$。</p>
<p><strong>$\Sigma^{\star} $上的序</strong>：$\Sigma = \{s_1, s_2, \cdots, s_m \}$ 上有序 $s_1 &lt; s_2 &lt; \cdots &lt; s_m$，$u &lt; v$ 如果 $|u| &lt; |v|$ 或 $|u| = |v|, u = xs_iu’ , x = xs_jv’, i &lt; j$。先比长度，后比第一个不同的字符。</p>
<hr>
<h1 id="算法问题"><a href="#算法问题" class="headerlink" title="算法问题"></a>算法问题</h1><h2 id="判定问题（decision-problem）"><a href="#判定问题（decision-problem）" class="headerlink" title="判定问题（decision problem）"></a>判定问题（decision problem）</h2><p>$A$ 是一个算法， $x$ 是输入，$A(x)$ 标记为输出。</p>
<p>一个<strong>判定问题</strong>是指三元组 $(L, U, \Sigma), L \subseteq U \subseteq\Sigma^\star$，一般情况下 $U = \Sigma$，记为 $(L, \Sigma)$。算法 $A$ 解决这个问题是指对任意 $x \in U$ 有： $A(x) = 1, x\in L$ 并且 $A(x) = 0, x \in U - L$。</p>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><h3 id="素性检测（primality-testing）"><a href="#素性检测（primality-testing）" class="headerlink" title="素性检测（primality testing）"></a>素性检测（primality testing）</h3><script type="math/tex; mode=display">(Prim, \Sigma_{bool}), Prim = \{ w\in \{0, 1\} ^{star} | Number(w) \text{ is a prime}\}.</script><h3 id="多项式相等问题（EQ-POL）"><a href="#多项式相等问题（EQ-POL）" class="headerlink" title="多项式相等问题（EQ-POL）"></a>多项式相等问题（EQ-POL）</h3><h3 id="一次分支相等问题-EQ-1BP"><a href="#一次分支相等问题-EQ-1BP" class="headerlink" title="一次分支相等问题 (EQ-1BP)"></a>一次分支相等问题 (EQ-1BP)</h3><h3 id="满足性问题-（SAT）"><a href="#满足性问题-（SAT）" class="headerlink" title="满足性问题 （SAT）"></a>满足性问题 （SAT）</h3><script type="math/tex; mode=display">\text{SAT} = \{ w \in \Sigma^{\star}_{logic} | w \text{ is a code of a satisfable formula in CNF}\}.</script><h3 id="分团问题（CLIQUE）"><a href="#分团问题（CLIQUE）" class="headerlink" title="分团问题（CLIQUE）"></a>分团问题（CLIQUE）</h3><p>图中是否有大小为 $k$ 的团（clique）。</p>
<script type="math/tex; mode=display">CLIQUE = \{ x \# w \in \{ 0, 1, \#\}^{\star} | x\in \{0,1\}^{\star}, w \text{是一个有大小为 } Number(x) \text{ 的团的图}\}.</script><h3 id="覆盖问题（VCP）"><a href="#覆盖问题（VCP）" class="headerlink" title="覆盖问题（VCP）"></a>覆盖问题（VCP）</h3><p>图中是否有大小为 $k$ 的点覆盖。</p>
<h3 id="哈密顿回路问题（HC）"><a href="#哈密顿回路问题（HC）" class="headerlink" title="哈密顿回路问题（HC）"></a>哈密顿回路问题（HC）</h3><p>图中是否有哈密顿回路。</p>
<h2 id="优化问题（optimization-problem）"><a href="#优化问题（optimization-problem）" class="headerlink" title="优化问题（optimization problem）"></a>优化问题（optimization problem）</h2><p>一个优化问题是一个七元组 $U = (\Sigma_I, \Sigma_O, L, L_I, \mathcal{M}, cost, goal)$，其中：</p>
<ol>
<li>$\Sigma_I$ 是一个字母表，叫 $U$ 的输入字母表。</li>
<li>$\Sigma_O$ 是一个字母表，叫 $U$ 的输出字母表。</li>
<li>$L \subseteq \Sigma_{I}^{\star}$ 是可满足的问题实例的语言。</li>
<li>$L_I \subseteq L$ 是 $U$ 中的问题实例的语言。</li>
<li>$\mathcal{M} $ 是一个 $L \to Pot(\Sigma_{O}^{\star})$ 的函数，对任意的 $x \in L, \mathcal{M}(x)$ 是 $x$ 的可行解的集合。</li>
<li>$cost$ 是代价函数，对任意的数对 $(u,x), u \in \mathcal{M}(x), x \in L$，赋实数值 $cost(u,v)$。</li>
<li>$goal \in \{maximum, minimum\}$。</li>
</ol>
<h1 id="复杂度理论"><a href="#复杂度理论" class="headerlink" title="复杂度理论"></a>复杂度理论</h1><p>两个复杂度衡量：<strong>均匀消耗（uniform cost）</strong>，<strong>对数消耗（logarithmic cost）</strong></p>
<p>$Time_A(x), Space_A(x)$：算法 $A$ 对输入的 $x$ 的计算时间空间复杂度。</p>
<p><strong>定理 2.3.3.3.</strong> 对于一个判定问题 $(L , \Sigma_{bool})$，每个算法 $A$ 都存在另一个算法 $B$ 使得 $Time_B(n) = \log_2{(Time_A(n))}$</p>
<p><em>这个定理告诉我们对于 $L$ 不存在最优的算法，对 $L$ 的复杂度的定义也是没有意义的。所以人们通常不定义算法问题的复杂度而研究问题复杂度的上下界（符号标记同TC）。</em></p>
<p><strong>图灵机</strong>是算法的直观概念的形式化，这意味着问题 $U$ 可以被算法解决当且仅当存在一个图灵机可以解决它。</p>
<p>对于每个递增函数 $f : \mathbb{N} \to \mathbb{R}^+$：</p>
<p>（1）存在一个判定问题，每个图灵机可以在 $\Omega(f(n))$ 的复杂度内解决它。</p>
<p>（2）也存在一个图灵机可以在 $O(f(n)\cdot \log{(f(n))})$  的复杂度内解决它。</p>
<p>这意味着判定问题有无穷级的难度。复杂度理论的主要内容就是<em>寻找一类可实际解决的问题的形式化说明</em> 和 <em>开发能够根据它们在这类问题中的关系进行分类的方法</em>。</p>
<h2 id="形式化描述"><a href="#形式化描述" class="headerlink" title="形式化描述"></a>形式化描述</h2><p>对于一个图灵机（算法）$M$，$L(M)$ 是 $M$ 决定（decide）的语言。</p>
<p>能够在多项式时间内完成的复杂度问题类： </p>
<script type="math/tex; mode=display">P = \{L = L(M) |\exists c \in \mathbb{Z} , Time_M(n) \in O(n^c) \}</script><p>一个语言（判定问题）是<strong>可处理的（tractable）</strong> 当 $L \in P$，否则不可处理（intractable）。</p>
<p><strong>非确定性计算（nondeterministic computation）</strong>：引入随机数操作。</p>
<p>让 $M$ 为一个非确定性图灵机，定义 $M$ <strong>接受（accept）语言</strong> $L$，$L = L(M)$ 如果：</p>
<p>（1）对于任意 $x \in L$，$M$ 中存在至少一个接受（accept）$x$ 的计算。</p>
<p>（2）对于任意 $y \notin L$，$M$ 的所有计算拒绝（reject） $y$ 。</p>
<p>对于任意输入 $w$，$M$ 的时间复杂度 $Time_M(w)$ 是 $M$ 中的最短的接受的计算（accepting computation）。</p>
<p>通过非确定性算法能够在多项式时间内完成的判定问题类： </p>
<script type="math/tex; mode=display">NP = \{L(M) | M \text{是多项式时间的非确定性算法} \}</script><p>算法 <strong>B</strong> 对于输入 $x$ 的接受计算（accepting rejecting）可以看做对于是否 $x \in L$ 的证明。</p>
<p><em>这意味着确定性计算的复杂度是证明输出的正确性，非确定性计算的复杂度是对于一个给定的证明的确定的验证</em></p>
<p>$L \subseteq \Sigma^{\star}$ 是一个语言，一个输入为 $\Sigma^{\star} \times \{0, 1\}^{\star}$  的算法 <strong>A</strong> 称为 $L$ 的<strong>验证（verifier）</strong>，记作 $L = V(\textbf{A})$，如果 </p>
<script type="math/tex; mode=display">L = \{ w \in \Sigma^{\star} | A \text{ accept } (w,c) \text{，对于某个 } c \in \{0,1\}^{\star}\}</script><p>如果 $\textbf{A} \text{ accept } (w,c)$，我们称 $c$ 是事实（fact） $w \in L$ 的一个<strong>证明（proof/certificate）</strong>。</p>
<p><strong>A</strong> 是一个<strong>多项式时间验证（polynomial-time verifier）</strong>如果存在一个整数 $d$ ，使得对任意 $w\in L$，$Time_A(w, c) \in O(|w|^d)$ 对于某个 $w$ 的证明 $c$。</p>
<p>定义<strong>可多项式验证的语言类（class of polynomially verifiable languages）</strong>： </p>
<script type="math/tex; mode=display">VP = \{VP | A \text{ 是一个多项式验证}\}</script><p><strong>定理 2.3.3.9.</strong> $NP = VP$</p>
<p>$L_1 \subseteq \Sigma_1^{\star}, L_2 \subseteq \Sigma^{\star}_2$ 是两个语言，我们说 $L_1$ 对于 $L_2$ 是<strong>多项式时间可约的（polynomial-time reducible）</strong>，记作 $L_1 \leq_p L_2$，如果存在一个多项式时间算法 <strong>A</strong> 计算一个 $L_1$ 到 $L_2$ 的映射，即 </p>
<p><script type="math/tex">\forall x \in \Sigma_1^{\star}, x \in L_1 \Leftrightarrow A(x) \in L_2</script>。</p>
<p><strong>A</strong> 称作从 $L_1$ 到 $L_2$ 的多项式时间规约（reduction）。（说明 $L_2$ 至少和 $L_1$ 一样难。）</p>
<p>一个语言 $L$ 是 <strong>NP-hard</strong>，如果对于任意 $U \in NP, U \leq_p L$。</p>
<p>一个语言 $L$ 是 <strong>NP-complete</strong>，如果 $L \in NP$ 且 $L$ 是 NP-$hard$。</p>
]]></content>
      <categories>
        <category>课程</category>
        <category>problem-solving</category>
      </categories>
  </entry>
  <entry>
    <title>问题求解笔记-群同态基本定理与正规子群</title>
    <url>/2021/03/11/group-isomorphism-and-normal-subgroups/</url>
    <content><![CDATA[<hr>
<h1 id="同构-Isomorphisms"><a href="#同构-Isomorphisms" class="headerlink" title="同构 Isomorphisms"></a>同构 Isomorphisms</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>群$(G, \cdot )$ 和 $(H, \circ)$ <strong>同构（isomorphic）</strong>，如果存在一个双射 $\phi: G \rightarrow H$ 且满足</p>
<script type="math/tex; mode=display">\forall a, b \in G, \phi(a \cdot b) = \phi(a) \circ \phi(b)</script><p>记作 $G \cong H$。映射 $\phi$ 称为<strong>同构（isomorphism）</strong>。</p>
<a id="more"></a>
<p><strong>定理 9.6.</strong> $\phi: G \rightarrow H$ 是两个群上的同构，则有</p>
<ol>
<li>$\phi^{-1}: H \rightarrow G$ 也是一个同构。</li>
<li>$|G| = |H|$</li>
<li>$G$ 是阿贝尔群 $\Rightarrow$ $H$ 是阿贝尔群。</li>
<li>$G$ 是循环群 $\Rightarrow$ $H$ 是循环群。</li>
<li>$G$有一个 $n$ 阶子群 $\Rightarrow$ $H$ 有一个 $n$ 阶子群。</li>
</ol>
<p><strong>定理 9.7.</strong> 所有无限循环群和 $\mathbb{Z}$ 同构。</p>
<p><strong>定理 9.8.</strong> 所有 $n$ 阶循环群和 $\mathbb{Z}_{n}$ 同构。</p>
<p><strong>推论 9.9.</strong> $G$ 是一个 $p$ 阶群，$p$ 是一个质数，则 $G$ 同构于 $\mathbb{Z}_{p}$。</p>
<p><strong>定理 9.10.</strong> 群上的同构形成了所有群的类型的等价关系。</p>
<h3 id="凯利定理-Cayley’s-Theorem"><a href="#凯利定理-Cayley’s-Theorem" class="headerlink" title="凯利定理 Cayley’s Theorem"></a>凯利定理 Cayley’s Theorem</h3><p><strong>定理 9.12. Cayley</strong> 每个群都同构于一个置换群。</p>
<h2 id="外直积-External-Direct-Products"><a href="#外直积-External-Direct-Products" class="headerlink" title="外直积 External Direct Products"></a>外直积 External Direct Products</h2><p><strong>命题 9.13.</strong> $(G, \cdot), (H, \circ)$ 是两个群，$(g, h) \in G \times H, g \in G, h \in H$, 定义 $G \times H$ 上的二元运算：$(g_1, h_1)(g_2, h_2) = (g_1 \cdot g_2, h_1 \circ h_2)$，是一个群。</p>
<p>群 $G \times H$ 叫做 $G$ 和 $H$ 的<strong>外直积（external direct product）</strong>。</p>
<p><strong>定理 9.17.</strong> $(g, h) \in G \times H$，$g, h$ 都有有限的序数 $r, s$，则 $(g,h)$ 的序数是 $r$ 和 $s$ 的最小公倍数。</p>
<p><strong>定理 9.21.</strong> $\mathbb{Z}_m \times \mathbb{Z}_n \cong \mathbb{Z}_{mn} \Leftrightarrow gcd(m,n) = 1$</p>
<h2 id="内直积-Internal-Direct-Products"><a href="#内直积-Internal-Direct-Products" class="headerlink" title="内直积 Internal Direct Products"></a>内直积 Internal Direct Products</h2><p>如果群 $G$ 的两个子群 $H$ 和 $K$ 满足以下条件：</p>
<ul>
<li>$ G= HK = \{ hk: h \in H, k \in K \}$</li>
<li>$H \cap K = \{ e \}$</li>
<li>$\forall k \in K, \forall h \in H, hk = kh$</li>
</ul>
<p>则 $G$ 叫做 $H$ 和 $K$ 的<strong>内直积（internal direct product）</strong>。</p>
<p><strong>定理 9.27.</strong> $G$ 是 $H$ 和 $K$ 的内直积 $\Rightarrow G \cong H \times K$。</p>
<hr>
<h1 id="正规子群和商群"><a href="#正规子群和商群" class="headerlink" title="正规子群和商群"></a>正规子群和商群</h1><h2 id="正规子群-Normal-Subgroups"><a href="#正规子群-Normal-Subgroups" class="headerlink" title="正规子群 Normal Subgroups"></a>正规子群 Normal Subgroups</h2><p>群 $G$ 的一个子群 $H$ 是<strong>正规的（normal）</strong>，如果 $\forall g \in G, gH = Hg$</p>
<p><strong>定理 10.3.</strong> $N$ 为群 $G$ 的一个子群，则以下命题等价：</p>
<ol>
<li>$N$ 是正规的。</li>
<li>$\forall g \in G, g N g^{-1} \subseteq N$</li>
<li>$\forall g \in G, g N g^{-1} = N$</li>
</ol>
<h2 id="商群-Factor-Groups"><a href="#商群-Factor-Groups" class="headerlink" title="商群 Factor Groups"></a>商群 Factor Groups</h2><p>$N$ 为群 $G$ 的一个子群，则 $N$ 的所有陪集形成了一个群 $G/N$，二元运算为 $(aN)(bN) = abN$。这个群叫做 $G$ 和 $N$ 的<strong>因子（factor）</strong>或<strong>商群（quotient group）</strong>。</p>
<p><strong>定理 10.4.</strong> $N$ 为群 $G$ 的一个子群，则 $N$ 的所有陪集形成了一个 $[G:N]$ 阶群 $G/N$。</p>
<h2 id="对换群的simplicity"><a href="#对换群的simplicity" class="headerlink" title="对换群的simplicity"></a>对换群的simplicity</h2><p>没有非平凡正规子群的群叫做<strong>（simple group）</strong>。</p>
<p><strong>引理 10.8.</strong> 对换群 $A_n$ 由三元环（3-cycles）生成。</p>
<p><strong>引理 10.9.</strong> $N$ 为群 $A_n$ 的一个子群。如果 $N$ 包含一个三元环，则 $N = A_n$。</p>
<p><strong>引理 10.10.</strong> 对于 $n \geq 5$，$A_n$ 的每个非平凡正规子群 $N$ 包含一个三元环。</p>
<p><strong>定理 10.11.</strong> 置换群 $A_n(n \geq 5)$ 是simple的。</p>
<hr>
<h1 id="同态"><a href="#同态" class="headerlink" title="同态"></a>同态</h1><h2 id="群同态-Group-Homomorphisms"><a href="#群同态-Group-Homomorphisms" class="headerlink" title="群同态 Group Homomorphisms"></a>群同态 Group Homomorphisms</h2><p>群 $(G, \cdot )$ 和 $(H, \circ)$ 间的<strong>同态（homomorphisms）</strong>是一个映射（不一定双射） $\phi: G \rightarrow H$ 且满足</p>
<script type="math/tex; mode=display">\phi(a \cdot b) = \phi(a) \circ \phi(b)</script><p>$\phi$ 在 $H$ 中的值域叫做<strong>同态像（homomorphic image）</strong>。</p>
<p><strong>命题 11.4.</strong> $\phi: G_1 \rightarrow G_2$ 是一个同态，则：</p>
<ol>
<li>$e$ 是 $G_1$ 的单位元 $\Rightarrow \phi(e) $ 是 $G_2$ 的单位元。</li>
<li>$\forall g \in G_1, \phi(g^{-1}) = [\phi(g)]^{-1}$</li>
<li>$H_1$ 是 $G_1$ 的子群 $\Rightarrow \phi(H_1)$ 是 $G_2$ 的子群。</li>
<li>$H_2$ 是 $G_2$ 的子群 $\Rightarrow \phi^{-1}(H_2) = \{ g \in G: \phi(g) \in H_2 \}$ 是 $G_1$ 的子群。进一步，$H_2$ 是 $G_2$ 的正规子群 $\Rightarrow \phi^{-1}(H_2)$ 是 $G_1$ 的正规子群。</li>
</ol>
<p>$\phi : G \Rightarrow H$ 是一个同态，$\phi^{-1}(\{ e \})$ 是 $G$ 的一个子集，叫做 $\phi$ 的<strong>核（kernel）</strong>，记作 $ker~\phi$。</p>
<p><strong>定理 11.5.</strong> $\phi :G \to H$ 是一个同态，则 $\phi$ 的核是 $G$ 的一个正规子群。</p>
<h2 id="同构定理-Isomorphism-Theorems"><a href="#同构定理-Isomorphism-Theorems" class="headerlink" title="同构定理 Isomorphism Theorems"></a>同构定理 Isomorphism Theorems</h2><p>$H$ 为群 $G$ 的一个正规子群，定义<strong>自然同态（natural homomorphism）</strong> $\phi: G \rightarrow G / H, \phi(g) = gH$。</p>
<p><strong>定理 11.10. 同构第一定理</strong> $\psi: G \rightarrow H$ 是一个群同态，核为 $K = ker~\psi$ 是一个正规子群。$\phi:G \rightarrow G /K$ 为自然同态，则存在一个唯一同构 $\eta: G / K \rightarrow \psi(G)$，使得 $\psi = \eta \phi$。</p>
<p><strong>定理 11.12. 同构第二定理</strong> $H$ 是群 $G$ 的子群（不一定是正规的），$N$ 是 $G$ 的正规子群，则 $HN$ 是 $G$ 的子群，$H \cap N$ 是 $H$ 的正规子群，且</p>
<script type="math/tex; mode=display">H / (H \cap N) \cong (HN) / N</script><p><strong>定理 11.13 一致定理</strong> $N$ 是群 $G$ 的正规子群，则 $H \rightarrow H /N$ 是一个从 $G$ 的包含 $N$ 的子群的集合到 $G /N$ 的子群的集合的满射。进一步，从 $G$ 的包含 $N$ 的<strong>正规</strong>子群的集合到 $G /N$ 的子群的集合是双射。</p>
<p><strong>定理 11.14. 同构第三定理</strong> $G$ 是一个群，$N$ 和 $H$ 是正规子群且有 $N \subset H$，则 $G / H \cong \dfrac{GN}{HN}$.</p>
]]></content>
      <categories>
        <category>课程</category>
        <category>problem-solving</category>
      </categories>
  </entry>
  <entry>
    <title>问题求解笔记-群论初步</title>
    <url>/2021/02/26/group/</url>
    <content><![CDATA[<h1 id="群-Group"><a href="#群-Group" class="headerlink" title="群 Group"></a>群 Group</h1><h2 id="整数等价类-Integer-Equivalence"><a href="#整数等价类-Integer-Equivalence" class="headerlink" title="整数等价类 Integer Equivalence"></a>整数等价类 Integer Equivalence</h2><p>模运算下整数等价类的性质:</p>
<ol>
<li>加法乘法交换律</li>
<li>加法乘法结合律</li>
<li>加法(0)乘法(1)单位元</li>
<li>乘法分配律</li>
<li>任意元素存在加法逆元</li>
<li>$a$为非零整数，$gcd(a,n) = 1 \Leftrightarrow a$存在乘法逆元</li>
</ol>
<a id="more"></a>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><ul>
<li>集合$G$上的一个<strong>二元运算(binary operation)</strong>或者一个<strong>合成律(law of composition)</strong>: 一个函数$f : G \times G \rightarrow G$, $f(a,b) = a \circ b$ or $ab$</li>
<li><strong>群</strong>$(G, \circ)$: 带有一个合成律的集合$G$, 且该合成律$(a, b) \rightarrowtail a \circ b$满足:<ul>
<li><strong>结合律(associative)</strong>: $\forall a, b, c \in G, (a \circ b) \circ c = a \circ (b \circ c)$</li>
<li><strong>单位元(identity element)</strong>: $\exists e \in G, \forall a \in G, a \circ e = e \circ a = a$</li>
<li><strong>逆元(inverse element)</strong>: $\forall a \in G, \exists a^{-1}\in G, a \circ a^{-1} = a^{-1}\circ a = e$</li>
</ul>
</li>
<li><strong>阿贝尔群(abelian)</strong>: 群$G$, $\forall  a, b \in G, a\circ b = b \circ a$</li>
<li><strong>凯利表(Cayley table)</strong>或<strong>交换群(commutative)</strong>: 通过加法或乘法来描述一个群的表格。</li>
<li><strong>可逆元素群(group of units)</strong>: 拥有逆元的元素（即所有和 $n$ 互质的数）组成的乘法群</li>
<li>一般线性群(general linear group)</li>
<li>四元群(quaternion group)</li>
<li>群是有限的(finite)，或者说有有限序数(has finite order)，当它具有有限个元素，否则是无限的(infinite)或有无限序数(infinite order)</li>
</ul>
<h2 id="群的基本性质"><a href="#群的基本性质" class="headerlink" title="群的基本性质"></a>群的基本性质</h2><p><strong>命题3.17.</strong> 单位元唯一<br><strong>命题3.18.</strong> 逆元唯一<br><strong>命题3.19.</strong> $\forall a, b \in G, (ab)^{-1} = b^{-1}a^{-1}$</p>
<p><strong>命题3.20.</strong> $\forall a \in G, (a^{-1})^{-1} = a$</p>
<p><strong>命题3.21.</strong> $\forall a, b \in G,$ 方程$ax=b$和$xa=b$有唯一解</p>
<p><strong>命题3.22.</strong> $\forall a, b,bc \in G, ab=ac \Rightarrow b=c$, $ba=ca  \Rightarrow b= c$（左右约分）</p>
<p><strong>定理3.23.</strong> 基本指数运算都成立，$\forall g, h \in G$,</p>
<ol>
<li>$g^{m}g^{n} = g ^{m+n}$</li>
<li>$(g^{m})^{n} = g^{mn}$</li>
<li>$(gh)^{n} = (h^{-1}g^{-1})^{-n}$. $G$ is an abelian $\Rightarrow (gh)^{n} = g^{n}h^{n}$</li>
</ol>
<h2 id="子群-Subgroups"><a href="#子群-Subgroups" class="headerlink" title="子群 Subgroups"></a>子群 Subgroups</h2><ul>
<li>子群是父群的子集，也是个群</li>
<li><strong>平凡子群(trivival subgroup)</strong>: $H = \{ e \}$</li>
<li>真子群</li>
</ul>
<p><strong>定理3.30.</strong> $H$是$G$的子群$\Leftrightarrow$</p>
<ol>
<li>$G$的单位元$e \in H$</li>
<li>$\forall h_{1}, h_{2} \in H, h_{1}h_{2} \in H$</li>
<li>$\forall h \in H,  h^{-1} \in H$</li>
</ol>
<p><strong>定理3.31.</strong> $H \subset G$是$G$的子群$\Leftrightarrow H \neq \emptyset \wedge \forall g,h\in H, gh^{-1} \in H$</p>
<hr>
<h1 id="循环群-Cyclic-Groups"><a href="#循环群-Cyclic-Groups" class="headerlink" title="循环群 Cyclic Groups"></a>循环群 Cyclic Groups</h1><p><strong>定理4.3.</strong>  Let $G$ be a group and $a$ be any element in $G$. Then the set $\langle a\rangle =\{a^{k}:k\in\mathbb{Z}\}$ is a subgroup of $G$. Furthermore, $⟨a⟩$ is the smallest subgroup of $G$ that contains $a$.</p>
<ul>
<li><strong>循环子群(Cyclic Subgroups)</strong>: $\langle a \rangle$</li>
<li><strong>循环群</strong>: 包含了某个元素$a$，使得$G=⟨a⟩$.此时$⟨a⟩$是$G$的<strong>生成元(generator)</strong>.</li>
<li>$a$的<strong>序数(order)</strong>: 最小的正整数$n$, s.t. $a^{n} = e$, 记作$|a| = n$. 如果不存在，则为正无穷。</li>
</ul>
<p><strong>定理4.9.</strong> 所有循环群都是阿贝尔群。</p>
<h2 id="循环群的子群"><a href="#循环群的子群" class="headerlink" title="循环群的子群"></a>循环群的子群</h2><p><strong>定理4.10.</strong> 循环群的子群也都是循环群。<br><strong>推论4.11.</strong> $\mathbb{Z}$的所有子群就是$n\mathbb{Z}, n = 0,1,2,…$</p>
<p><strong>命题4.12.</strong> $G$为一个$n$阶循环群，$a$是$G$的一个生成元。则$a^{k} = e \Leftrightarrow n | k$（$k$是$n$的倍数）。</p>
<p><strong>定理4.13.</strong> $G$为一个$n$阶循环群，$a$是$G$的一个生成元。$b = a^{k} \Rightarrow b$的序数为$n / d, d = gcd(k,n)$。</p>
<p><strong>推论4.14.</strong> $\mathbb{Z}_{n}$的所有生成元$r$满足$1 \leq r &lt; n \wedge gcd(n,r) = 1$。</p>
<h2 id="复数乘法群-Multiplicative-Group-of-Complex-Numbers"><a href="#复数乘法群-Multiplicative-Group-of-Complex-Numbers" class="headerlink" title="复数乘法群  Multiplicative Group of Complex Numbers"></a>复数乘法群  Multiplicative Group of Complex Numbers</h2><p>复数知识</p>
<ul>
<li>$\cos \theta + i \sin \theta$ 记作 $cis\theta$<br><strong>定理2.22. DeMoivre</strong> $(r~cis\theta)^{n} = r^{n}~cis(n\theta)$</li>
</ul>
<h2 id="圆群-Circle-Group"><a href="#圆群-Circle-Group" class="headerlink" title="圆群 Circle Group"></a>圆群 Circle Group</h2><p>$\mathbb{T}: \{ z \in \mathbb{C}: |z| = 1 \}$<br><strong>命题4.24.</strong> 圆群是$\mathbb{C}^{*}$的一个子群。</p>
<p>满足$z^{n} = 1$的复数$z$称为$n$次<strong>单位根($n$-th roots of unity)</strong>。<br><strong>定理4.25.</strong> $z^{n} = 1 \Rightarrow z = cis(\dfrac{2k\pi}{n}), k = 0, 1, … , n-1$. $n$次单位根构成$\mathbb{T}$的一个圆子群。</p>
<h2 id="重复平方法-The-Method-of-Repeated-Squares"><a href="#重复平方法-The-Method-of-Repeated-Squares" class="headerlink" title="重复平方法 The Method of Repeated Squares"></a>重复平方法 The Method of Repeated Squares</h2><p>快速幂。</p>
]]></content>
      <categories>
        <category>课程</category>
        <category>problem-solving</category>
      </categories>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2021/02/15/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="中文"><a href="#中文" class="headerlink" title="中文"></a>中文</h2><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>问题求解笔记-启发式算法</title>
    <url>/2021/06/06/heuristics/</url>
    <content><![CDATA[<hr>
<h1 id="模拟退火算法（Simulated-Annealing）"><a href="#模拟退火算法（Simulated-Annealing）" class="headerlink" title="模拟退火算法（Simulated Annealing）"></a>模拟退火算法（Simulated Annealing）</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul>
<li><p>$U = (\Sigma_I, \Sigma_O, L, L_I, \mathcal{M}, cost, goal)$ 是一个优化问题，对任意的 $x \in L_I$，$\mathcal{M}(x)$ 的一个<strong>邻域（Neighborhood）</strong>是一个映射 $f_x: \mathcal{M}(x) \to Pot(\mathcal{M(x)})$ 满足：</p>
<ul>
<li><p>$\forall \alpha \in \mathcal{M}(x), \alpha \in f_x(\alpha)$</p>
</li>
<li><p>如果 $\exists \alpha \in \mathcal{M}(x), \beta\in f_x(\alpha)$，则 $\alpha \in f_x(\beta)$</p>
</li>
<li><p>对任意的 $\alpha, \beta \in \mathcal{M}(x)$，所在一个正整数 $k$ 和 $\gamma_1, …, \gamma_k \in \mathcal{M}(x)$，使得 </p>
<p>$\gamma_1 \in f_x(\alpha), \gamma_{i  +1 } \in f_x(\gamma_i), i = 1, … k-1$ 且 $\beta \in f_x(\gamma_k)$​</p>
</li>
</ul>
</li>
<li>$LSS(Neigh)$</li>
</ul>
<a id="more"></a>
<p>模拟退火算法是从一个物理退火模型（Metropolos Algorithm）类比启发启发得到的，和 $LSS$ 的区别是有概率允许状态向恶化的方向转化，来防止局部最优解。</p>
<p>一些概念的类比：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">物理退火</th>
<th style="text-align:center">模拟退火</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">系统状态</td>
<td style="text-align:center">可行解集合</td>
</tr>
<tr>
<td style="text-align:center">状态能量</td>
<td style="text-align:center">可行解的代价</td>
</tr>
<tr>
<td style="text-align:center">扰动机制</td>
<td style="text-align:center">从邻居的随机选择</td>
</tr>
<tr>
<td style="text-align:center">一个最优状态</td>
<td style="text-align:center">一个最优可行解</td>
</tr>
<tr>
<td style="text-align:center">温度</td>
<td style="text-align:center">控制参数</td>
</tr>
</tbody>
</table>
</div>
<p>所以模拟退火其实就是建立在类比 Metropolos Algorithm上，得到的一个局部搜索算法。</p>
<p>对于一个优化问题的固定的邻域，模拟退火可以被描述为：</p>
<p><img data-src="SA.png" alt title="模拟退火算法"></p>
<h2 id="理论和经验"><a href="#理论和经验" class="headerlink" title="理论和经验"></a>理论和经验</h2><p><strong>定理 6.2.2.1.</strong>  $U$ 是一个最小化问题且让 $Neigh$ 是 $U$ 的邻域，对于输入 $x$ 模拟退火算法的渐进收敛（asymptotic convergence）被保证如果以下两个条件被满足：</p>
<ul>
<li>每个 $\mathcal{M}(x)$ 的可行解是可到达的，从每个其他的 $\mathcal{M}(x)$ 的可行解。（邻域的定义条件III）</li>
<li>初始条件 $T$ 至少是最深的局部最小值。</li>
</ul>
<p><em>渐进收敛意味着随着迭代次数的增长，到达全局最优的概率趋向于1</em>。</p>
<p><em>就是说模拟退火算法会 find itself at a  global optimum an increasing percentage of the time as time grows.</em>（我也没读懂）</p>
<p>一般还有要求：</p>
<ul>
<li>邻域的对称性（邻域定义的条件II）</li>
<li><p>邻域的均匀性，$\forall \alpha, \beta \in \mathcal{M}(x),|Neigh_x (\alpha)| = |Neigh_x(\beta)|$</p>
</li>
<li><p>通过特定的 <em>cool schedules</em>，$T$ 增长缓慢</p>
</li>
</ul>
<p>很遗憾在限定的迭代次数内，模拟退火并不能确保可行解的高质量，一般需要解空间的平方大小的次数的迭代。所以相比运行模拟退火到确保一个正确的近似解，还不如直接搜索全空间。。。</p>
<p>模拟退火的收敛率是和迭代次数成对数关系。</p>
<p>应该尽可能地避免以下结构的邻域：</p>
<ul>
<li>毛刺结构</li>
<li>深沟</li>
<li>大高原</li>
</ul>
<p><em>因为模拟退火第一部分是在高温下完成的，使得所有恶化方向都有可能，所以可以视为对初始解的随机搜索。然后当 $T$ 很小，大的恶化基本上是不可能的。</em></p>
<p>如果谈到 cool schedules，以下的参数一定要确定：</p>
<ul>
<li>初始温度 $T$</li>
<li>温度缩减函数 $f(T, t)$</li>
<li>终止条件</li>
</ul>
<h3 id="初始温度的选择"><a href="#初始温度的选择" class="headerlink" title="初始温度的选择"></a>初始温度的选择</h3><p>需要很大。</p>
<p>一种方法是取任意两个相邻解的最大差值，比较难算，所以只要高效地找到一个估计值（上界）就足够了。</p>
<p>另一种实用的方法是，从任意 $T$ 值开始，在选择了初始情况 $\alpha$ 的邻居 $\beta$ 后，以一种使 $\beta$ 以靠近1的概率被接受的方式增加 $T$。进行若干次迭代，可以得到一个合理的初值 $T$。<em>可以看作物理类比的加热过程</em></p>
<h3 id="选择温度缩减函数（Temperature-Reduction-Function）"><a href="#选择温度缩减函数（Temperature-Reduction-Function）" class="headerlink" title="选择温度缩减函数（Temperature Reduction Function）"></a>选择温度缩减函数（Temperature Reduction Function）</h3><p>一个一般的方式是 $T$ 乘以一个系数 $0.8\leq r \leq 0.99$，工作常数 $d$ 步得到 $T := r \cdot T$。</p>
<p>也就是缩减 $k$ 次得到的温度 $T_k$ 为 $r_k\cdot T$。</p>
<p>另一个选择是 $T_k := \dfrac{T}{\log_2{(k  +2)}}$，常数 $k$ 一般为邻域的大小。</p>
<h3 id="终止条件"><a href="#终止条件" class="headerlink" title="终止条件"></a>终止条件</h3><p>一个方法是终止条件独立于 $T$，当解的代价（cost）不变时终止。</p>
<p>另一种是设一个常数 $term$，当 $T \leq term$ 时终止。</p>
<h3 id="一般性质"><a href="#一般性质" class="headerlink" title="一般性质"></a>一般性质</h3><ul>
<li>模拟退火算法能够得到一个高质量解，但需要大开销的运算。</li>
<li>最终解的质量和初始解的选择关系不大。</li>
<li>重要的两个参数是缩减率和邻域，有必要做一定的工作来调参。</li>
<li>平均复杂度和最差复杂度很接近。</li>
<li>在相同的邻域下，模拟退火一般比局部搜索或多源局部搜索表现得更好。</li>
</ul>
<h3 id="一些问题的应用"><a href="#一些问题的应用" class="headerlink" title="一些问题的应用"></a>一些问题的应用</h3><ul>
<li>图位置算法（最大最小割）一般表现较好。</li>
<li>工程问题（VLSI）是最好的算法。</li>
<li>TSP等表现不太好。</li>
</ul>
<h2 id="随机禁忌？搜索（Randomized-Tabu-Search）"><a href="#随机禁忌？搜索（Randomized-Tabu-Search）" class="headerlink" title="随机禁忌？搜索（Randomized Tabu Search）"></a>随机禁忌？搜索（Randomized Tabu Search）</h2><p>思想就是：保存一些之前的可行解的信息，并在决定下一个可行解时用上。</p>
<p><img data-src="RTS.png" alt></p>
<hr>
<h1 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h1><h2 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h2><p>遗传算法是从个体种群的进化优化启发而来，进化可以被概括为以下过程：</p>
<ol>
<li>一个个体可以用一个字符串来表示（DNA序列）。</li>
<li><strong>交配（crossover）</strong>，也就是交换两个父母字符串的一部分，获得子串。</li>
<li>存在一个<strong>适应值（fitness value）</strong>，适应值高的串以更高的概率被选择。</li>
<li><strong>变异（mutation）</strong>，随机变一位。</li>
<li>去世。</li>
</ol>
<p>一些概念的类比：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">种群遗传进化</th>
<th style="text-align:center">遗传算法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">一个个体</td>
<td style="text-align:center">一个可行解</td>
</tr>
<tr>
<td style="text-align:center">一个基因</td>
<td style="text-align:center">解中的一位</td>
</tr>
<tr>
<td style="text-align:center">适应值</td>
<td style="text-align:center">代价函数</td>
</tr>
<tr>
<td style="text-align:center">种群</td>
<td style="text-align:center">可行解的一个子集</td>
</tr>
<tr>
<td style="text-align:center">变异</td>
<td style="text-align:center">随机局部转化</td>
</tr>
</tbody>
</table>
</div>
<p>遗传算法和模拟退火有两个区别，一是用一组可行解而不是一个可行解，二是不能被视为纯粹的局部搜索。</p>
<p>在一个合适的可行解表示下，遗传算法可以被描述为：</p>
<p><img data-src="GAS.png" alt title="GAS"></p>
<p>如果不考虑交配，只考虑变异，那么遗传算法和模拟退火很类似。</p>
<ul>
<li><p>$U = (\Sigma_I, \Sigma_O, L, L_I, \mathcal{M}, cost, goal)$ 是一个优化问题，对一个给定大小为 $n$ 的输入 $x \in L_I$，每个 $\alpha \in \{0, 1\}^n$ 代表 $x$ 的一个可行解，即 $\mathcal{M}(x) = \{0, 1\}^n$。$\mathcal{M}(x)$ 的一个 <strong>模式（schema）</strong> 是一个向量 $s = (s_1, s_2, …, s_n) \in \{0, 1, \star\}^n$。$s$ 的一组可行解是</p>
<script type="math/tex; mode=display">
Schema(s_1, s_2, ..., s_n) = \{\gamma_1, \gamma_2, ..., \gamma_n \in \mathcal{x} | \gamma_i = s_i , s_i \in \{0,1\} ~or~ \gamma_i \in \{0,1\}, s_i = \star \}</script></li>
<li><p>模式 $s$ 的长度 $length(s)$ 是第一位到最后一个非 $\star$ 位的距离。阶 $order(s)$ 是非 $\star$ 的位数。</p>
</li>
<li><p>在一个种群 $P$ 中的<strong>适应度（Fitness）</strong> 是 $Schema(s)$ 的可行解的平均适应度：</p>
<script type="math/tex; mode=display">
Fitness(s, P) = \dfrac{1}{|Schema(s) \cap P|} \cdot \sum\limits_{\gamma \in Schema(s) \cap P} cost(\gamma).</script></li>
<li><p>在一个种群 $P$ 中的<strong>适应率（Fitness ratio）</strong>是：</p>
<script type="math/tex; mode=display">
Fit\text{-}ratio = \dfrac{Fitness(s, P)}{\frac{1}{|P|} \sum_{\beta \in P}cost(\beta)}</script></li>
<li><p>$Aver\text{-}Fit(P) = \frac{1}{|P|}\sum_{\beta\in P} cost(\beta)$ 叫做种群 $P$ 的平均适应度。</p>
</li>
</ul>
<p><em>其实模式（schema）就是在可行解的表达中固定一些基因的值，其中 $\star$ 位就是自由变量，所以 $|Schema(s)|$ 就是$\star$的数量的2次幂。一个有比种群平均适应度高很多的适应度的模式就是一个很好的基因，应该被在进化中被遗传。</em></p>
<ul>
<li>$P_0$ 为初始种群，$P_t$ 为第 $t$ 次迭代基因算法后的种群，$X_{t + 1}(s)$ 是一个随机变量，是从 $P_t$ 中随机选择的父母且在 $Schema(s)$ 的数量。选择 $\alpha \in P_t$ 的概率是：$Pr_{par}(\alpha) = \dfrac{cost(\alpha)}{\sum_{\beta\in P_t} cost(\beta)} = \dfrac{cost(\alpha)}{|P_t| \cdot Aver\text{-}Fit(P_t)}$</li>
</ul>
<p><strong>引理 6.3.1.2.</strong> 对任意的 $t \in \mathbb{N}$ 和任意模式 $s$，$E[X_{t + 1}(s)] = Fit\text{-}ratio(s, P_t) \cdot |P_t \cap Schema(s)|$。</p>
<p><strong>引理 6.3.1.3.</strong> 对任意的模式 $s$ 和任意 $t = 1, 2,…$，</p>
<script type="math/tex; mode=display">
E[Y_{t+1}(s)] \geq \dfrac{|P_t|}{2} \cdot \left[2 \cdot \left( \dfrac{E[X_{t+1}(s)]}{|P_t|}\right)^2 + 2 \cdot \dfrac{n - length(s)}{n} \cdot\dfrac{E[X_{t+1}(s)]}{|P_t|} \cdot \left( 1 - \dfrac{E[X_{t+1}(s)]}{|P_t|} \right) \right].</script><p><strong>引理 6.3.1.4.</strong> 对任意的模式 $s$ 和任意 $t = 1, 2,…$，</p>
<script type="math/tex; mode=display">
E[Y_{t + 1}(s)] \geq \dfrac{n - length(s)}{n} \cdot E[X_{t+1}(s)].</script><p><strong>引理 6.3.1.5.</strong> 对任意的模式 $s$ 和任意 $t = 1, 2,…$，</p>
<script type="math/tex; mode=display">
E[Z_{t + 1}(s)] \geq (1 - pr_{m})^{order(s)} \cdot E[Y_{t+1}(s)] \geq (1 - order(s) \cdot pr_m) \cdot E[Y_{t+1}(s)].</script><p><strong>定理 6.3.1.6.（GAS的模式定理）</strong></p>
<p>对任意的模式 $s$ 和任意 $t = 1, 2,…$，在第 $(t+1)-st$ 的种群 $P_{t+1}$ 中的为模式 $Schema(s)$ 的个体的数量的期望为</p>
<script type="math/tex; mode=display">
E[Z_{t+1}] \geq Fit\text{-}ratio(s, P_t) \cdot \dfrac{n-  length(s)}{n} \cdot (1 - order(s)\cdot pr_m) \cdot |P_t \cdot Schema(s)|.</script><h2 id="参数调整"><a href="#参数调整" class="headerlink" title="参数调整"></a>参数调整</h2><h3 id="种群大小"><a href="#种群大小" class="headerlink" title="种群大小"></a>种群大小</h3><ul>
<li>小种群容易导致局部最优解。</li>
<li>反之大种群更容易到达全局最优解。</li>
</ul>
<h3 id="初始种群的选择"><a href="#初始种群的选择" class="headerlink" title="初始种群的选择"></a>初始种群的选择</h3><ul>
<li>常用的是随机选择。</li>
<li>将一些预处理的高质量可行解引入初始种群可以加快收敛速度。</li>
<li>最好是结合起来，部分随机部分高适应度。</li>
</ul>
<h3 id="适应度评估和父母选择机制"><a href="#适应度评估和父母选择机制" class="headerlink" title="适应度评估和父母选择机制"></a>适应度评估和父母选择机制</h3><ul>
<li>适应度 $\alpha$ 直接用 $cost(\alpha)$，然后按概率分布 $p(\alpha) = \dfrac{cost(\alpha)}{\sum_{\beta \in P} cost(\beta)}$ 选择父母。</li>
<li>当 $max(P)$ 和 $min(P)$ 很接近时，$P$ 几乎均匀分布，一般选择 $fitness(\alpha) = cost(\alpha) - C, C &lt; min(P)$</li>
<li>选择 $k / 2$ 个最好的个体，再随机选择 $k/2$ 个个体进行配偶。</li>
<li>通过<strong>秩（ranking）</strong>来随机选择，按代价排序后 $cost(\alpha_1) \leq cost(\alpha_2) \leq …\leq cost(\alpha_n)$，然后分配概率 $prob(\alpha_i) = \dfrac{2i}{n(n+1)}$ 选择父母。</li>
</ul>
<h3 id="个体和交配的形式表示"><a href="#个体和交配的形式表示" class="headerlink" title="个体和交配的形式表示"></a>个体和交配的形式表示</h3><ul>
<li>允许不代表任何可行解的表示存在，但可能降低复杂度。</li>
<li>不直接在父母的表示上进行交配，而是通过一些特点参数，寻找子代的交叉混合属性。</li>
<li>在上面所有的情况，交配都是只考虑一个交配位置的简单交配，可以与更复杂的交配的概念结合起来。</li>
</ul>
<h3 id="变异的可能性"><a href="#变异的可能性" class="headerlink" title="变异的可能性"></a>变异的可能性</h3><ul>
<li>通常改变一位基因的概率调整为小于 $\frac{1}{100}$。</li>
<li>有时候取 $\frac{1}{n}$ 或 $\frac{1}{k^{0.93}\sqrt{n}}$，$n$ 是基因数量，$k$ 是种群规模。</li>
<li>有时候在计算过程中动态改变概率，如果种群中个体过于相似，增加突变概率，防止局部最优。</li>
</ul>
<h3 id="新种群的选择机制"><a href="#新种群的选择机制" class="headerlink" title="新种群的选择机制"></a>新种群的选择机制</h3><ul>
<li>子代完全取代父代，<strong>en block</strong> 策略。</li>
<li>从子代和父代中同时选择，需要调整子代对父代的偏爱程度。一种常见的是从父代中挑选一些适应度最高的，种群中绝大多数是子代。</li>
<li>在选择了新的种群后，可以立即开始交配生产新种群，也可以提高种群平均适应度，可以选择小邻域进行局部搜索或模拟退火，得到一个局部最优的个体。</li>
</ul>
<h3 id="终止标准"><a href="#终止标准" class="headerlink" title="终止标准"></a>终止标准</h3><ul>
<li>在一开始确定时间限制。</li>
<li>测量种群的平均适应度和个体之间的差异，如果平均适应度没有本质改变，或个体非常相似，遗传可以停止。</li>
</ul>
]]></content>
      <categories>
        <category>课程</category>
        <category>problem-solving</category>
      </categories>
  </entry>
  <entry>
    <title>计算机网络课程笔记-第一章计算机网络与因特网</title>
    <url>/2021/09/06/internet1/</url>
    <content><![CDATA[<hr>
<h1 id="什么是因特网-Internet"><a href="#什么是因特网-Internet" class="headerlink" title="什么是因特网 Internet"></a>什么是因特网 Internet</h1><a id="more"></a>
<h2 id="具体构成"><a href="#具体构成" class="headerlink" title="具体构成"></a>具体构成</h2><ul>
<li><p><strong>主机（host）/ 端系统（end system）</strong>：与因特网相连的设备。</p>
</li>
<li><p>端系统通过<strong>通信链路（communication link）</strong>和<strong>分组交换机（packet switch）</strong>连接到一起。</p>
<ul>
<li>通信链路由不同类型的物理媒体组成，有<strong>传输速率（transmission rate）</strong>。</li>
<li>分组交换机包括<strong>路由器（router）</strong>和<strong>链路层交换机（link-layer switch）</strong>。</li>
</ul>
</li>
<li><p>端系统通过<strong>因特网服务提供商（Internet Service Provide, ISP）</strong>接入因特网。</p>
</li>
<li>端系统、分组交换机和其他部件都要运行一系列<strong>协议（protocol）</strong>，协议控制因特网中信息的接收和发送。<ul>
<li><strong>TCP（Transmission Control Protocol）</strong>：传输控制协议。</li>
<li><strong>IP（Internet Protocol）</strong>：网络协议。定义了在路由器和端系统之间发送和接收的分组格式。</li>
</ul>
</li>
<li><strong>新特网标准（Internet standard）</strong> 由 IETE 研发。</li>
</ul>
<h2 id="服务描述"><a href="#服务描述" class="headerlink" title="服务描述"></a>服务描述</h2><ul>
<li>分布式应用程序。</li>
<li>端系统提供<strong>套接字接口（socket interface）</strong>，规定了端系统上的程序, 请求因特网基础设施, 向另一个端系统上程序, 交付数据的方式。</li>
</ul>
<h2 id="协议"><a href="#协议" class="headerlink" title="协议"></a>协议</h2><p><strong>协议（protocol）</strong>定义了在两个或多个通信实体之间交换的报文的格式和顺序，以及报文发送和接收一条报文或其他事件所采取的动作。</p>
<hr>
<h1 id="网络边缘-Edge"><a href="#网络边缘-Edge" class="headerlink" title="网络边缘 Edge"></a>网络边缘 Edge</h1><p>端系统也被称为<strong>主机（host）</strong>，容纳运行应用程序。</p>
<p>主机被划为<strong>客户（client）</strong>和<strong>服务器（server）</strong>。</p>
<h2 id="接入网-Access"><a href="#接入网-Access" class="headerlink" title="接入网 Access"></a>接入网 Access</h2><p>指将端系统物理连接到其<strong>边缘路由器（edge router）</strong>。</p>
<ul>
<li>家庭接入<ul>
<li>DSL</li>
<li>电缆</li>
<li>FTTH</li>
<li>拨号</li>
<li>卫星</li>
</ul>
</li>
<li>企业接入<ul>
<li>以太网（最为流行）</li>
<li>WIFI</li>
</ul>
</li>
<li>广域无线接入<ul>
<li>3/4/5G</li>
<li>LTE</li>
</ul>
</li>
</ul>
<h2 id="物理媒体"><a href="#物理媒体" class="headerlink" title="物理媒体"></a>物理媒体</h2><ul>
<li>导向型<ul>
<li>双绞铜线</li>
<li>同轴电缆(可用作共享媒体)</li>
<li>光纤</li>
</ul>
</li>
<li>非导向型<ul>
<li>陆地无线电</li>
<li>卫星无线电</li>
</ul>
</li>
</ul>
<hr>
<h1 id="网络核心-Core"><a href="#网络核心-Core" class="headerlink" title="网络核心 Core"></a>网络核心 Core</h1><h2 id="分组交换"><a href="#分组交换" class="headerlink" title="分组交换"></a>分组交换</h2><ul>
<li>端系统彼此交换<strong>报文（message）</strong>，包含控制功能或数据等。</li>
<li>源端系统会将长报文划分为较小的数据块，叫<strong>分组（packet）</strong>。通过通信链路和分组交换机传送。</li>
<li>存储转发传输</li>
<li>排队实验和分组丢失</li>
<li>转发表和选择协议</li>
</ul>
<h2 id="电路交换"><a href="#电路交换" class="headerlink" title="电路交换"></a>电路交换</h2><ul>
<li><p>预留资源, 需要建立连接/断开连接 </p>
</li>
<li><p>端到端连接: 专用电路, 恒定时延/速率, 稳定的性能</p>
</li>
<li><p>复用（multiplexing）</p>
<ul>
<li>频分复用（FDM）</li>
<li>时分复用（TDM）</li>
</ul>
</li>
</ul>
<h2 id="网络的网络"><a href="#网络的网络" class="headerlink" title="网络的网络"></a>网络的网络</h2><ul>
<li>Tier-1/2/3</li>
<li>IXP</li>
</ul>
<hr>
<h1 id="分组交换网络中的问题"><a href="#分组交换网络中的问题" class="headerlink" title="分组交换网络中的问题"></a>分组交换网络中的问题</h1><h2 id="时延-Delay"><a href="#时延-Delay" class="headerlink" title="时延 Delay"></a>时延 Delay</h2><ul>
<li><p>处理时延: 节点 检查首部, 决定转发, 进行校验 的时间, 微秒-。</p>
</li>
<li><p>排队时延: 在链路前的队列等待传输, 微秒~毫秒。</p>
</li>
<li><p>传输时延: L/R, 将所有比特推向链路的时间, 微秒~毫秒。</p>
</li>
<li><p>传播时延: d/s, 速度s略小于光速, 毫秒。</p>
</li>
</ul>
<h2 id="丢包-Loss"><a href="#丢包-Loss" class="headerlink" title="丢包 Loss"></a>丢包 Loss</h2><ul>
<li><p>队列近满, 部分设备采取按照概率丢弃分组, 队列越长概率越大。</p>
</li>
<li><p>队列满, 再来就丢, 用丢包率衡量。</p>
</li>
</ul>
<h2 id="吞吐量-Throughput"><a href="#吞吐量-Throughput" class="headerlink" title="吞吐量 Throughput"></a>吞吐量 Throughput</h2><ul>
<li>瞬时/平均吞吐量</li>
<li>瓶颈链路: 吞吐量是各个子链路吞吐量的最小值。<ul>
<li>因特网中, 吞吐量瓶颈在接入网。</li>
</ul>
</li>
<li>共享链路: 多个链路共享某一段链路, 则需要共享这个链路的吞吐量。</li>
</ul>
<h1 id="协议层次及其服务模型"><a href="#协议层次及其服务模型" class="headerlink" title="协议层次及其服务模型"></a>协议层次及其服务模型</h1><h2 id="分层的体系结构"><a href="#分层的体系结构" class="headerlink" title="分层的体系结构"></a>分层的体系结构</h2><ul>
<li>协议分层<ul>
<li>以分层的方式组织协议以及实现这些协议的网络软硬件。</li>
<li>每个协议属于层次之一，某层向上层提供服务，就是一层<strong>服务模型（service model）</strong>。</li>
<li>各层所有协议称为协议栈。</li>
</ul>
</li>
<li>（TCP/IP）协议栈<ul>
<li>应用层<ul>
<li>支持网络应用程序: FTP(端系统文件传输), SMTP(电子邮件报文传输), HTTP(Web文档请求和传送)。</li>
</ul>
</li>
<li>运输层<ul>
<li>进程间数据传输: TCP, UDP。</li>
<li>信息分组: 报文段（segment）</li>
</ul>
</li>
<li>网络层<ul>
<li>信息分组: 数据报（datagram）</li>
<li>IP：数据报中的各个字段以及端系统和路由器如何作用于这些字段。</li>
</ul>
</li>
<li>链路层<ul>
<li>信息分组：帧</li>
</ul>
</li>
<li>物理层</li>
</ul>
</li>
<li>OSI<ul>
<li>应用层: 应用访问OSI模型的环境</li>
<li>表示层, 会话层: 并入应用层</li>
<li>运输层<ul>
<li>端系统间通信</li>
<li>可靠传输 或 单块传输</li>
<li>连接建立, 维持, 释放</li>
</ul>
</li>
<li>网络层<ul>
<li>分组在多个网络或链路上传输</li>
<li>编址, 路由, 转发, 拥塞控制</li>
<li>连接建立, 维持, 拆除</li>
</ul>
</li>
<li>数据链路层<ul>
<li>链路层帧</li>
<li>媒体访问控制, 差错检测和重传, 流量控制</li>
<li>连接激活, 维持和失活</li>
</ul>
</li>
<li>物理层</li>
</ul>
</li>
</ul>
<h2 id="网络攻击-Attack"><a href="#网络攻击-Attack" class="headerlink" title="网络攻击 Attack"></a>网络攻击 Attack</h2><ul>
<li>攻击个人电脑<ul>
<li>恶意软件<ul>
<li>大多数都是自我复制的</li>
<li>病毒（virus）: 需要某种形式的用户交互来感染用户设备</li>
<li>蠕虫（worm）: 无需任何明显用户交互就能进入设备</li>
<li>木马（horse）: 伪装成无害程序, 吸引用户点击</li>
<li>后门（backdoor）: 绕过授权验证</li>
<li>广告软件（adware）: 访问弹出广告</li>
<li>间谍软件（spyware）: 收集用户的输入, 记录用户活动</li>
</ul>
</li>
</ul>
</li>
<li>攻击服务器和网络基础设施<ul>
<li>拒绝服务攻击（Denial of Service, DOS）<ul>
<li>使得服务不能由合法用户使用</li>
<li>弱点攻击: 针对易受攻击的程序或操作系统, 引发停止运行或崩溃</li>
<li>带宽洪泛: 大量发送分组到目标, 使链路拥塞</li>
<li>连接洪泛: 创建大量半开或全开的TCP连接, 耗尽资源</li>
</ul>
</li>
<li>分布式DoS(DDoS)<ul>
<li>攻击者控制多个源</li>
<li>僵尸网路: 攻击者用恶意软件控制大量计算机, 作为DDoS的源头等</li>
</ul>
</li>
</ul>
</li>
<li>嗅探分组（IP Spoofing）<ul>
<li>分组嗅探器: 记录每个流经的分组副本的被动接收机</li>
<li>防范: 加密</li>
</ul>
</li>
<li>伪装（Masquerade）<ul>
<li>IP欺骗: 将具有虚假源地址的分组注入因特网</li>
<li>重放攻击</li>
<li>中间人攻击</li>
<li>连接劫持</li>
<li>解决方案: 加密, 数字签名, MAC</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>课程</category>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title>计算机网络课程笔记-第六章链路层和局域网</title>
    <url>/2021/09/06/internet6/</url>
    <content><![CDATA[<hr>
<h1 id="链路层-Link"><a href="#链路层-Link" class="headerlink" title="链路层 Link"></a>链路层 Link</h1><ul>
<li>运行链路层协议的任何设备均称为<strong>节点（node）</strong>，包括主机、路由器、交换机和 WiFi 接入点。</li>
<li>沿着通信路径连接的相邻节点的通信信道称为<strong>链路（link）</strong>。</li>
<li>在经过特定的链路时，传输节点将数据报封装在链路层帧中，并将该帧传输到链路中。</li>
</ul>
<a id="more"></a>
<h2 id="链路层提供的服务"><a href="#链路层提供的服务" class="headerlink" title="链路层提供的服务"></a>链路层提供的服务</h2><ul>
<li><strong>成帧（framing）</strong>。</li>
<li><p><strong>链路接入</strong>。媒体访问控制（Medium Access Control, MAC）协议。</p>
</li>
<li><p><strong>可靠交付</strong>。</p>
</li>
<li><strong>差错检测和纠正</strong>。</li>
</ul>
<h2 id="链路层实现"><a href="#链路层实现" class="headerlink" title="链路层实现"></a>链路层实现</h2><p>链路层的主体部分是在<strong>网络适配器（network adapter）</strong>中实现的，也称<strong>网络接口卡（Network Interface Card, NIC）</strong>。</p>
<hr>
<h1 id="差错检测和纠正-Error-Dectection-and-Correction"><a href="#差错检测和纠正-Error-Dectection-and-Correction" class="headerlink" title="差错检测和纠正 Error Dectection and Correction"></a>差错检测和纠正 Error Dectection and Correction</h1><ul>
<li><strong>差错检测和纠正比特（Error-Detection-and-Correction, EDC）</strong></li>
<li><strong>未检出比特差错（undetected bit error）</strong></li>
</ul>
<p>3种技术（都是学过的）：</p>
<ul>
<li>奇偶校验</li>
<li>检验和（checksum，适用于运输层）、因特网检验和</li>
<li>循环冗余检测（CRC，适用于适配器中的链路层）：多项式编码、生成多项式</li>
</ul>
<hr>
<h1 id="多路访问链路和协议"><a href="#多路访问链路和协议" class="headerlink" title="多路访问链路和协议"></a>多路访问链路和协议</h1><ul>
<li><strong>点对点链路（point-to-point link）</strong>，由链路一段发送方和另一端单个接收方组成。<ul>
<li>点对点协议（point-to-point protocol, PPP）</li>
<li>高级数据链路控制（high-level data link control, HDLC）</li>
</ul>
</li>
<li><strong>广播链路（broadcast link）</strong>，多点、共享信道。</li>
</ul>
<h2 id="信道划分协议-Channel-Partitioning-Protocol"><a href="#信道划分协议-Channel-Partitioning-Protocol" class="headerlink" title="信道划分协议 Channel Partitioning Protocol"></a>信道划分协议 Channel Partitioning Protocol</h2><h2 id="随机接入协议-Random-Access-Protocol"><a href="#随机接入协议-Random-Access-Protocol" class="headerlink" title="随机接入协议 Random Access Protocol"></a>随机接入协议 Random Access Protocol</h2><h2 id="轮流协议-Taking-turns-Protocol"><a href="#轮流协议-Taking-turns-Protocol" class="headerlink" title="轮流协议 Taking-turns Protocol"></a>轮流协议 Taking-turns Protocol</h2><hr>
<h1 id="交换局域网"><a href="#交换局域网" class="headerlink" title="交换局域网"></a>交换局域网</h1><h2 id="链路层寻址和-ARP"><a href="#链路层寻址和-ARP" class="headerlink" title="链路层寻址和 ARP"></a>链路层寻址和 ARP</h2><ul>
<li><p>MAC 地址</p>
<ul>
<li>主机和路由器不具有链路层地址</li>
<li>它们的适配器（网络接口）具有链路层地址</li>
<li>具有多个网络接口的主机具有与之相关联的多个链路层地址</li>
<li>链路层交换机并不具有与接口相关联的链路层地址（交换机交换是透明的）</li>
</ul>
</li>
<li><p><strong>地址解析协议（Address Resolution Protocol）</strong>，网络层地址（因特网中的IP地址）和链路层地址（MAC地址）进行转换。</p>
<ul>
<li>发送方构造一个称为ARP分组（ARP packet）的特殊分组，一个ARP分组有几个字段：发送、接收IP地址以及MAC地址。ARP查询分组的目的时查询子网上所有其他主机和路由器，以确定对于要解析的IP地址的MAC地址。</li>
<li>适配器封装ARP分组，用广播地址作为帧的目的地址，将帧传输进子网中。</li>
<li>目标主机给查询主机发送回一个带有所希望映射的响应ARP分组（使用标准帧而不是广播地址），然后查询主机更新ARP表，并发送IP数据报。</li>
</ul>
</li>
<li><p>发送数据报到子网以外</p>
<ul>
<li>路由器每个端口均有 MAC 和 IP。</li>
<li>路由器将相应 ARP, 主机获得的此 IP 的 MAC 地址是路由器这一端口的 MAC。</li>
</ul>
</li>
</ul>
<h2 id="以太网-Ethernet"><a href="#以太网-Ethernet" class="headerlink" title="以太网 Ethernet"></a>以太网 Ethernet</h2><ul>
<li>帧结构<ul>
<li>数据字段（46-1500字节）。承载了 IP 数据报。</li>
<li>目的地址（6字节）。包含目的适配器的 MAC 地址。</li>
<li>原地址（6字节）。包含传输该帧到局域网上的适配器的 MAC 地址。</li>
<li>类型字段（2字节）。允许以太网复用多种网络层协议。</li>
<li>CRC（4字节）。检错。</li>
<li></li>
<li>前同步码（8字节）。前7字节为<code>10101010</code>，最后字节为<code>10101011</code>，用于唤醒接受适配器并同步时钟。</li>
</ul>
</li>
</ul>
<p>以太网技术向网络层提供：</p>
<ul>
<li>无连接服务：发送数据报帧无需事先握手</li>
<li>不可靠服务：接收器进行CRC校验，不管是否通过都不返回确认（ACK）/否定确认（NEG）帧</li>
</ul>
<h2 id="链路层交换机"><a href="#链路层交换机" class="headerlink" title="链路层交换机"></a>链路层交换机</h2>]]></content>
      <categories>
        <category>课程</category>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title>leetcode 刷题笔记</title>
    <url>/2021/07/05/leetcode/</url>
    <content><![CDATA[<p>问求上完了，依然不能放松对数据结构和算法的训练，要坚持不懈的刷题。着重对一些经典题型和方法进行总结。</p>
<a id="more"></a>
<h1 id="基本常见算法"><a href="#基本常见算法" class="headerlink" title="基本常见算法"></a>基本常见算法</h1><h2 id="两数之和"><a href="#两数之和" class="headerlink" title="两数之和"></a>两数之和</h2><blockquote>
<p>给定一个整数数组 <code>nums</code> 和一个整数目标值 <code>target</code>，请你在该数组中找出 <strong>和为目标值</strong> <code>target</code> 的那 <strong>两个</strong> 整数，并返回它们的数组下标。</p>
</blockquote>
<p>首先肯定有一个 $O(n^2)$ 的朴素解法，然后我想到先排序然后用首尾指针向中间移动，$O(n\log{n})$。但是需要一个哈希来存排序前的下标，既然这样其实就可以直接用哈希 $O(n)$ 解了。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">twoSum</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = nums.size();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = i + <span class="number">1</span>; j &lt; n; ++j) &#123;</span><br><span class="line">                <span class="keyword">if</span> (nums[i] + nums[j] == target) &#123;</span><br><span class="line">                    <span class="keyword">return</span> &#123;i, j&#125;;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="剑指offer系列"><a href="#剑指offer系列" class="headerlink" title="剑指offer系列"></a>剑指offer系列</h1><h2 id="04-二维数组中查找"><a href="#04-二维数组中查找" class="headerlink" title="04. 二维数组中查找"></a>04. 二维数组中查找</h2><blockquote>
<p>在一个 n * m 的二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个高效的函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。</p>
</blockquote>
<p>原来是想的二分查找但是思路很明显是错的。只要从一个角落开始，每个元素比较，如果不一样就超朝另外两个方向移动。</p>
<p>其实也是一个压缩解空间的思想，比如我从右上角开始，当前元素比target大，说明在该元素下面所有的元素都不符合，只能向左移动一位。</p>
<p>注意边界条件<code>matrix = []</code>。复杂度 $O(m + n)$。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">findNumberIn2DArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; matrix, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> len1 = matrix.size();</span><br><span class="line">        <span class="keyword">if</span> (len1 == <span class="number">0</span>) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">int</span> len2 = matrix[<span class="number">0</span>].size();</span><br><span class="line">        <span class="keyword">int</span> h = <span class="number">0</span>, v = len2 - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (h &lt; len1 &amp;&amp; v &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">int</span> p = matrix[h][v];</span><br><span class="line">            <span class="keyword">if</span> (p == target) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">if</span> (p &gt; target) &#123;</span><br><span class="line">                v--;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (p &lt; target) &#123;</span><br><span class="line">                h++;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>问题求解笔记-NP完全性</title>
    <url>/2021/05/10/np-completeness/</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><ul>
<li><p>P类问题就是在多项式时间内可以被解决的（solvable）问题。</p>
</li>
<li><p>NP类问题就是可以在多项式时间内可以被证明的（verifiable）问题。</p>
<ul>
<li>verifiable：给定问题解的<strong>证书（certificate）</strong>，然后我们可以在多项式时间验证这个 certificate 是对的。</li>
</ul>
</li>
<li><p>如果一个NP问题和其他任何NP问题一样“不易解决”，则是NPC问题。</p>
<a id="more"></a>
</li>
</ul>
<h2 id="规约-Reduction"><a href="#规约-Reduction" class="headerlink" title="规约 Reduction"></a>规约 Reduction</h2><p>有一个判定问题 A，一个判定问题 B，我们知道 $B$ 可以在多项式时间下解决，一个多项式时间规约算法（reduction algorithm）是将 A 的输入实例 $\alpha$ 转化成 B 的输入实例 $\beta$ 的过程，且：</p>
<ul>
<li>转换操作需要多项式时间。</li>
<li>两个实例的解是相同的。</li>
</ul>
<p><img data-src="Screenshot from 2021-06-17 20-36-37.png" alt></p>
<p><em>reduce方法的更深用意在于证明某个问题有多难。</em></p>
<p>证明B有多难：假设A不是P问题，B不可能是P问题；反证法：如果B是P问题，那么A是P问题</p>
<hr>
<h1 id="多项式时间"><a href="#多项式时间" class="headerlink" title="多项式时间"></a>多项式时间</h1><p><strong>引理 34.1 </strong> 设 $Q$ 是定义在一个实例集 $I$ 上的抽象判定问题，$e_1$ 和 $e_2$ 是 $I$ 上多项式相关的编码，则 $e_1(Q) \in P \Leftrightarrow e_2(Q) \in P$。 </p>
<p><em>今后不再讨论代价极高的编码，统一以二进制编码为基准。理论上，问题的难度是固有的。但是算法的复杂度，有时候却依赖于问题的编码。</em></p>
<h2 id="接受-判定"><a href="#接受-判定" class="headerlink" title="接受/判定"></a>接受/判定</h2><ul>
<li>对任意输入 $x$，算法输出 $A(x) = 1$，就叫算法 $A$ <strong>接受（accept）</strong>$x \in \{0,1\}^{\star}$。被算法 $A$ 接受的语言就是集合 $L = \{x\in \{0,1\}^{\star} : A(x) = 1\}$。</li>
<li>对任意的输入 $x$，算法输出 $A(x) = 0$，就叫算法 $A$ <strong>拒绝（reject）</strong>$x \in \{0, 1\}^{\star}$。</li>
<li>算法 $A$ <strong>判定（decide）</strong>一个语言 $L$ 指对任意输入 $x$，在 $L$ 中的接受，不在 $L$ 中的拒绝。</li>
<li>区别在于 $A$ 接受 $L$ 只能确保在 $L$ 中的 $x$ 接受，不在 $L$ 中的也有可能接受，或没有结果（不终止）。</li>
</ul>
<p>复杂类 $P$ 的定义：</p>
<script type="math/tex; mode=display">P = \{L \subseteq \{0,1\}^{\star} : \text{存在一个算法 A，可以在多项式时间判定 }L\}</script><p><strong>定理 34.2 </strong> <script type="math/tex">P = \{L \subseteq \{0,1\}^{\star} : \text{存在一个算法 A，可以在多项式时间接受 }L\}</script></p>
<p><em>这个定理说明接受一个语言的多项式算法，一定可以被改造成判定一个语言的多项式算法。</em></p>
<p>（A 最多 $cn^k$ 步接受输入，构造 A’ 跑 $cn^k$ 步，接受返回 1，否则返回 0，判定了 $L$。）</p>
<h1 id="多项式时间的验证-Verification"><a href="#多项式时间的验证-Verification" class="headerlink" title="多项式时间的验证 Verification"></a>多项式时间的验证 Verification</h1><p>一份<strong>验证算法（verifier）</strong> 是一个两个变量的算法 A，一个输入为串 $x$，另一个是叫<strong>证书（certificate）</strong>的二进制串 $y$。如果存在一个证书 $y$ 使得 $A(x, y) = 1$，则称 $A$ <strong>验证（verify）</strong>了输入 $x$。</p>
<p>一个由算法 A 验证的语言是 <script type="math/tex">L = \{x \in \{0, 1\}^{\star} : \exists y \in \{0,1\}^{\star} , A(x, y) = 1\}</script>。</p>
<p>复杂类 NP 定义：一个语言 $L$ 属于 NP 当且仅当存在一个两输入多项式时间算法 $A$ 和常数 $c$，使得</p>
<p><script type="math/tex">L = \{x \in \{0, 1\}^{\star} : \text{存在一个证书 }y, |y| = O(|x|^c), A(x,y) = 1 \}</script>。</p>
<p><em>当证书是超多项式的时候，验算证书的代价不再是保证为多项式了。</em></p>
<p>P问题是可以在多项式时间内被判定的问题。</p>
<p>NP问题是所有可以在多项式时间内被验证的问题。</p>
<p>co-NP是所有它的补是NP问题的问题。</p>
<p><strong>大多数人相信 $P \neq NP$。</strong></p>
<hr>
<h1 id="NP完全性与可规约性"><a href="#NP完全性与可规约性" class="headerlink" title="NP完全性与可规约性"></a>NP完全性与可规约性</h1><p>引入NPC：只要有一个NPC可以多项式求解，那么所有的NP问题都可多项式求解。</p>
<h2 id="可规约性-Reducibility"><a href="#可规约性-Reducibility" class="headerlink" title="可规约性 Reducibility"></a>可规约性 Reducibility</h2><ul>
<li>如果存在一个多项式时间可计算函数 $f : \{0, 1\}^{\star} \to \{0, 1\}^{\star}$，满足 $\forall x \in \{0 , 1\}^{\star}, x \in L_1 \leftrightarrow f(x) \in L_2$，则称语言 $L_1$ 可以在多项式时间内规约到语言 $L_2$，记作 $L_1 \leq_p L_2$。<ul>
<li>$f$ 叫规约函数</li>
<li>算 $f$ 的算法叫规约算法</li>
</ul>
</li>
</ul>
<p><strong>引理 34.3 </strong> 如果 $L_1, L_2 \subseteq \{0 ,1\}^{\star}$ 是满足 $L_1 \leq_p L_2$ 的语言，则 $L_2 \in P \Rightarrow L_1 \in P$。</p>
<p><em>隐藏着：L1的复杂度不会“f多项式以上”超过L2的复杂度。</em></p>
<p><em>更深层次的隐藏着：如果L1很难，L2不能很简单：最多相差多项式因子。</em></p>
<p><em>揭示了认知一个问题的难度的方法学：如果一个老问题能够归约到待认知的新问题，这个新问题的难度最多简单到多项式因子</em></p>
<h2 id="NP完全性"><a href="#NP完全性" class="headerlink" title="NP完全性"></a>NP完全性</h2><p>语言 $L$ 是 NP完全的，如果：</p>
<ol>
<li>$L \in NP$</li>
<li>$\forall L’ \in NP, L’ \leq_p L$</li>
</ol>
<ul>
<li>如果 $L$ 只满足性质2，但不一定满足性质1，叫 <strong>NP难（NP-hard）</strong>的。</li>
</ul>
<p><strong>定理 34.4 </strong> 如果任何 NP完全问题是多项式时间可解的，则 $P = NP$。等价的，如果存在某一个NP问题不是多项式可解的，则所有NP完全问题都不是多项式时间可解的。</p>
<p><strong>定理 34.7 </strong> 电路可满足性问题是NP完全的。</p>
]]></content>
      <categories>
        <category>课程</category>
        <category>problem-solving</category>
      </categories>
  </entry>
  <entry>
    <title>问题求解笔记-数论算法</title>
    <url>/2021/04/08/number-theory-algorithms/</url>
    <content><![CDATA[<hr>
<h1 id="数论算法"><a href="#数论算法" class="headerlink" title="数论算法"></a>数论算法</h1><p><strong>定理 31.2.</strong> 如果任意整数 $a$ 和 $b$ 都不为 $0$，则 $gcd(a, b)$ 是 $a$ 与 $b$ 的线性组合集 $\{ ax + by : x, y \in \mathbb{Z} \}$ 中的最小正元素。</p>
<p><strong>推论 31.3.</strong> 任意整数 $a$ 和 $b$，$d | a \wedge d|b \Rightarrow d | gcd(a,b)$。</p>
<p><strong>推论 31.4</strong> 对所有整数 $a,b$ 及任意非负整数 $n$，有 <script type="math/tex">gcd(an,bn) = n~gcd(a,b)</script>。</p>
<p><strong>推论 31.5.</strong> 对所有正整数 $a,b,n$，$n | ab \wedge gcd(a, n) = 1 \Rightarrow n | b$。</p>
<p><strong>定理 31.6.</strong> 对所有正整数 $a,b,p$，$gcd(a, p ) = 1 \wedge gcd(b, p) = 1 \Rightarrow gcd(ab, p) = 1$。</p>
<p><strong>定理 31.7.</strong> 对所有的素数 $p$ 和所有整数 $a,b$， $p | ab \Rightarrow p |a \vee p | b$。</p>
<p><strong>定理 31.8.</strong>（唯一因子分解定理）合数 $a$ 的素因子分解是唯一的。</p>
<a id="more"></a>
<p><strong>定理 31.9</strong> $\forall a, b, gcd(a, b) = gcd(b, a ~mod ~b)$.</p>
<h2 id="欧几里得算法"><a href="#欧几里得算法" class="headerlink" title="欧几里得算法"></a>欧几里得算法</h2><p><img data-src="euclid.png" alt="ECULID" title="eculid"></p>
<p>（这个图片为什么这么大。。。就这样吧反正我也不会调）</p>
<h3 id="欧几里得算法的运行时间"><a href="#欧几里得算法的运行时间" class="headerlink" title="欧几里得算法的运行时间"></a>欧几里得算法的运行时间</h3><p><strong>引理 31.10.</strong> 如果 $a &gt; b \geq 1$ 并且 ECULID$(a,b)$ 执行了 $k \geq 1$ 次递归调用，则 $a \geq F_{k+2}, b \geq F_{k+1}$。</p>
<p><strong>引理 31.11.</strong>（Lame引理）对任意整数 $k \geq 1$，如果 $a &gt; b \geq 1$，且 $b &lt; F_{k+1}$，则 EUCLID$(a,b)$ 的递归调用次数少于 $k$ 次。</p>
<h3 id="扩展欧几里得算法"><a href="#扩展欧几里得算法" class="headerlink" title="扩展欧几里得算法"></a>扩展欧几里得算法</h3><p><img data-src="extgcd.png" alt="EXT-ECULID" title="ext-eculid"></p>
<ul>
<li>递归调用次数为 $O(\lg{b})$。</li>
</ul>
<h2 id="模运算"><a href="#模运算" class="headerlink" title="模运算"></a>模运算</h2><ul>
<li><p><strong>欧拉函数</strong>：<script type="math/tex">\phi(n) = n \prod\limits_{p\text{是素数且}p|n} (1 - \dfrac{1}{p})</script></p>
</li>
<li><p>$\phi(n) &gt; \dfrac{n}{e^\gamma \ln{\ln{n}} + \dfrac{3}{\ln{\ln{n}}}}$</p>
</li>
</ul>
<p>  <strong>定理31.15.</strong> （拉格朗日定理）：如果 $(S, \oplus)$ 是一个有限群，$(S’, \oplus )$ 是 $(S,\oplus)$ 的一个子群，则 $|S’|$ 是 $|S|$ 的一个约数。（我觉得比TJ上讲的通俗易懂得多。。。）</p>
<p>  <strong>推论 31.16.</strong> 如果 $S’$ 是有限群 $S$ 的真子群，则 $|S’ | \leq |S| / 2$。</p>
<p><strong>定理 31.17.</strong> 对任意有限群 $(S, \oplus)$ 和任意 $a \in S$，一个元素的阶等于它所生成的子群的规模，即 $ord(a) = |\langle a \rangle|$。</p>
<p><strong>推论 31.18.</strong> 序列 $a^{(1)}, a^{(2)}, …$是周期序列，其周期为 $t = ord(a)$，即 $a^{(i)} = a^{(j)} \Leftrightarrow i \equiv j~ (\text{mod }t)$。</p>
<p><strong>推论 31.19.</strong> 如果 $(S, \oplus)$ 是具有单位元的有限群，则对所有 $a \in S$，$a^{(|S|)} = e$。</p>
<h2 id="求解模线性方程"><a href="#求解模线性方程" class="headerlink" title="求解模线性方程"></a>求解模线性方程</h2><p><strong>定理 31.20.</strong> 对任意正整数 $a$ 和 $n$，如果 $d = gcd(a,n)$，则在 $\mathbb{Z}_n$ 中，$\langle a \rangle = \langle d \rangle = \{ 0, d, 2d, \cdots , ((n/d) - 1)d \}$。因此，$|\langle a \rangle| = n /d$。</p>
<p><strong>推论 31.21.</strong> $d | b \Leftrightarrow \text{方程} ax \equiv b (\text{mod } n)$ 对 $x$ 有解，$d  =gcd(a,n)$。 </p>
<p><strong>推论 31.22</strong> 方程 $ax \equiv b (\text{mod } n)$ 要么模 $n$ 下有 $d$ 个不同的解，要么无解。</p>
<p><strong>定理 31.23.</strong> 假设对某些整数 $x’, y’$ 有 $d = ax’ + ny’$。如果 $d | b$，则方程 $ax \equiv b(\text{mod } n)$ 有一个解的值为 $x_0$，<script type="math/tex">x_0 = x'(b / d) \mod n</script></p>
<p><strong>定理 31.24.</strong> 假设方程 $ax \equiv b(\text{mod } n)$ 有解，且 $x_0$ 是该方程的任意一个解。则该方程对模 $n$ 恰有 $d = gcd(a,n)$ 个解为 $x_i = x_0 + i(n /d), i =0, 1, \cdots, d- 1$。</p>
<h2 id="中国剩余定理"><a href="#中国剩余定理" class="headerlink" title="中国剩余定理"></a>中国剩余定理</h2><p><strong>定理 31.27.</strong>（中国剩余定理） 令 $n = n_1 n_2\cdots n_k$，其中因子 $n_i$ 两两互质。考虑以下对应关系：</p>
<script type="math/tex; mode=display">a \leftrightarrow (a_1, a_2, \cdots, a_k)</script><p>这里 $a \in \mathbb{Z}_n, a_i \in \mathbb{Z}_{n_i}$，且对 $i = 1, 2, \cdots, k$，</p>
<script type="math/tex; mode=display">a_i =  a \mod n_i</script><p>则通过在合适的系统中对每个坐标位置独立地执行操作，对 $\mathbb{Z}_n$ 中元素所执行地运算可以等价地作用于对应的 $k$ 元组。</p>
<p><strong>推论 31.28.</strong> 如果 $n_1, n_2, \cdots, n_k$ 两两互质，则关于未知量 $x$ 地联立方程组 <script type="math/tex">x \equiv a_i (\text{mod } n_i), i = 1, 2, \cdots,k</script> 对模 $n$ 有唯一解。</p>
<p><strong>推论 31.29.</strong> 如果 $n_1, n_2, \cdots, n_k$ 两两互质，则对所有整数 $x$ 和 $a$，$x \equiv a(\text{mod } n_i), i = 1, 2, \cdots, k \Leftrightarrow x \equiv a (\text{mod } n)$。</p>
<h2 id="元素的幂"><a href="#元素的幂" class="headerlink" title="元素的幂"></a>元素的幂</h2><p><strong>定理 31.32.</strong> 对所有地素数 $p &gt; 2$ 和所有的正整数 $e$，使得 $\mathbb{Z}_n^*$ 是循环群的 $n &gt; 1$ 的值为 $2, 4, p^e, p^{2e}$。</p>
<p>如果 $g$ 是 $\mathbb{Z}_{b}^{\star}$ 的一个原根且 $a$ 是 $\mathbb{Z}_{b}^{\star}$ 中的任意元素，则存在一个 $z$，使得 $g^{z} \equiv a(\text{mod } n)$ 。这个 $z$ 称为对模 $n$ 到基 $g$ 上的一个<strong>离散对数</strong>。</p>
<p><strong>定理 31.33.</strong>（离散对数定理）如果 $g$ 是 $\mathbb{Z}_b^*$ 的一个原根，$x \equiv y(\text{mod } \phi(n)) \Leftrightarrow g^x \equiv g^y (\text{mod } \phi(n))$。</p>
<p><strong>定理 31.34.</strong> $p$ 是一个奇素数且 $e &gt; 1 \Rightarrow x^2 \equiv 1(\text{mod } p^e)$ 仅有两个解 $x =1, x= -1$。</p>
<h2 id="反复平方法"><a href="#反复平方法" class="headerlink" title="反复平方法"></a>反复平方法</h2><p>快速幂</p>
]]></content>
      <categories>
        <category>课程</category>
        <category>problem-solving</category>
      </categories>
  </entry>
  <entry>
    <title>问题求解笔记-数论基础</title>
    <url>/2021/03/20/number-theory/</url>
    <content><![CDATA[<h1 id="数学归纳法-Mathematical-Induction"><a href="#数学归纳法-Mathematical-Induction" class="headerlink" title="数学归纳法 Mathematical Induction"></a>数学归纳法 Mathematical Induction</h1><p>没啥好讲的。<br><strong>第一第二数学归纳法</strong><br><strong>良序公理</strong><br><a id="more"></a></p>
<h1 id="辗转相除法-Division-Algorithm"><a href="#辗转相除法-Division-Algorithm" class="headerlink" title="辗转相除法 Division Algorithm"></a>辗转相除法 Division Algorithm</h1><p><strong>定理 2.9.</strong> $a$ 和 $b$ 为整数，$b &gt; 0$，则存在<strong>唯一整数</strong> $q$ 和 $r$，使得</p>
<script type="math/tex; mode=display">a = bq + r</script><p>其中 $0 \leq r &lt; b$。</p>
<p>$a$ 整除 $b$ 记作 $a | b$。<br>最大公约数、互质等概念略。</p>
<p><strong>定理 2.10.</strong> $a$ 和 $b$ 为非零整数，则存在整数 $r，s$，使得</p>
<script type="math/tex; mode=display">gcd(a,b) = ar + bs</script><p>进一步，$a,b$的最大公约数是唯一的。</p>
<p><strong>推论 2.11.</strong> $a,b$ 互质，则存在整数 $r,s$，使得 $ar + bs = 1$。（可证反之也成立）</p>
<h2 id="欧几里得算法-Euclidean-Algorithm"><a href="#欧几里得算法-Euclidean-Algorithm" class="headerlink" title="欧几里得算法 Euclidean Algorithm"></a>欧几里得算法 Euclidean Algorithm</h2><h2 id="质数-Prime-Number"><a href="#质数-Prime-Number" class="headerlink" title="质数 Prime Number"></a>质数 Prime Number</h2><p><strong>定理 2.14.</strong> 存在无穷多质数。（证明挺重要的）</p>
<p><strong>定理 2.15. 算数基本定理</strong> $n$ 为整数，则 $n$ 可以唯一表示为多个素数的乘积。</p>
<script type="math/tex; mode=display">n = p_1 p_2 ... p_k</script><hr>
<h1 id="乘法逆元和最大公约数"><a href="#乘法逆元和最大公约数" class="headerlink" title="乘法逆元和最大公约数"></a>乘法逆元和最大公约数</h1><p><strong>乘法逆元 Multipllication Inverse</strong>: $a’ \in \mathbb{Z}_n, a \in \mathbb{Z}_n, a \cdot_n a’ = a$，则称 $a’$ 是 $a$ 在 $\mathbb{Z}_n$ 中的乘法逆元。</p>
<p><strong>引理 2.5.</strong> 解模方程： 如果 $a$ 在 $\mathbb{Z}_n$ 中有逆元 $a’$，则对任意 $b \in \mathbb{Z}_n$，方程</p>
<script type="math/tex; mode=display">a \cdot_n x = b</script><p>有唯一解 $x = a’ \cdot_n b$。</p>
<p><strong>推论 2.6.</strong> 如果存在 $b \in \mathbb{Z}_n$，方程</p>
<script type="math/tex; mode=display">a \cdot_n x = b</script><p>无解，则 $a$ 没有逆元。</p>
<p><strong>定理 2.7.</strong> 若有逆元则唯一。</p>
<p><strong>引理 2.8.</strong> 方程</p>
<script type="math/tex; mode=display">a \cdot_n x = 1</script><p>有解当且仅当存在整数 $x,y$，使得</p>
<script type="math/tex; mode=display">ax + ny = 1</script><p><strong>定理 2.9.</strong> $a \in \mathbb{Z}_n$ 有逆元 $\Leftrightarrow$ 存在整数 $x，y$，使得 $ax + ny = 1$。</p>
<p><strong>推论 2.10.</strong> $a \in \mathbb{Z}_n$ 且存在整数 $x，y$，使得 $ax + ny = 1$，则 $a$ 的逆元为 $x$ (mod $n$)。</p>
<p><strong>引理 2.11.</strong> 给定 $a,n$，如果存在整数 $x，y$，使得 $ax + ny = 1$，则 $a,n$ 互质，即 $gcd(a,n) = 1$。</p>
<p><strong>定理 2.12.</strong> 同最上面一条。</p>
<p><strong>引理 2.13.</strong> $j,k,q,r$ 是正整数，满足 $k = j q + r$，则</p>
<script type="math/tex; mode=display">gcd(j,k) = gcd(r, j)</script><p><strong>定理 2.15.</strong> $gcd(j,k)  = 1 \Leftrightarrow \exists x,y, jx + ky = 1$.</p>
<p><strong>推论 2.16.</strong> $a\in \mathbb{Z}_n$ 在 $\mathbb{Z}_n$ 中有逆元 $\Leftarrow gcd(a,n) = 1$。</p>
<p><strong>推论 2.17.</strong> 对任意质数 $p$，每个非零整数 $a \in \mathbb{Z}_p$ 都有逆元。</p>
]]></content>
      <categories>
        <category>课程</category>
        <category>problem-solving</category>
      </categories>
  </entry>
  <entry>
    <title>numpy</title>
    <url>/2021/07/09/numpy/</url>
    <content><![CDATA[<p>本文主要是对于python库<code>numpy</code>的学习使用笔记。</p>
<p>参考网站：</p>
<p><a href="https://numpy.org/">numpy官方网站</a></p>
<p><a href="https://numpy.org.cn/">numpy中文网</a></p>
<a id="more"></a>
<h1 id="numpy是啥"><a href="#numpy是啥" class="headerlink" title="numpy是啥"></a>numpy是啥</h1><p>NumPy是Python中科学计算的基础包。它是一个Python库，提供多维数组对象，各种派生对象（如掩码数组和矩阵），以及用于数组快速操作的各种API，有包括数学、逻辑、形状操作、排序、选择、输入输出、离散傅立叶变换、基本线性代数，基本统计运算和随机模拟等等。</p>
<p><code>numpy</code> 的核心是它的 <code>ndarray</code> 数组对象，它比python原生数组更加高效，支持更多操作。</p>
<p>其两个特征：矢量化和广播。通过预编译的C代码达到快速。</p>
<h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><p><code>ndarray</code> 是一个元素表（通常是数字），所有类型都相同，由非负整数元组索引。在NumPy维度中称为 <em>轴</em> 。</p>
<h2 id="数组属性"><a href="#数组属性" class="headerlink" title="数组属性"></a>数组属性</h2><p>它有很多重要属性，通过一个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.arange(<span class="number">15</span>).reshape(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">       [ <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">       [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape <span class="comment"># 数组的维度</span></span><br><span class="line">(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.ndim <span class="comment"># 数组的维度的个数</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.dtype.name <span class="comment"># 描述数组元素的类型</span></span><br><span class="line"><span class="string">&#x27;int64&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.itemsize <span class="comment"># 每个元素的字节大小</span></span><br><span class="line"><span class="number">8</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.size <span class="comment"># 数组元素个数，维度的乘积</span></span><br><span class="line"><span class="number">15</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">type</span>(a)</span><br><span class="line">&lt;<span class="built_in">type</span> <span class="string">&#x27;numpy.ndarray&#x27;</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = np.array([<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">array([<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">type</span>(b)</span><br><span class="line">&lt;<span class="built_in">type</span> <span class="string">&#x27;numpy.ndarray&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>
<h2 id="数组创建"><a href="#数组创建" class="headerlink" title="数组创建"></a>数组创建</h2><h3 id="直接从python元组创建"><a href="#直接从python元组创建" class="headerlink" title="直接从python元组创建"></a>直接从python元组创建</h3><p>一定要是传入一个列表！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.array([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = np.array([(<span class="number">1.5</span>,<span class="number">2</span>,<span class="number">3</span>), (<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">array([[ <span class="number">1.5</span>,  <span class="number">2.</span> ,  <span class="number">3.</span> ],</span><br><span class="line">       [ <span class="number">4.</span> ,  <span class="number">5.</span> ,  <span class="number">6.</span> ]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = np.array( [ [<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>] ], dtype=<span class="built_in">complex</span> ) <span class="comment"># 也可以显式指定类型</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c</span><br><span class="line">array([[ <span class="number">1.</span>+<span class="number">0.j</span>,  <span class="number">2.</span>+<span class="number">0.j</span>],</span><br><span class="line">       [ <span class="number">3.</span>+<span class="number">0.j</span>,  <span class="number">4.</span>+<span class="number">0.j</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="numpy提供初始化一些函数"><a href="#numpy提供初始化一些函数" class="headerlink" title="numpy提供初始化一些函数"></a>numpy提供初始化一些函数</h3><p>通常，数组的元素最初是未知的，但它的大小是已知的。因此，NumPy提供了几个函数来创建具有初始占位符内容的数组。这就减少了数组增长的必要，因为数组增长的操作花费很大。</p>
<p>函数<code>zeros</code>创建一个由0组成的数组，函数 <code>ones</code>创建一个完整的数组，函数<code>empty</code> 创建一个数组，其初始内容是随机的，取决于内存的状态。默认情况下，创建的数组的dtype是 <code>float64</code> 类型的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.zeros((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.ones( (<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>), dtype=np.int16 )                <span class="comment"># dtype can also be specified</span></span><br><span class="line">array([[[ <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]],</span><br><span class="line">       [[ <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]], dtype=int16)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.empty( (<span class="number">2</span>,<span class="number">3</span>) )                                 <span class="comment"># uninitialized, output may vary</span></span><br><span class="line">array([[  <span class="number">3.73603959e-262</span>,   <span class="number">6.02658058e-154</span>,   <span class="number">6.55490914e-260</span>],</span><br><span class="line">       [  <span class="number">5.30498948e-313</span>,   <span class="number">3.14673309e-307</span>,   <span class="number">1.00000000e+000</span>]])</span><br></pre></td></tr></table></figure>
<p><code>numpy</code> 提供了一个类似于<code>range</code>的函数，该函数返回数组而不是列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.arange( <span class="number">0</span>, <span class="number">2</span>, <span class="number">0.3</span> )                 <span class="comment"># it accepts float arguments</span></span><br><span class="line">array([ <span class="number">0.</span> ,  <span class="number">0.3</span>,  <span class="number">0.6</span>,  <span class="number">0.9</span>,  <span class="number">1.2</span>,  <span class="number">1.5</span>,  <span class="number">1.8</span>])</span><br></pre></td></tr></table></figure>
<p>当<code>arange</code>与浮点参数一起使用时，由于有限的浮点精度，通常不可能预测所获得的元素的数量。出于这个原因，通常最好使用<code>linspace</code>函数来接收我们想要的元素数量的函数，而不是步长（step）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> numpy <span class="keyword">import</span> pi</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.linspace( <span class="number">0</span>, <span class="number">2</span>, <span class="number">9</span> )                 <span class="comment"># 9 numbers from 0 to 2</span></span><br><span class="line">array([ <span class="number">0.</span>  ,  <span class="number">0.25</span>,  <span class="number">0.5</span> ,  <span class="number">0.75</span>,  <span class="number">1.</span>  ,  <span class="number">1.25</span>,  <span class="number">1.5</span> ,  <span class="number">1.75</span>,  <span class="number">2.</span>  ])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = np.linspace( <span class="number">0</span>, <span class="number">2</span>*pi, <span class="number">100</span> )        <span class="comment"># useful to evaluate function at lots of points</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = np.sin(x)</span><br></pre></td></tr></table></figure>
<h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><p>数组上的操作会作用到每个元素上。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.array( [<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>,<span class="number">50</span>] )</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = np.arange( <span class="number">4</span> )</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = a-b</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c</span><br><span class="line">array([<span class="number">20</span>, <span class="number">29</span>, <span class="number">38</span>, <span class="number">47</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b**<span class="number">2</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">10</span>*np.sin(a)</span><br><span class="line">array([ <span class="number">9.12945251</span>, -<span class="number">9.88031624</span>,  <span class="number">7.4511316</span> , -<span class="number">2.62374854</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a&lt;<span class="number">35</span></span><br><span class="line">array([ <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>])</span><br></pre></td></tr></table></figure>
<p>需要注意的就是，乘法运算符 <code>*</code> 是逐元素对应操作而不是矩阵相乘。矩阵乘积可以用 <code>@</code> 运算符。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A = np.array( [[<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line"><span class="meta">... </span>            [<span class="number">0</span>,<span class="number">1</span>]] )</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>B = np.array( [[<span class="number">2</span>,<span class="number">0</span>],</span><br><span class="line"><span class="meta">... </span>            [<span class="number">3</span>,<span class="number">4</span>]] )</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A * B                       <span class="comment"># elementwise product</span></span><br><span class="line">array([[<span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A @ B                       <span class="comment"># matrix product</span></span><br><span class="line">array([[<span class="number">5</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure>
<ul>
<li>ndarray 乘法操作的一些探究</li>
</ul>
<p>事实上我在向量化操作的时候遇到了挺多问题，有一些模糊的地方，在此测试阐明一些东西。</p>
<p>首先 <code>numpy</code> 中是有 <code>matrix</code> 类型的，主要用于线性代数中，不过在绝大部分场合 <code>ndarray</code> 可以完全替代 <code>matrix</code>，所以我们决定基本弃用 <code>matrix</code>。</p>
<ul>
<li>对于最简单的向量，也就是一个长度为 n 的 array。<ul>
<li>其转置始终是它自己，<code>*</code> 是逐元素相乘，返回相同长度的向量；</li>
<li><code>@</code> 是矩阵乘法，返回向量内积也就是一个数。</li>
<li>这也就要求两个向量的长度相同。</li>
<li><code>A @ v</code> 将 <code>v</code> 视为列向量，而 <code>v @ A</code> 将 <code>v</code> 视为行向量。这可以节省键入许多转置。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">array([<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a * b</span><br><span class="line">array([ <span class="number">0</span>,  <span class="number">6</span>, <span class="number">14</span>, <span class="number">24</span>, <span class="number">36</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a @ b</span><br><span class="line"><span class="number">80</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = np.array([<span class="number">10</span>, <span class="number">20</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a * c</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">ValueError: operands could <span class="keyword">not</span> be broadcast together <span class="keyword">with</span> shapes (<span class="number">5</span>,) (<span class="number">2</span>,) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a @ c</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 5)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>当我们尝试多维向量时，就可以把这个 <code>ndarray</code> 看作矩阵来用了，$N \times 1$ 和 $1 \times N$ 和 $N$ 都是不一样的 array。</p>
<p><code>*</code> 其实是一个广播操作，两个矩阵不一定是要 $m \times k$ 和 $k \times n$，但也不是随意的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>A</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>B</span><br><span class="line">array([<span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A * B</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">4</span>],</span><br><span class="line">       [ <span class="number">8</span>, <span class="number">12</span>]])</span><br><span class="line"><span class="comment"># 广播到每个元素</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>C </span><br><span class="line">array([<span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A * C</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">5</span>],</span><br><span class="line">       [ <span class="number">8</span>, <span class="number">15</span>]])</span><br><span class="line"><span class="comment"># 逐元素相乘</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>D </span><br><span class="line">array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A * D</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">ValueError: operands could <span class="keyword">not</span> be broadcast together <span class="keyword">with</span> shapes (<span class="number">2</span>,<span class="number">2</span>) (<span class="number">3</span>,) </span><br><span class="line"><span class="comment"># 广播不了了</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>E</span><br><span class="line">array([[<span class="number">4</span>],</span><br><span class="line">       [<span class="number">5</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A * E</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">4</span>],</span><br><span class="line">       [<span class="number">10</span>, <span class="number">15</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>F</span><br><span class="line">array([[<span class="number">4</span>],</span><br><span class="line">       [<span class="number">5</span>],</span><br><span class="line">       [<span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A * F</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">ValueError: operands could <span class="keyword">not</span> be broadcast together <span class="keyword">with</span> shapes (<span class="number">2</span>,<span class="number">2</span>) (<span class="number">3</span>,<span class="number">1</span>) </span><br></pre></td></tr></table></figure>
<p>所以可以看出来其实是要符合<strong>广播的规则</strong>，所相乘的矩阵行或列的个数要么是1（广播到每一个元素），要么和原矩阵行列元素个数相等（逐元素操作）。这套规则其实不止是乘法，<code>+-/</code> 都是一样的。</p>
<ul>
<li>当 <code>ndarray</code> 多维时，可以作为矩阵，用 <code>@</code> 进行运算，这时候就不会帮你自动转置了，要符合矩阵相乘条件也就是 $(m,k) \times (k,n)$。</li>
</ul>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>G</span><br><span class="line">array([[<span class="number">4</span>, <span class="number">6</span>],</span><br><span class="line">       [<span class="number">5</span>, <span class="number">7</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A @ G</span><br><span class="line">array([[ <span class="number">5</span>,  <span class="number">7</span>],</span><br><span class="line">       [<span class="number">23</span>, <span class="number">33</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>H</span><br><span class="line">array([[<span class="number">4</span>, <span class="number">7</span>],</span><br><span class="line">       [<span class="number">5</span>, <span class="number">8</span>],</span><br><span class="line">       [<span class="number">6</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>A @ H</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 3 is different from 2)</span><br></pre></td></tr></table></figure>
<p><code>ndarray</code> 还提供了很多一元操作</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.<span class="built_in">sum</span>()</span><br><span class="line"><span class="number">2.5718191614547998</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.<span class="built_in">min</span>()</span><br><span class="line"><span class="number">0.1862602113776709</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.<span class="built_in">max</span>()</span><br><span class="line"><span class="number">0.6852195003967595</span></span><br><span class="line"><span class="comment"># 可以指定轴</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">       [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">       [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]])</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.<span class="built_in">sum</span>(axis=<span class="number">0</span>)                            <span class="comment"># sum of each column</span></span><br><span class="line">array([<span class="number">12</span>, <span class="number">15</span>, <span class="number">18</span>, <span class="number">21</span>])</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.<span class="built_in">min</span>(axis=<span class="number">1</span>)                            <span class="comment"># min of each row</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.cumsum(axis=<span class="number">1</span>)                         <span class="comment"># cumulative sum along each row</span></span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">3</span>,  <span class="number">6</span>],</span><br><span class="line">       [ <span class="number">4</span>,  <span class="number">9</span>, <span class="number">15</span>, <span class="number">22</span>],</span><br><span class="line">       [ <span class="number">8</span>, <span class="number">17</span>, <span class="number">27</span>, <span class="number">38</span>]])</span><br></pre></td></tr></table></figure>
<p>一些<strong>通函数</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.exp(B)</span><br><span class="line">array([ <span class="number">1.</span>        ,  <span class="number">2.71828183</span>,  <span class="number">7.3890561</span> ])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.sqrt(B)</span><br><span class="line">array([ <span class="number">0.</span>        ,  <span class="number">1.</span>        ,  <span class="number">1.41421356</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>C = np.array([<span class="number">2.</span>, -<span class="number">1.</span>, <span class="number">4.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.add(B, C)</span><br><span class="line">array([ <span class="number">2.</span>,  <span class="number">0.</span>,  <span class="number">6.</span>])</span><br></pre></td></tr></table></figure>
<p>和python原生数组一样可以索引，切片和迭代。<strong>多维</strong>的数组每个轴可以有一个索引。这些索引以逗号分隔的元组给出。当提供的索引少于轴的数量时，缺失的索引被认为是完整的切片。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">       [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>],</span><br><span class="line">       [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>],</span><br><span class="line">       [<span class="number">30</span>, <span class="number">31</span>, <span class="number">32</span>, <span class="number">33</span>],</span><br><span class="line">       [<span class="number">40</span>, <span class="number">41</span>, <span class="number">42</span>, <span class="number">43</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b[<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line"><span class="number">23</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b[<span class="number">0</span>:<span class="number">5</span>, <span class="number">1</span>]                       <span class="comment"># each row in the second column of b</span></span><br><span class="line">array([ <span class="number">1</span>, <span class="number">11</span>, <span class="number">21</span>, <span class="number">31</span>, <span class="number">41</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b[ : ,<span class="number">1</span>]                        <span class="comment"># equivalent to the previous example</span></span><br><span class="line">array([ <span class="number">1</span>, <span class="number">11</span>, <span class="number">21</span>, <span class="number">31</span>, <span class="number">41</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b[<span class="number">1</span>:<span class="number">3</span>, : ]                      <span class="comment"># each column in the second and third row of b</span></span><br><span class="line">array([[<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>],</span><br><span class="line">       [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b[-<span class="number">1</span>]                                  <span class="comment"># the last row. Equivalent to b[-1,:]</span></span><br><span class="line">array([<span class="number">40</span>, <span class="number">41</span>, <span class="number">42</span>, <span class="number">43</span>])</span><br></pre></td></tr></table></figure>
<p>如果想要对数组中的每个元素执行操作，可以使用<code>flat</code>属性，该属性是数组的所有元素的迭代器：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> element <span class="keyword">in</span> b.flat:</span><br><span class="line"><span class="meta">... </span>    print(element)</span><br><span class="line">...</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">...</span><br><span class="line"><span class="number">43</span></span><br></pre></td></tr></table></figure>
<h2 id="形状操纵"><a href="#形状操纵" class="headerlink" title="形状操纵"></a>形状操纵</h2><p>以下三个命令都返回一个修改后的数组，但不会更改原始数组：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([[ <span class="number">2.</span>,  <span class="number">8.</span>,  <span class="number">0.</span>,  <span class="number">6.</span>],</span><br><span class="line">       [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">       [ <span class="number">8.</span>,  <span class="number">9.</span>,  <span class="number">3.</span>,  <span class="number">6.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.ravel()  <span class="comment"># returns the array, flattened</span></span><br><span class="line">array([ <span class="number">2.</span>,  <span class="number">8.</span>,  <span class="number">0.</span>,  <span class="number">6.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>,  <span class="number">3.</span>,  <span class="number">6.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.reshape(<span class="number">6</span>,<span class="number">2</span>)  <span class="comment"># returns the array with a modified shape</span></span><br><span class="line">array([[ <span class="number">2.</span>,  <span class="number">8.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">6.</span>],</span><br><span class="line">       [ <span class="number">4.</span>,  <span class="number">5.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">       [ <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">       [ <span class="number">3.</span>,  <span class="number">6.</span>]])</span><br><span class="line"><span class="comment"># 如果在 reshape 操作中将 size 指定为-1，则会自动计算其他的 size 大小：</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.T  <span class="comment"># returns the array, transposed</span></span><br><span class="line">array([[ <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">8.</span>],</span><br><span class="line">       [ <span class="number">8.</span>,  <span class="number">5.</span>,  <span class="number">9.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">3.</span>],</span><br><span class="line">       [ <span class="number">6.</span>,  <span class="number">1.</span>,  <span class="number">6.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.T.shape</span><br><span class="line">(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">(<span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p> <code>ndarray.resize</code>]方法会修改数组本身:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.resize((<span class="number">2</span>,<span class="number">6</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([[ <span class="number">2.</span>,  <span class="number">8.</span>,  <span class="number">0.</span>,  <span class="number">6.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>,  <span class="number">3.</span>,  <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure>
<p>数组堆叠</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([[ <span class="number">8.</span>,  <span class="number">8.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">8.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">4.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.vstack((a,b))</span><br><span class="line">array([[ <span class="number">8.</span>,  <span class="number">8.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">8.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">4.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.hstack((a,b))</span><br><span class="line">array([[ <span class="number">8.</span>,  <span class="number">8.</span>,  <span class="number">1.</span>,  <span class="number">8.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">4.</span>]])</span><br></pre></td></tr></table></figure>
<p>拆数组</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([[ <span class="number">9.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">3.</span>,  <span class="number">6.</span>,  <span class="number">8.</span>,  <span class="number">0.</span>,  <span class="number">7.</span>,  <span class="number">9.</span>,  <span class="number">7.</span>,  <span class="number">2.</span>,  <span class="number">7.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">4.</span>,  <span class="number">9.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">6.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">0.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.hsplit(a,<span class="number">3</span>)   <span class="comment"># Split a into 3</span></span><br><span class="line">[array([[ <span class="number">9.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">3.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">4.</span>,  <span class="number">9.</span>,  <span class="number">2.</span>]]), array([[ <span class="number">6.</span>,  <span class="number">8.</span>,  <span class="number">0.</span>,  <span class="number">7.</span>],</span><br><span class="line">       [ <span class="number">2.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">6.</span>]]), array([[ <span class="number">9.</span>,  <span class="number">7.</span>,  <span class="number">2.</span>,  <span class="number">7.</span>],</span><br><span class="line">       [ <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">0.</span>]])]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.hsplit(a,(<span class="number">3</span>,<span class="number">4</span>))   <span class="comment"># Split a after the third and the fourth column</span></span><br><span class="line">[array([[ <span class="number">9.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">4.</span>,  <span class="number">9.</span>]]), array([[ <span class="number">3.</span>],</span><br><span class="line">       [ <span class="number">2.</span>]]), array([[ <span class="number">6.</span>,  <span class="number">8.</span>,  <span class="number">0.</span>,  <span class="number">7.</span>,  <span class="number">9.</span>,  <span class="number">7.</span>,  <span class="number">2.</span>,  <span class="number">7.</span>],</span><br><span class="line">       [ <span class="number">2.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">6.</span>,  <span class="number">2.</span>,  <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">0.</span>]])]</span><br></pre></td></tr></table></figure>
<h2 id="拷贝和视图"><a href="#拷贝和视图" class="headerlink" title="拷贝和视图"></a>拷贝和视图</h2><p>简单分配不会复制数组对象或其数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.arange(<span class="number">12</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a            <span class="comment"># no new object is created</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b <span class="keyword">is</span> a           <span class="comment"># a and b are two names for the same ndarray object</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.shape = <span class="number">3</span>,<span class="number">4</span>    <span class="comment"># changes the shape of a</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">(<span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>不同的数组对象可以共享相同的数据。该<code>view</code>方法创建一个查看相同数据的新数组对象。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = a.view()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c <span class="keyword">is</span> a</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.base <span class="keyword">is</span> a                        <span class="comment"># c is a view of the data owned by a</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.flags.owndata</span><br><span class="line"><span class="literal">False</span></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.shape = <span class="number">2</span>,<span class="number">6</span>                      <span class="comment"># a&#x27;s shape doesn&#x27;t change</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.shape</span><br><span class="line">(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c[<span class="number">0</span>,<span class="number">4</span>] = <span class="number">1234</span>                      <span class="comment"># a&#x27;s data changes</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([[   <span class="number">0</span>,    <span class="number">1</span>,    <span class="number">2</span>,    <span class="number">3</span>],</span><br><span class="line">       [<span class="number">1234</span>,    <span class="number">5</span>,    <span class="number">6</span>,    <span class="number">7</span>],</span><br><span class="line">       [   <span class="number">8</span>,    <span class="number">9</span>,   <span class="number">10</span>,   <span class="number">11</span>]])</span><br></pre></td></tr></table></figure>
<p>切片数组会返回一个视图:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = a[ : , <span class="number">1</span>:<span class="number">3</span>]     <span class="comment"># spaces added for clarity; could also be written &quot;s = a[:,1:3]&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s[:] = <span class="number">10</span>           <span class="comment"># s[:] is a view of s. Note the difference between s=10 and s[:]=10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([[   <span class="number">0</span>,   <span class="number">10</span>,   <span class="number">10</span>,    <span class="number">3</span>],</span><br><span class="line">       [<span class="number">1234</span>,   <span class="number">10</span>,   <span class="number">10</span>,    <span class="number">7</span>],</span><br><span class="line">       [   <span class="number">8</span>,   <span class="number">10</span>,   <span class="number">10</span>,   <span class="number">11</span>]])</span><br></pre></td></tr></table></figure>
<p>深拷贝。该<code>copy</code>方法生成数组及其数据的完整副本。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = a.copy()                          <span class="comment"># a new array object with new data is created</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d <span class="keyword">is</span> a</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d.base <span class="keyword">is</span> a                           <span class="comment"># d doesn&#x27;t share anything with a</span></span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d[<span class="number">0</span>,<span class="number">0</span>] = <span class="number">9999</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([[   <span class="number">0</span>,   <span class="number">10</span>,   <span class="number">10</span>,    <span class="number">3</span>],</span><br><span class="line">       [<span class="number">1234</span>,   <span class="number">10</span>,   <span class="number">10</span>,    <span class="number">7</span>],</span><br><span class="line">       [   <span class="number">8</span>,   <span class="number">10</span>,   <span class="number">10</span>,   <span class="number">11</span>]])</span><br></pre></td></tr></table></figure>
<p>有时，如果不再需要原始数组，则应在切片后调用 <code>copy</code>。</p>
<h1 id="np-random"><a href="#np-random" class="headerlink" title="np.random"></a>np.random</h1><h2 id="np-random-choice"><a href="#np-random-choice" class="headerlink" title="np.random.choice"></a>np.random.choice</h2><p><code>random.choice(a, size=None, replace=True, p=None)</code></p>
<p>从一个给定1维向量生成一个随机样本。</p>
<ul>
<li><code>a</code> 是一个list或整数，如果是整数则 <code>np.arrange(a)</code>，从这个范围中随机生成。</li>
<li><code>size</code> 是输出的形状。</li>
</ul>
<h1 id="np-expand-dims"><a href="#np-expand-dims" class="headerlink" title="np.expand_dims()"></a>np.expand_dims()</h1><p><code>np.expand_dims(a, axis)</code></p>
<p>扩展 array 的形状，在 axis 处插入一个新轴。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = np.array([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.shape</span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = np.expand_dims(x, axis=<span class="number">1</span>)</span><br><span class="line">y</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>array([[<span class="number">1</span>],</span><br><span class="line">           [<span class="number">2</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.shape</span><br><span class="line">(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = np.expand_dims(x, axis=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y</span><br><span class="line">array([[[<span class="number">1</span>, <span class="number">2</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = np.expand_dims(x, axis=(<span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y</span><br><span class="line">array([[[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>]]])</span><br></pre></td></tr></table></figure>
<h1 id="np-squeeze"><a href="#np-squeeze" class="headerlink" title="np.squeeze()"></a>np.squeeze()</h1><p><code>np.squeeze(a, axis)</code></p>
<p>删除 array 的一个轴。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = np.array([[[<span class="number">0</span>], [<span class="number">1</span>], [<span class="number">2</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.shape</span><br><span class="line">(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.squeeze(x).shape</span><br><span class="line">(<span class="number">3</span>,)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.squeeze(x, axis=<span class="number">0</span>).shape</span><br><span class="line">(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.squeeze(x, axis=<span class="number">1</span>).shape</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">...</span><br><span class="line">ValueError: cannot select an axis to squeeze out which has size <span class="keyword">not</span> equal to one</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.squeeze(x, axis=<span class="number">2</span>).shape</span><br><span class="line">(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = np.array([[<span class="number">1234</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.shape</span><br><span class="line">(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.squeeze(x)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>array(<span class="number">1234</span>)  <span class="comment"># 0d array</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.squeeze(x).shape</span><br><span class="line">()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.squeeze(x)[()]</span><br><span class="line"><span class="number">1234</span></span><br></pre></td></tr></table></figure>
<h1 id="np-meshgrid"><a href="#np-meshgrid" class="headerlink" title="np.meshgrid()"></a>np.meshgrid()</h1><p><code>np.meshgrid(*xi, copy=True, sparse=False, indexing=&#39;xy&#39;)</code></p>
<p>从输入的数组创建一个坐标轴/网格。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>nx, ny = (<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = np.linspace(<span class="number">0</span>, <span class="number">1</span>, nx)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = np.linspace(<span class="number">0</span>, <span class="number">1</span>, ny)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>xv, yv = np.meshgrid(x, y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>xv</span><br><span class="line">array([[<span class="number">0.</span> , <span class="number">0.5</span>, <span class="number">1.</span> ],</span><br><span class="line">       [<span class="number">0.</span> , <span class="number">0.5</span>, <span class="number">1.</span> ]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>yv</span><br><span class="line">array([[<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>
<h1 id="np-where"><a href="#np-where" class="headerlink" title="np.where()"></a>np.where()</h1><p><code>np.where(condition, x, y)</code></p>
<p>根据条件从 <code>x</code> 和 <code>y</code> 中选择元素。如果条件为真选 <code>x</code> 否则 <code>y</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.where([[<span class="literal">True</span>, <span class="literal">False</span>], [<span class="literal">True</span>, <span class="literal">True</span>]],</span><br><span class="line"><span class="meta">... </span>         [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]],</span><br><span class="line"><span class="meta">... </span>         [[<span class="number">9</span>, <span class="number">8</span>], [<span class="number">7</span>, <span class="number">6</span>]])</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">8</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title>OJ模板整理</title>
    <url>/2021/06/24/oj-template/</url>
    <content><![CDATA[<p>快期末oj了，也是最后一学期的oj，整理一下常用的算法模板，既是复习（考试偷看），也是保留一份以后时常用得上的自己习惯的板子。</p>
<a id="more"></a>
<h1 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h1><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> ans = <span class="number">0</span>, l, r;</span><br><span class="line">    <span class="comment">// some input ...</span></span><br><span class="line">    <span class="keyword">while</span> (l &lt;= r) &#123;</span><br><span class="line">        <span class="keyword">int</span> m = (l + r) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (check(m)) &#123;</span><br><span class="line">            ans = m;</span><br><span class="line">            l = m + <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            r = m - <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h1 id="数位dp"><a href="#数位dp" class="headerlink" title="数位dp"></a>数位dp</h1><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">ll <span class="title">solve</span><span class="params">(<span class="keyword">int</span> pos, <span class="keyword">bool</span> flag, ...<span class="comment">/*cur_state*/</span>)</span> </span>&#123;	<span class="comment">// flag 标识会不会超过最大数</span></span><br><span class="line">    <span class="keyword">if</span> (cond) <span class="keyword">return</span> ;</span><br><span class="line">    <span class="keyword">if</span> (flag == <span class="literal">false</span> &amp;&amp; dp[pos][...] != <span class="number">-1</span>) <span class="keyword">return</span> dp[pos][...];</span><br><span class="line">    ll bound = (flag == <span class="literal">true</span> ? (ll)(s[pos - <span class="number">1</span>] - <span class="string">&#x27;0&#x27;</span>) : (ll)<span class="number">9</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> digit = <span class="number">0</span>; digit &lt;= bound; ++i) &#123;</span><br><span class="line">        <span class="comment">// calculate new state</span></span><br><span class="line">        <span class="keyword">bool</span> new_flag = <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (flag == <span class="number">1</span> &amp;&amp; digit == bound) new_flag = <span class="literal">true</span>;</span><br><span class="line">        solve(pos - <span class="number">1</span>, new_flag, ...<span class="comment">/*new_state*/</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="并查集"><a href="#并查集" class="headerlink" title="并查集"></a>并查集</h1><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> fa[MAXN] = &#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; ++i) fa[i] = i;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (x != fa[x]) &#123;</span><br><span class="line">        fa[x] = find(fa[x]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> fa[x];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 一般不使用按秩合并就足够好了</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">    x = find(x);</span><br><span class="line">    y = find(y);</span><br><span class="line">    fa[x] = y;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<h1 id="最小生成树"><a href="#最小生成树" class="headerlink" title="最小生成树"></a>最小生成树</h1><figure class="highlight c++"><figcaption><span>kruskal</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment">// 基本思想是从小到大加入边，是个贪心算法。</span></span><br><span class="line"><span class="comment">// 一般都用kruskal</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Edge</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> from, to, val;</span><br><span class="line">  Edge(<span class="keyword">int</span> from, <span class="keyword">int</span> to, <span class="keyword">int</span> val)</span><br><span class="line">      : from(from), to(to), val(val) &#123;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">bool</span> <span class="keyword">operator</span>&lt;(<span class="keyword">const</span> struct Edge &amp;rhs) <span class="keyword">const</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> val &lt; rhs.val;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="built_in">vector</span>&lt;Edge&gt; edges;</span><br><span class="line"><span class="keyword">int</span> fa[MAXN] = &#123;&#125;;	<span class="comment">// 维护点集合</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kruskal</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> cnt = <span class="number">0</span>, ans = <span class="number">0</span>;</span><br><span class="line">    sort(edges.begin(), edges.end());</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> &amp;e: edges) &#123;</span><br><span class="line">        <span class="keyword">if</span> (cnt &gt;= n) <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">if</span> (find(e.from) != find(e.to)) &#123;</span><br><span class="line">            merge(e.from, e.to);</span><br><span class="line">            ans += e.val;</span><br><span class="line">            ++cnt;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 读入</span></span><br><span class="line"><span class="comment">for (int i = 1; i &lt;= m; ++i) &#123;</span></span><br><span class="line"><span class="comment">    int p, q, val;</span></span><br><span class="line"><span class="comment">    scanf(&quot;%d%d%d&quot;, &amp;p, &amp;q, &amp;val);</span></span><br><span class="line"><span class="comment">    edges.emplace_back(Edge(p, q, val));</span></span><br><span class="line"><span class="comment">    edges.emplace_back(Edge(q, p, val));</span></span><br><span class="line"><span class="comment">&#125;*/</span></span><br></pre></td></tr></table></figure>
<h1 id="最短路"><a href="#最短路" class="headerlink" title="最短路"></a>最短路</h1><h2 id="单源最短路"><a href="#单源最短路" class="headerlink" title="单源最短路"></a>单源最短路</h2><figure class="highlight c++"><figcaption><span>dijkstra</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">pair</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt; edges;</span><br><span class="line"><span class="keyword">bool</span> vis[MAXN] = &#123;&#125;;</span><br><span class="line"><span class="keyword">int</span> cost[MAXN] = &#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dijkstra</span><span class="params">(<span class="keyword">int</span> src)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">priority_queue</span>&lt;<span class="built_in">pair</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;&gt; pq;	<span class="comment">// 用于记录到达每个点的消耗，pair第一分量为cost的相反数，第二分量为到达的点的编号</span></span><br><span class="line">    cost[src] = <span class="number">0</span>;</span><br><span class="line">    pq.emplace(<span class="number">0</span>, src);</span><br><span class="line">    <span class="keyword">while</span> (!pq.empty()) &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">auto</span> state = pq.top(); pq.pop();</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> cur_cost = state.first;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> cur_idx  = state.second;</span><br><span class="line">        <span class="keyword">if</span> (vis[cur_idx]) <span class="keyword">continue</span>;</span><br><span class="line">        vis[cur_idx] = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span> &amp;edge: edges[cur_idx]) &#123;</span><br><span class="line">            <span class="keyword">const</span> <span class="keyword">int</span> new_cost = cur_cost - edge.second;	<span class="comment">// 因为对cost取负，所以用减法</span></span><br><span class="line">            <span class="keyword">const</span> <span class="keyword">int</span> new_idx  = edge.first;</span><br><span class="line">            <span class="keyword">if</span> (new_cost &gt; cost[new_idx]) &#123;</span><br><span class="line">                pq.push(&#123;new_cost, new_idx&#125;);</span><br><span class="line">                cost[new_idx] = new_cost;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><figcaption><span>多状态版dijkstra</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> &#123;</span>...&#125; State;</span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">int</span> Value;</span><br><span class="line"></span><br><span class="line">pair&lt;Value, State&gt; get_next(const State current, int idx) &#123;</span><br><span class="line">    <span class="comment">//return new state</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">dijkstra</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">priority_queue</span>&lt;<span class="built_in">pair</span>&lt;Value, State&gt;&gt; pq;	<span class="comment">// 原来second只记录点编号，现在用来表示更多的状态，相当于增加维度（分层图）</span></span><br><span class="line">    cost[src] = <span class="number">0</span>;</span><br><span class="line">    pq.emplace(<span class="number">0</span>, src);</span><br><span class="line">    <span class="keyword">while</span>(!pq.empty()) &#123;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">auto</span> [cur_cost, cur_state] = pq.top(); pq.pop();</span><br><span class="line">        <span class="keyword">if</span>(vis[cur_state]) <span class="keyword">continue</span>;</span><br><span class="line">        vis[cur_state] = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">const</span> <span class="keyword">auto</span>&amp; edge : edges[cur_idx]) &#123;</span><br><span class="line">            <span class="keyword">const</span> <span class="keyword">auto</span> [new_cost, new_state] = get_next(cur_state, edge);</span><br><span class="line">            <span class="keyword">if</span>(new_cost &gt; cost[new_state]) &#123;</span><br><span class="line">                <span class="comment">// pq.push</span></span><br><span class="line">                <span class="comment">// cost[cur_state] =</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="多源最短路"><a href="#多源最短路" class="headerlink" title="多源最短路"></a>多源最短路</h2><figure class="highlight c++"><figcaption><span>floyd</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">floyd</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> k = <span class="number">1</span>; k &lt;= n; ++k) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; ++i) &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= n; ++j)&#123;</span><br><span class="line">                <span class="keyword">if</span>(dis[i][j] &gt; dis[i][k] + dis[k][j]) &#123;</span><br><span class="line">                    dis[i][j] = dis[i][k] + dis[k][j];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="连通分量"><a href="#连通分量" class="headerlink" title="连通分量"></a>连通分量</h1><p>说实话tarjan我一直没搞懂<span class="github-emoji" alias="sob" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">&#x1f62d;</span>，直接搬助教的模板了。。</p>
<figure class="highlight c++"><figcaption><span>tarjan</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">bool</span> vis[MAXN] = &#123;&#125;;</span><br><span class="line"><span class="keyword">int</span> low[MAXN] = &#123;&#125;;</span><br><span class="line"><span class="keyword">int</span> dfn[MAXN] = &#123;&#125;;</span><br><span class="line"><span class="keyword">int</span> belong[MAXN] = &#123;&#125;;	<span class="comment">// 第i个点属于第几个连通分量</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> pos, <span class="keyword">int</span> pre)</span> </span>&#123;                     <span class="comment">// pos：当前位置，pre：上一个位置</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">int</span> ts = <span class="number">0</span>;                           <span class="comment">// 利用C++的static特性声明一个只有函数内有效的变量，也可以用全局变量</span></span><br><span class="line">    vis[pos] = <span class="literal">true</span>;                             <span class="comment">// vis也可以叫做instack，用途是判断当前点是否在栈内</span></span><br><span class="line">    s.emplace(pos);                              <span class="comment">// 将当前节点压入栈</span></span><br><span class="line">    low[pos] = dfn[pos] = ++ts;                  <span class="comment">// 访问新的节点，更新时间戳</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> &amp;nex : edges[pos]) &#123;               <span class="comment">// 便利所有可能的下一个节点</span></span><br><span class="line">        <span class="keyword">if</span> (dfn[nex] == <span class="number">0</span>) &#123;</span><br><span class="line">            dfs(nex, pos);                       <span class="comment">// 如果还没有访问过，则递归访问并更新最小节点编号</span></span><br><span class="line">            low[pos] = min(low[pos], low[nex]);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (vis[nex] <span class="keyword">and</span> nex != pre) &#123;          <span class="comment">// 这里判断vis的原因是要处理单向边的情况，如果dfn&gt;0但!vis，则说明只可以单向访问，一定无法和这个点在同一个强连通分量中</span></span><br><span class="line">            <span class="comment">// 如果有向图则不需要判定 nex != pre</span></span><br><span class="line">                low[pos] = min(low[pos], dfn[nex]); <span class="comment">// 如果已经访问过了，则判断是否是更小的节点编号，不需要递归访问</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (low[pos] == dfn[pos] <span class="keyword">and</span> vis[pos]) &#123;        <span class="comment">// 如果在栈内部且low=dfn，那么就找到了一个新的强连通分量</span></span><br><span class="line">        ++cnt;                                      <span class="comment">// 强连通分量的总数加一</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">not</span> s.empty() <span class="keyword">and</span> s.top() != pos) &#123;  <span class="comment">// 不断弹出栈中的元素，标记当前的强连通分量的编号</span></span><br><span class="line">            belong[s.top()] = cnt;</span><br><span class="line">            vis[s.top()] = <span class="literal">false</span>;</span><br><span class="line">            s.pop();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">not</span> s.empty()) s.pop();</span><br><span class="line">        belong[pos] = cnt, vis[pos] = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 总之tarjan可以把所有的点分到强连通分量里去</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> deg[MAXN] = &#123;&#125;;	<span class="comment">// 分量的度</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> &amp;j : edges[i]) &#123;</span><br><span class="line">        <span class="keyword">if</span> (belong[i] != belong[j]) &#123;  <span class="comment">// 如果某一条边的两个顶点不在同一个分量中</span></span><br><span class="line">            ++deg[belong[i]];          <span class="comment">// i对应的分量出度+1</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="完美匹配"><a href="#完美匹配" class="headerlink" title="完美匹配"></a>完美匹配</h1><figure class="highlight c++"><figcaption><span>hungarian</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">bool</span> used[MAXN] = &#123;&#125;;	<span class="comment">//unordered_set&lt;int&gt; used;</span></span><br><span class="line"><span class="keyword">int</span> belong&#123;MAXN&#125; = &#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> &amp;y: edges[x]) &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">not</span> used[y]) &#123;</span><br><span class="line">            used[y] = <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">if</span> (belong[y] == <span class="number">0</span> <span class="keyword">or</span> find(belong[y])) &#123;</span><br><span class="line">                belong[y] = x;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">hungarian</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> tot = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; ++i) &#123;</span><br><span class="line">        used.clear();</span><br><span class="line">        <span class="keyword">if</span> (find(x)) &#123;</span><br><span class="line">            tot++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> tot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="网络流"><a href="#网络流" class="headerlink" title="网络流"></a>网络流</h1><p>网络流Dinic我其实也没搞懂qvp，照搬oiwiki了<span class="github-emoji" alias="cry" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f622.png?v8">&#x1f622;</span>。</p>
<figure class="highlight c++"><figcaption><span>Dinic</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Edge</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> from, to, cap, flow;</span><br><span class="line">  Edge(<span class="keyword">int</span> u, <span class="keyword">int</span> v, <span class="keyword">int</span> c, <span class="keyword">int</span> f) : from(u), to(v), cap(c), flow(f) &#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Dinic</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> n, m, s, t;</span><br><span class="line">  <span class="built_in">vector</span>&lt;Edge&gt; edges;</span><br><span class="line">  <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; G[maxn];</span><br><span class="line">  <span class="keyword">int</span> d[maxn], cur[maxn];</span><br><span class="line">  <span class="keyword">bool</span> vis[maxn];</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) G[i].clear();</span><br><span class="line">    edges.clear();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">AddEdge</span><span class="params">(<span class="keyword">int</span> from, <span class="keyword">int</span> to, <span class="keyword">int</span> cap)</span> </span>&#123;</span><br><span class="line">    edges.push_back(Edge(from, to, cap, <span class="number">0</span>));</span><br><span class="line">    edges.push_back(Edge(to, from, <span class="number">0</span>, <span class="number">0</span>));</span><br><span class="line">    m = edges.size();</span><br><span class="line">    G[from].push_back(m - <span class="number">2</span>);</span><br><span class="line">    G[to].push_back(m - <span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">BFS</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">memset</span>(vis, <span class="number">0</span>, <span class="keyword">sizeof</span>(vis));</span><br><span class="line">    <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; Q;</span><br><span class="line">    Q.push(s);</span><br><span class="line">    d[s] = <span class="number">0</span>;</span><br><span class="line">    vis[s] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (!Q.empty()) &#123;</span><br><span class="line">      <span class="keyword">int</span> x = Q.front();</span><br><span class="line">      Q.pop();</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; G[x].size(); i++) &#123;</span><br><span class="line">        Edge&amp; e = edges[G[x][i]];</span><br><span class="line">        <span class="keyword">if</span> (!vis[e.to] &amp;&amp; e.cap &gt; e.flow) &#123;</span><br><span class="line">          vis[e.to] = <span class="number">1</span>;</span><br><span class="line">          d[e.to] = d[x] + <span class="number">1</span>;</span><br><span class="line">          Q.push(e.to);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> vis[t];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">int</span> <span class="title">DFS</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> a)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (x == t || a == <span class="number">0</span>) <span class="keyword">return</span> a;</span><br><span class="line">    <span class="keyword">int</span> flow = <span class="number">0</span>, f;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span>&amp; i = cur[x]; i &lt; G[x].size(); i++) &#123;</span><br><span class="line">      Edge&amp; e = edges[G[x][i]];</span><br><span class="line">      <span class="keyword">if</span> (d[x] + <span class="number">1</span> == d[e.to] &amp;&amp; (f = DFS(e.to, min(a, e.cap - e.flow))) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        e.flow += f;</span><br><span class="line">        edges[G[x][i] ^ <span class="number">1</span>].flow -= f;</span><br><span class="line">        flow += f;</span><br><span class="line">        a -= f;</span><br><span class="line">        <span class="keyword">if</span> (a == <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> flow;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">int</span> <span class="title">Maxflow</span><span class="params">(<span class="keyword">int</span> s, <span class="keyword">int</span> t)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>-&gt;s = s;</span><br><span class="line">    <span class="keyword">this</span>-&gt;t = t;</span><br><span class="line">    <span class="keyword">int</span> flow = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (BFS()) &#123;</span><br><span class="line">      <span class="built_in">memset</span>(cur, <span class="number">0</span>, <span class="keyword">sizeof</span>(cur));</span><br><span class="line">      flow += DFS(s, INF);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> flow;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h1 id="数论"><a href="#数论" class="headerlink" title="数论"></a>数论</h1><figure class="highlight c++"><figcaption><span>gcd</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function">ll <span class="title">exgcd</span><span class="params">(ll a, ll b, ll &amp;x, ll &amp;y)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (b == <span class="number">0</span>) &#123;</span><br><span class="line">    x = <span class="number">1LL</span>, y = <span class="number">0LL</span>;</span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">  &#125;</span><br><span class="line">  ll r = exgcd(b, a % b, y, x);</span><br><span class="line">  y -= (a / b) * x;</span><br><span class="line">  <span class="keyword">return</span> r;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><figcaption><span>快速幂</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function">ll <span class="title">qmul</span><span class="params">(ll a, ll b, ll p)</span> </span>&#123;</span><br><span class="line">  ll ret = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span> (b) &#123;</span><br><span class="line">    <span class="keyword">if</span> (b &amp; <span class="number">1</span>) &#123;</span><br><span class="line">      ret = (ret + a) % p;</span><br><span class="line">    &#125;</span><br><span class="line">    a = (a + a) % p;</span><br><span class="line">    b &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">ll <span class="title">qpow</span><span class="params">(ll x, ll n, ll p)</span> </span>&#123;</span><br><span class="line">  ll ret = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">while</span> (n) &#123;</span><br><span class="line">    <span class="keyword">if</span> (n &amp; <span class="number">1</span>) &#123;</span><br><span class="line">      ret = ret * x % p;</span><br><span class="line">    &#125;</span><br><span class="line">    x = x * x % p;</span><br><span class="line">    n &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><figcaption><span>乘法逆元</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function">ll <span class="title">inverse</span><span class="params">(ll x, ll p)</span> </span>&#123;	<span class="comment">// p为质数</span></span><br><span class="line">  <span class="keyword">return</span> qpow(x, p - <span class="number">2</span>, p);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">ll <span class="title">inverse</span><span class="params">(ll a, ll b)</span> </span>&#123;	<span class="comment">// b不为质数</span></span><br><span class="line">    ll x, y;</span><br><span class="line">    exgcd(a, b, x, y);</span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><figcaption><span>组合数</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function">ll <span class="title">choose</span><span class="params">(ll n, ll m, ll p)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (m == <span class="number">0</span> || m == n) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  ll x = <span class="number">1</span>, y = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">for</span> (ll i = n; i &gt; n - m; --i) &#123;</span><br><span class="line">    x = x * i % p;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span> (ll i = <span class="number">1</span>; i &lt;= m; ++i) &#123;</span><br><span class="line">    y = y * i % p;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> x * inverse(y, p) % p;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 也可以递推动态规划</span></span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><figcaption><span>卢卡斯定理</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function">ll <span class="title">lucas</span><span class="params">(ll n, ll m, ll p)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (m == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> choose(n % p, m % p, p) * lucas(n / p, m / p, p) % p;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><figcaption><span>中国剩余定理</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function">ll <span class="title">crt</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;ll&gt; &amp;a, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;ll&gt; &amp;m, ll p)</span> </span>&#123;</span><br><span class="line">  ll ret = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>, n = a.size(); i &lt; n; ++i) &#123;</span><br><span class="line">    ll w = p / m[i], x, y;</span><br><span class="line">    exgcd(w, m[i], x, y);</span><br><span class="line">    x = (x % m[i] + m[i]) % m[i];</span><br><span class="line">    ret = (ret + qmul(qmul(a[i], w, p), x, p)) % p;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> (ret + p) % p;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><figcaption><span>欧拉函数打表</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> ans[MAXN] = &#123;<span class="number">1</span>&#125;;</span><br><span class="line"><span class="keyword">bool</span> fact[MAXN] = &#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; MAXN; ++i)</span><br><span class="line">	ans[i]=i;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; MAXN; ++i) &#123;</span><br><span class="line">    <span class="keyword">if</span>(fact[i]) <span class="keyword">continue</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = i; j &lt; MAXN; j += i)&#123;</span><br><span class="line">        fact[j] = <span class="number">1</span>;</span><br><span class="line">        ans[j] /= i;</span><br><span class="line">        ans[j] *= i<span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><figcaption><span>素数判定</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">check</span><span class="params">(ll a,ll n,ll x,ll t)</span></span>&#123;   <span class="comment">//以a为基，n-1=x*2^t，检验n是不是合数</span></span><br><span class="line">    ll ret=pow_mod(a,x,n),last=ret;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=t;i++)&#123;</span><br><span class="line">        ret=muti_mod(ret,ret,n);</span><br><span class="line">        <span class="keyword">if</span> (ret==<span class="number">1</span> &amp;&amp; last!=<span class="number">1</span> &amp;&amp; last!=n<span class="number">-1</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        last=ret;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (ret!=<span class="number">1</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">Miller_Rabin</span><span class="params">(ll n)</span> </span>&#123;</span><br><span class="line">    ll x=n<span class="number">-1</span>,t=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> ((x&amp;<span class="number">1</span>)==<span class="number">0</span>) x&gt;&gt;=<span class="number">1</span>,t++;</span><br><span class="line">    <span class="keyword">bool</span> flag=<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span> (t&gt;=<span class="number">1</span> &amp;&amp; (x&amp;<span class="number">1</span>)==<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> k=<span class="number">0</span>;k&lt;S;k++)&#123;</span><br><span class="line">            ll a=rand()%(n<span class="number">-1</span>)+<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span> (check(a,n,x,t)) &#123;flag=<span class="number">1</span>;<span class="keyword">break</span>;&#125;</span><br><span class="line">            flag=<span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!flag || n==<span class="number">2</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="字符串匹配"><a href="#字符串匹配" class="headerlink" title="字符串匹配"></a>字符串匹配</h1><figure class="highlight c++"><figcaption><span>kmp</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">char</span> s1[]; <span class="keyword">int</span> len1;	<span class="comment">// 主串</span></span><br><span class="line"><span class="keyword">char</span> s2[]; <span class="keyword">int</span> len2;	<span class="comment">// 模式串</span></span><br><span class="line"><span class="keyword">int</span> nxt[] = &#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">build_nxt</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">1</span>, now = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (i &lt; len2) &#123;</span><br><span class="line">        <span class="keyword">if</span> (s2[i] == s2[now]) &#123;</span><br><span class="line">            now++;</span><br><span class="line">            nxt[i] = now;</span><br><span class="line">            i++;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (now != <span class="number">0</span>) &#123;</span><br><span class="line">            now = nxt[now - <span class="number">1</span>];</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            i++;</span><br><span class="line">            nxt[i] = now;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">kmp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> tar = <span class="number">0</span>, pos = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (tar &lt; len1) &#123;</span><br><span class="line">        <span class="keyword">if</span> (s1[tar] == s2[pos]) &#123;</span><br><span class="line">            tar++;</span><br><span class="line">            pos++;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (pos != <span class="number">0</span>) &#123;</span><br><span class="line">            pos = nxt[pos - <span class="number">1</span>];</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            tar++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (pos == len2) &#123;</span><br><span class="line">            <span class="comment">// find an answer = tar - pos + 1</span></span><br><span class="line">            pos = nxt[pos - <span class="number">1</span>];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="模拟退火"><a href="#模拟退火" class="headerlink" title="模拟退火"></a>模拟退火</h1><p>考试整不出来只能玄学调参了<span class="github-emoji" alias="joy" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f602.png?v8">&#x1f602;</span>。。。</p>
<figure class="highlight c++"><figcaption><span>simulateAnneal</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdlib&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ctime&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">Rand</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> (<span class="keyword">double</span>)rand() / RAND_MAX; &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> dis, ansx;</span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">cal</span><span class="params">(<span class="keyword">double</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">double</span> res = <span class="number">0</span>;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span> (res &lt; dis) dis = res, ansx = x;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">simulateAnneal</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">double</span> T = T_init;</span><br><span class="line">    <span class="keyword">double</span> cur_x = ansx;</span><br><span class="line">    <span class="keyword">while</span> (T &gt; T_min) &#123;</span><br><span class="line">        <span class="keyword">double</span> new_x = get_next_state(cur_x, t);</span><br><span class="line">        <span class="keyword">double</span> delta = cal(new_x) - cal(cur_x);</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">exp</span>(-delta / T) &gt; Rand()) cur_x = new_x;	<span class="comment">// 具体看求min还是max</span></span><br><span class="line">        <span class="comment">//max: if (delta &gt; 0 || exp(delta / T) &gt; Rand())</span></span><br><span class="line">        T *= rate;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10000</span>; ++i) &#123;	<span class="comment">// 小温度跑一跑找到局部最优</span></span><br><span class="line">        <span class="keyword">double</span> new_x = get_next_state(ans_x, T);</span><br><span class="line">        cal(new_x);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">int main() &#123;</span></span><br><span class="line"><span class="comment">    srand(time(0));</span></span><br><span class="line"><span class="comment">    init(ansx); dis = cal(ansx);</span></span><br><span class="line"><span class="comment">    simulateAnneal();</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://oiwiki.org/">oiwiki</a></p>
<p><a href="https://mengzelev.github.io/2019/04/19/oj-templates/">学姐blog</a></p>
<p><a href="https://oj-solutions.njujb.com/">oj题解</a></p>
]]></content>
      <categories>
        <category>课程</category>
        <category>problem-solving</category>
      </categories>
  </entry>
  <entry>
    <title>pandas</title>
    <url>/2021/03/06/pandas/</url>
    <content><![CDATA[<p>本文主要是对于python库<code>pandas</code>的学习使用笔记。<br>参考网站：<br><a href="https://pandas.pydata.org/">pandas官方网站</a><br><a href="https://www.pypandas.cn/">一个不错的pandas中文网站</a><br><a id="more"></a></p>
<h1 id="pandas是啥"><a href="#pandas是啥" class="headerlink" title="pandas是啥"></a>pandas是啥</h1><p>这个库主要是用来干啥的？<br>Python在数据处理和准备方面一直做得很好，pandas主要用来做数据分析和建模而不必切换到更特定于领域的语言，如R。<br>方便数据的读写，智能数据对齐和丢失数据的综合处理，数据集的灵活调整和旋转，基于智能标签的切片、花式索引和大型数据集的子集。。。</p>
<hr>
<h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><p>pandas最重要的就是它的数据结构：<br><a href="https://pandas.pydata.org/pandas-docs/stable/reference/series.html">Series</a>: 带标签的一维同构数组<br><a href="https://pandas.pydata.org/pandas-docs/stable/reference/frame.html">DataFrame</a>: 带标签的，大小可变的，二维异构表格</p>
<h2 id="Series"><a href="#Series" class="headerlink" title="Series"></a>Series</h2><figure class="highlight python"><figcaption><span>创建Series</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = pd.Series(data, index=index) <span class="comment">#index: 索引名字</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>data</code>是多维数组时，<code>index</code>长度必须与<code>data</code>长度一致，默认<code>[0, 1, ..., len(data)-1]</code><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = pd.Series(np.random.randn(<span class="number">5</span>), index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s </span><br><span class="line">a    <span class="number">0.469112</span></span><br><span class="line">b   -<span class="number">0.282863</span></span><br><span class="line">c   -<span class="number">1.509059</span></span><br><span class="line">d   -<span class="number">1.135632</span></span><br><span class="line">e    <span class="number">1.212112</span></span><br><span class="line">dtype: float64</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.index</span><br><span class="line">Index([<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>], dtype=<span class="string">&#x27;object&#x27;</span>)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;&gt; s.dtype</span><br><span class="line">dtype(<span class="string">&#x27;float64&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
<li><code>data</code>是字典时，直接key-value，按插入顺序排列</li>
<li><p><code>data</code>是标量，按索引重复</p>
</li>
<li><p><code>Series</code>操作与<code>ndarray</code>类似，支持大多数 NumPy 函数，还支持索引切片。</p>
</li>
</ul>
<h3 id="支持矢量操作"><a href="#支持矢量操作" class="headerlink" title="支持矢量操作"></a>支持矢量操作</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.exp(s)</span><br><span class="line">a         <span class="number">1.598575</span></span><br><span class="line">b         <span class="number">0.753623</span></span><br><span class="line">c         <span class="number">0.221118</span></span><br><span class="line">d         <span class="number">0.321219</span></span><br><span class="line">e    <span class="number">162754.791419</span></span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure>
<ul>
<li>Series 和多维数组的主要区别在于， Series 之间的操作会自动基于标签对齐数据。因此，不用顾及执行计算操作的 Series 是否有相同的标签。</li>
<li>操作未对齐索引的 Series， 其计算结果是所有涉及索引的并集。如果在 Series 里找不到标签，运算结果标记为<code>NaN</code>，即缺失值。</li>
</ul>
<h3 id="名称"><a href="#名称" class="headerlink" title="名称"></a>名称</h3><p><code>Series</code> 支持 <code>name</code>属性，就是这列数据的名称。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = pd.Series(np.random.randn(<span class="number">5</span>), name=<span class="string">&#x27;something&#x27;</span>)</span><br><span class="line">s2 = s.rename(<span class="string">&quot;different&quot;</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><p><code>DataFrame</code> 是由多种类型的列构成的二维标签数据结构，类似于 Excel 、SQL 表，或 Series 对象构成的字典。</p>
<h3 id="用-Series-字典生成-DataFrame"><a href="#用-Series-字典生成-DataFrame" class="headerlink" title="用 Series 字典生成 DataFrame"></a>用 Series 字典生成 DataFrame</h3><ul>
<li>生成的索引是每个 Series 索引的并集。先把嵌套字典转换为 Series。如果没有指定列，DataFrame 的列就是字典键的有序列表。</li>
<li>index 和 columns 属性分别用于访问行、列标签：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = &#123;<span class="string">&#x27;one&#x27;</span>: pd.Series([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]), <span class="string">&#x27;two&#x27;</span>: pd.Series([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>], index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>])&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.DataFrame(d)</span><br><span class="line">   one  two</span><br><span class="line">a  <span class="number">1.0</span>  <span class="number">1.0</span></span><br><span class="line">b  <span class="number">2.0</span>  <span class="number">2.0</span></span><br><span class="line">c  <span class="number">3.0</span>  <span class="number">3.0</span></span><br><span class="line">d  NaN  <span class="number">4.0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.DataFrame(d, index=[<span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>])</span><br><span class="line">   one  two</span><br><span class="line">d  NaN  <span class="number">4.0</span></span><br><span class="line">b  <span class="number">2.0</span>  <span class="number">2.0</span></span><br><span class="line">a  <span class="number">1.0</span>  <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.DataFrame(d, index=[<span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>], columns=[<span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;three&#x27;</span>]) </span><br><span class="line">   two three</span><br><span class="line">d  <span class="number">4.0</span>   NaN</span><br><span class="line">b  <span class="number">2.0</span>   NaN</span><br><span class="line">a  <span class="number">1.0</span>   NaN</span><br></pre></td></tr></table></figure>
<h3 id="备选构建器"><a href="#备选构建器" class="headerlink" title="备选构建器"></a>备选构建器</h3><ul>
<li>DataFrame.from_dict</li>
<li>DataFrame.from_records</li>
</ul>
<h3 id="提取、添加、删除列"><a href="#提取、添加、删除列" class="headerlink" title="提取、添加、删除列"></a>提取、添加、删除列</h3><p>就像带索引的Series字典<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 提取</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span> df[<span class="string">&#x27;one&#x27;</span>]</span><br><span class="line">a    <span class="number">1.0</span></span><br><span class="line">b    <span class="number">2.0</span></span><br><span class="line">c    <span class="number">3.0</span></span><br><span class="line">d    NaN</span><br><span class="line">Name: one, dtype: float64</span><br><span class="line"><span class="comment"># 添加</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df[<span class="string">&#x27;three&#x27;</span>] = df[<span class="string">&#x27;one&#x27;</span>] * df[<span class="string">&#x27;two&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df[<span class="string">&#x27;flag&#x27;</span>] = df[<span class="string">&#x27;one&#x27;</span>] &gt; <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df</span><br><span class="line">   one  two  three   flag</span><br><span class="line">a  <span class="number">1.0</span>  <span class="number">1.0</span>    <span class="number">1.0</span>  <span class="literal">False</span></span><br><span class="line">b  <span class="number">2.0</span>  <span class="number">2.0</span>    <span class="number">4.0</span>  <span class="literal">False</span></span><br><span class="line">c  <span class="number">3.0</span>  <span class="number">3.0</span>    <span class="number">9.0</span>   <span class="literal">True</span></span><br><span class="line">d  NaN  <span class="number">4.0</span>    NaN  <span class="literal">False</span></span><br><span class="line"><span class="comment"># 删除</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">del</span> df[<span class="string">&#x27;two&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>three = df.pop(<span class="string">&#x27;three&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df</span><br><span class="line">   one   flag</span><br><span class="line">a  <span class="number">1.0</span>  <span class="literal">False</span></span><br><span class="line">b  <span class="number">2.0</span>  <span class="literal">False</span></span><br><span class="line">c  <span class="number">3.0</span>   <span class="literal">True</span></span><br><span class="line">d  NaN  <span class="literal">False</span></span><br><span class="line"><span class="comment"># 标量重复</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df[<span class="string">&#x27;foo&#x27;</span>] = <span class="string">&#x27;bar&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df</span><br><span class="line">   one   flag  foo</span><br><span class="line">a  <span class="number">1.0</span>  <span class="literal">False</span>  bar</span><br><span class="line">b  <span class="number">2.0</span>  <span class="literal">False</span>  bar</span><br><span class="line">c  <span class="number">3.0</span>   <span class="literal">True</span>  bar</span><br><span class="line">d  NaN  <span class="literal">False</span>  bar</span><br><span class="line"><span class="comment"># 插入与 DataFrame 索引不同的 Series 时，以 DataFrame 的索引为准</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df[<span class="string">&#x27;one_trunc&#x27;</span>] = df[<span class="string">&#x27;one&#x27;</span>][:<span class="number">2</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df</span><br><span class="line">   one   flag  foo  one_trunc</span><br><span class="line">a  <span class="number">1.0</span>  <span class="literal">False</span>  bar        <span class="number">1.0</span></span><br><span class="line">b  <span class="number">2.0</span>  <span class="literal">False</span>  bar        <span class="number">2.0</span></span><br><span class="line">c  <span class="number">3.0</span>   <span class="literal">True</span>  bar        NaN</span><br><span class="line">d  NaN  <span class="literal">False</span>  bar        NaN</span><br><span class="line"><span class="comment"># 可以插入原生多维数组，但长度必须与 DataFrame 索引长度一致。默认在 DataFrame 尾部插入列。insert 函数可以指定插入列的位置</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.insert(<span class="number">1</span>, <span class="string">&#x27;bar&#x27;</span>, df[<span class="string">&#x27;one&#x27;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>df</span><br><span class="line">   one  bar   flag  foo  one_trunc</span><br><span class="line">a  <span class="number">1.0</span>  <span class="number">1.0</span>  <span class="literal">False</span>  bar        <span class="number">1.0</span></span><br><span class="line">b  <span class="number">2.0</span>  <span class="number">2.0</span>  <span class="literal">False</span>  bar        <span class="number">2.0</span></span><br><span class="line">c  <span class="number">3.0</span>  <span class="number">3.0</span>   <span class="literal">True</span>  bar        NaN</span><br><span class="line">d  NaN  NaN  <span class="literal">False</span>  bar        NaN</span><br></pre></td></tr></table></figure></p>
<h3 id="asign分配新链"><a href="#asign分配新链" class="headerlink" title="asign分配新链"></a>asign分配新链</h3><ul>
<li><code>assign</code> 返回的都是副本数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>dfa = pd.DataFrame(&#123;<span class="string">&quot;A&quot;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="string">&quot;B&quot;</span>: [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dfa.assign(C=dfa[<span class="string">&#x27;A&#x27;</span>] + dfa[<span class="string">&#x27;B&#x27;</span>], D=<span class="keyword">lambda</span> x: x[<span class="string">&#x27;A&#x27;</span>] + x[<span class="string">&#x27;C&#x27;</span>])</span><br><span class="line">   A  B  C   D</span><br><span class="line"><span class="number">0</span>  <span class="number">1</span>  <span class="number">4</span>  <span class="number">5</span>   <span class="number">6</span></span><br><span class="line"><span class="number">1</span>  <span class="number">2</span>  <span class="number">5</span>  <span class="number">7</span>   <span class="number">9</span></span><br><span class="line"><span class="number">2</span>  <span class="number">3</span>  <span class="number">6</span>  <span class="number">9</span>  <span class="number">12</span></span><br></pre></td></tr></table></figure>
<h3 id="索引，选择"><a href="#索引，选择" class="headerlink" title="索引，选择"></a>索引，选择</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">操作</th>
<th style="text-align:left">句法</th>
<th style="text-align:left">结果</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">选择列</td>
<td style="text-align:left">df[col]</td>
<td style="text-align:left">Series</td>
</tr>
<tr>
<td style="text-align:left">用标签选择行</td>
<td style="text-align:left">df.loc[label]</td>
<td style="text-align:left">Series</td>
</tr>
<tr>
<td style="text-align:left">用整数位置选择行</td>
<td style="text-align:left">df.iloc[loc]</td>
<td style="text-align:left">Series</td>
</tr>
<tr>
<td style="text-align:left">行切片</td>
<td style="text-align:left">df[5:10]</td>
<td style="text-align:left">DataFrame</td>
</tr>
<tr>
<td style="text-align:left">用布尔向量选择行</td>
<td style="text-align:left">df[bool_vec]</td>
<td style="text-align:left">DataFrame</td>
</tr>
</tbody>
</table>
</div>
<p>选择行返回 Series，索引是 DataFrame 的列：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>df.loc[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">one              <span class="number">2</span></span><br><span class="line">bar              <span class="number">2</span></span><br><span class="line">flag         <span class="literal">False</span></span><br><span class="line">foo            bar</span><br><span class="line">one_trunc        <span class="number">2</span></span><br><span class="line">Name: b, dtype: <span class="built_in">object</span></span><br></pre></td></tr></table></figure></p>
<h3 id="数据对齐和运算"><a href="#数据对齐和运算" class="headerlink" title="数据对齐和运算"></a>数据对齐和运算</h3><ul>
<li>DataFrame 对象可以自动对齐列与索引（行标签）的数据，生成的结果是列和行标签的并集。</li>
<li>DataFrame 和 Series 之间执行操作时，默认操作是在 DataFrame 的列上对齐 Series 的索引，按行执行广播操作。</li>
<li>时间序列是特例，DataFrame 索引包含日期时，按列广播</li>
</ul>
<h3 id="转置"><a href="#转置" class="headerlink" title="转置"></a>转置</h3><p><code>df[:5].T</code></p>
<h3 id="DataFrame-应用-Numpy-函数"><a href="#DataFrame-应用-Numpy-函数" class="headerlink" title="DataFrame 应用 Numpy 函数"></a>DataFrame 应用 Numpy 函数</h3><ul>
<li>Series 与 DataFrame 可使用 log、exp、sqrt 等多种元素级 NumPy 通用函数（ufunc）</li>
<li>DataFrame 不是多维数组的替代品，它的索引语义和数据模型与多维数组都不同。</li>
</ul>
<h1 id="怎么总结呢？"><a href="#怎么总结呢？" class="headerlink" title="怎么总结呢？"></a>怎么总结呢？</h1><p>我一直在思考怎么来总结这些库的技巧，毕竟不可能对于每个方法API和参数都详尽地记录下来，我们有手册可以 <strong>rtfm</strong>，把手册再抄一遍没有意义。但是每次在用到一些方法时还是很难很快反应过来怎么用，或者不知道一些常用的方法来解决一些问题。</p>
<p>所以我选择用一种问题或任务驱动的方式来进行总结，在编程的时候遇到一些任务，或者在阅读优秀代码时看到的一些数据结构和方法参数时，看不懂就查手册总结下来，得到一些常用的方法。</p>
<p>这个方法不止在本篇 <code>pandas</code> 中，在其他几个包的教程中也如此。</p>
<h1 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h1>]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title>问题求解笔记-置换群和拉格朗日定理</title>
    <url>/2021/03/07/permutation-group-and-lagrange-theorem/</url>
    <content><![CDATA[<h1 id="置换群-Permutation-Groups"><a href="#置换群-Permutation-Groups" class="headerlink" title="置换群 Permutation Groups"></a>置换群 Permutation Groups</h1><h2 id="定义和记号"><a href="#定义和记号" class="headerlink" title="定义和记号"></a>定义和记号</h2><ul>
<li>集合$S$上的一个<strong>置换（permutation）</strong>：one-to-one and onto map: $\pi: S \rightarrow S$</li>
</ul>
<a id="more"></a>
<ul>
<li>集合$X$的所有置换构成一个群$S_{X}$，$X = \{ 1, 2, …, n\}$时记为$S_{n}$。该群叫<strong>对称群（Symmetric Group）</strong>。</li>
</ul>
<p><strong>定理5.1.</strong> $n$个letter的对成群$S_{n}$是一个有$n!$个元素的群，其二元运算为复合（composition）。</p>
<ul>
<li>$S_{n}$的子群称为<strong>置换群（Permutation Group）</strong></li>
</ul>
<h3 id="轮换（Cycle-Notation）"><a href="#轮换（Cycle-Notation）" class="headerlink" title="轮换（Cycle Notation）"></a>轮换（Cycle Notation）</h3><ul>
<li>一个置换$\sigma \in S_{X}$是一个长度为$k$的<strong>轮换（cycle）</strong>，如果存在$a_{1}, a_{2}, …, a_{k}\in X$，使得<br>$\sigma(a_{1}) = a_{2}, \sigma(a_{2}) = a_{3}, … ,\sigma(a_{k}) = a_{1}$。</li>
</ul>
<p><strong>命题5.8.</strong> $\sigma$ 和 $\tau$ 是$S_{X}$中两个不相交轮换 $\Rightarrow \sigma \tau = \tau \sigma$</p>
<p><strong>定理5.9.</strong> $S_{n}$中的每个置换都可以写成不相交轮换的乘积。</p>
<h3 id="对换（Transposition）"><a href="#对换（Transposition）" class="headerlink" title="对换（Transposition）"></a>对换（Transposition）</h3><p><strong>对换（Transposition）</strong>：长度为2的轮换。<br>$(a_{1}, a_{2}, …, a_{3}) = (a_{1}, a_{n})(a_{1}, a_{n-1})…(a_{1}a_{2})$</p>
<p><strong>命题5.12.</strong> 每个元素个数大于2的有限集合的置换都可以写成对换的乘积。</p>
<p><strong>引理5.14.</strong> 如果恒等变换被写成$id = \tau_{1} \tau_{2} … \tau_{r} \Rightarrow r$是偶数。</p>
<p><strong>定理5.15.</strong>  如果一个排列能被写成偶数个置换乘积的形式，那么另一个与之等价的排列也一定拥有偶数项置换乘积。奇数同理。<br>据此定理可以将排列分为奇偶两类</p>
<h3 id="交替群-Alternating-Groups"><a href="#交替群-Alternating-Groups" class="headerlink" title="交替群 Alternating Groups"></a>交替群 Alternating Groups</h3><p><strong>交替群</strong>：$S_{n}$的所有偶置换$A_{n}$。</p>
<p><strong>定理5.16.</strong> $A_{n}$ 是 $S_{n}$ 的子群</p>
<p><strong>命题5.17.</strong> 奇偶置换的个数相等，都是$n!/2$，即$A_{n}$的大小。</p>
<h2 id="二面体群-Dihedral-Groups"><a href="#二面体群-Dihedral-Groups" class="headerlink" title="二面体群 Dihedral Groups"></a>二面体群 Dihedral Groups</h2><p><strong>n阶二面体群</strong>：正n边形的刚性运动，记为$D_{n}$</p>
<p><strong>定理5.20.</strong> $D_{n}$ 是 $S_{n}$ 的大小为$2n$的子群。</p>
<p><strong>定理5.23.</strong> 包含所有旋转$r$ 和对称 $s$ 的乘积的群 $D_{n}$，满足以下两个关系：</p>
<script type="math/tex; mode=display">r^{n} = 1</script><script type="math/tex; mode=display">s^{2} = 1</script><script type="math/tex; mode=display">srs = r^{-1}</script><h2 id="立方体移动群-The-Motion-Group-of-a-Cube"><a href="#立方体移动群-The-Motion-Group-of-a-Cube" class="headerlink" title="立方体移动群 The Motion Group of a Cube"></a>立方体移动群 The Motion Group of a Cube</h2><p><strong>命题5.27.</strong> 立方体移动群有24个元素。<br><strong>定理5.28.</strong> 立方体移动群是$S_{4}$。</p>
<hr>
<h1 id="陪集和拉格朗日定理"><a href="#陪集和拉格朗日定理" class="headerlink" title="陪集和拉格朗日定理"></a>陪集和拉格朗日定理</h1><h2 id="陪集-Cosets"><a href="#陪集-Cosets" class="headerlink" title="陪集 Cosets"></a>陪集 Cosets</h2><p>$G$ 是一个群，$H$ 是 $G$ 的子群。集合$H$的代表元为$g\in G$的<strong>左陪集（Left Coset）</strong>：$gH = \{ gh: h \in H \}$。右陪集类似。</p>
<p><strong>引理6.3.</strong> $H$ 为群 $G$ 的子群，$g_{1}, g_{2} \in G$，则以下条件等价：</p>
<ol>
<li>$g_{1}H = g_{2} H$</li>
<li>$Hg_{1}^{-1} = Hg_{2}^{-1}$</li>
<li>$g_{1}H \subset g_{2}H$</li>
<li>$g_{2} \in g_{1}H$</li>
<li>$g_{1}^{-1}g_{2} \in H$</li>
</ol>
<p><strong>定理6.4.</strong> $H$ 为群 $G$ 的子群。$H$ 的所有左陪集分割(partition)了 $G$,即 $G$ 是 $H$ 的左陪集的disjoint union. (右陪集同理)</p>
<p>$H$ 的<strong>index</strong>：$G$ 中 $H$ 的左陪集的个数，记为$[G:H]$。</p>
<p><strong>定理6.8.</strong> $H$ 的左陪集和右陪集个数相等。</p>
<h2 id="拉格朗日定理-Lagrange’s-Theorem"><a href="#拉格朗日定理-Lagrange’s-Theorem" class="headerlink" title="拉格朗日定理 Lagrange’s Theorem"></a>拉格朗日定理 Lagrange’s Theorem</h2><p><strong>命题6.9.</strong> map $\phi: H \rightarrow gH$是双射。所以 $H$ 和 $gH$ 的元素个数相同。</p>
<p><strong>定理6.10. Lagrange</strong> $G$ 的元素个数是子群 $H$ 的元素个数的整数倍，为 $H$ 的所有左陪集的个数。即$|G| / |H| = [G:H]$。</p>
<p><strong>推论6.11.</strong> $G$ 的所有元素$g$的order是$|G|$的约数。</p>
<p><strong>推论6.12.</strong> $|G|= p$，$p$为质数，则 $G$ 是循环群且除了单位元$e$的元素都是生成元。</p>
<p><strong>推论6.13.</strong> $K \subset H \subset G \Rightarrow [G:K] = [G:H][H:K]$.</p>
<ul>
<li>拉格朗日定理的逆命题不一定成立。</li>
</ul>
<p><strong>定理6.16.</strong> $S_{n}$ 中的两个轮换 $\tau$ 和 $\mu$ 有相同的长度 $\Leftrightarrow \exists \sigma \in S_{n}, \mu =\sigma \tau \sigma^{-1}$. （相似？）</p>
<h2 id="费马与欧拉定理-Fermat’s-and-Euler’s-Theorem"><a href="#费马与欧拉定理-Fermat’s-and-Euler’s-Theorem" class="headerlink" title="费马与欧拉定理 Fermat’s and Euler’s Theorem"></a>费马与欧拉定理 Fermat’s and Euler’s Theorem</h2><p><strong>欧拉函数</strong>$\phi$-<em>function</em>: map $\phi: \mathbb{N} \rightarrow \mathbb{N}$, 与$n$互质的整数$m(1\leq m &lt;n)$的个数，$\phi(1) = 1$。</p>
<p><strong>定理6.17.</strong> $U(n)$ 为 $\mathbb{Z}_{n}$ 的units的群，则 $|U(n)| = \phi(n)$。</p>
<p><strong>定理6.18. Euler’s Theorem</strong> 正整数$a, n$，$n &gt; 0 \wedge gcd(a,n) = 1 \Rightarrow a^{\phi(n)} \equiv 1$ (mod $n$)。</p>
<p><strong>定理6.19. Fermat’s Little Theorem</strong> $p$为质数，$p \nmid a \Rightarrow a^{p-1} \equiv 1$ (mod $p$)。<br>$\forall b \in \mathbb{Z}, b^{p} \equiv b$ (mod $p$).</p>
]]></content>
      <categories>
        <category>课程</category>
        <category>problem-solving</category>
      </categories>
  </entry>
  <entry>
    <title>pytorch 安装教程</title>
    <url>/2021/07/20/pytorch/</url>
    <content><![CDATA[<p>之前已经安装好了 cuda，现在来安装 pytorch，顺便再装一个 anaconda。同样的，本篇是针对 Ubuntu </p>
<p>20.04 LTS 的 pytorch 和 anaconda 安装流程。</p>
<a id="more"></a>
<h1 id="安装-anaconda"><a href="#安装-anaconda" class="headerlink" title="安装 anaconda"></a>安装 anaconda</h1><p>以前一直没搞清已经安装的 python pip 和 anaconda 有什么区别，会不会有冲突，所以一直没敢装 anaconda，现在好像弄清楚了，它们基本上是两个独立的程序，conda会使用它自己的虚拟环境，所以影响应该不大，那我们就开始吧。</p>
<p><a href="https://docs.anaconda.com/anaconda/install/linux/">官方教程</a></p>
<p>首先到<a href="https://www.anaconda.com/products/individual#Downloads">官网下载</a>安装包，可以用 <code>sha256sum</code> 检查一下。</p>
<p>然后到安装路径运行它：</p>
<p><img data-src="installer.png" alt="installer"></p>
<p>然后有一些协议啊步骤啊，阅读同意就行。有一个初始化选项推荐 <code>yes</code>。</p>
<p>出现这个就是安装完成辣：</p>
<p><img data-src="install_succ.png" alt="install successfully!"></p>
<p>看看 <code>python</code>，发现已经是 anaconda 的了。</p>
<p><img data-src="python-conda.png" alt="python after installation"></p>
<p>改一下清华源：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda config --<span class="built_in">set</span> show_channel_urls yes</span></span><br></pre></td></tr></table></figure>
<p><code>$ anaconda-navigator</code> 可以打开 anaconda 的 gui 界面，不过好像窗口大小一直有问题。</p>
<p><code>$ conda update conda</code> 可以进行更新。</p>
<h1 id="安装-pytorch"><a href="#安装-pytorch" class="headerlink" title="安装 pytorch"></a>安装 pytorch</h1><p>到 <a href="https://pytorch.org/">pytorch 官网</a> 直接选择自己的电脑版本，然后输入他给的命令行就行了。</p>
<p><img data-src="pytorch_org.png" alt></p>
<p>出现这个就是开始下载了，有一点慢：</p>
<p><img data-src="pytorch_succ.png" alt></p>
<p>把里面所有东西都下好了基本上就完成了。我们可以在 Python 里面直接使用 <code>torch</code> 包辣：</p>
<p><img data-src="torch_comp.png" alt="torch complete"></p>
<p>也可以通过 <code>torch.cuda.is_available()</code> 看到我们的 cuda 是否可用。</p>
<h1 id="安装-TensorFlow"><a href="#安装-TensorFlow" class="headerlink" title="安装 TensorFlow"></a>安装 TensorFlow</h1><p>既然装了 pytorch 也把 TensorFlow 装一下（其实是mit和andrew的课都用的 TensorFlow）。</p>
<p><a href="https://www.tensorflow.org/install">TensorFlow 官方文档</a></p>
<p><a href="https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/">conda安装tensorflow的官方文档</a></p>
<p>首先新建一个 conda 的虚拟环境：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> conda create -n tf-gpu python=3.8.8	<span class="comment"># 初始装一个python</span></span></span><br></pre></td></tr></table></figure>
<p>这里正好了解一下 conda 对虚拟环境的管理，其实就是对所有包的管理。新建一个虚拟环境就是创一个什么包都没有的新环境（当然你可以初始安装包），达到不同工作环境间的隔离。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> conda activate tf-gpu		<span class="comment"># 转移到这个环境</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> conda deactivate		<span class="comment"># 取消当前环境，回到 base</span></span></span><br></pre></td></tr></table></figure>
<p>然后在新环境中安装 <code>tensorflow</code> 中即可，还需要一些常用的包 <code>numpy</code>, <code>pandas</code>, <code>matplotlib</code>等。</p>
<p>tensorflow 已经出到第2版了，和第一版还是有一些区别的。<code>tensorflow1</code> 是区分 CPU 和 GPU 的，分别是 <code>tensorflow</code> 和 <code>tensorflow-gpu</code>，<code>tensorflow2</code> 同时支持 CPU 和 GPU。</p>
<p>在安装时不显式指定版本即 <code>tensorflow=x.x</code> 默认安装最新版，也即 2.x 版不区分是否带 <code>-gpu</code>。</p>
<p>其实用 conda 也可以直接安装：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> conda install tensorflow</span></span><br></pre></td></tr></table></figure>
<p>但是官方建议用 pip 安装（但我不知道为啥）：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pip install tensorflow</span></span><br></pre></td></tr></table></figure>
<p>这里其实又涉及到 conda 和 pip 的区别，默认 pip 是会调用 <code>/bin/...</code> 下的程序，这个 pip 下载的不归 conda 管理。但是 conda 又是包含 pip 的，即可以 <code>$ conda install pip</code>，也是在新环境中装 <code>python</code> 时自带的，这个 pip 本身就归 conda 管，执行的是 <code>~/anaconda/bin/...</code> 下的程序（因为它在我们的环境变量 <code>$PATH</code> 中，优先找到它）。并且这个 pip 所下载的包也是归 conda 管理的，可以在 <code>conda list</code> 中看到。</p>
<p><em>至此大概将 conda 和 pip 的区别搞清楚了，不过是我的个人理解，可能有误，欢迎也感谢指正。</em></p>
<p>最后 check 一下：</p>
<p><img data-src="tf_complete.png" alt="tf complete"></p>
<p>我的 zsh 可以显示当前的环境，是我们创建的 <code>tf-gpu</code>，TensorFlow 可以被成功 import，查看到版本，以及可以在 cuda下工作。</p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title>问题求解笔记-随机算法</title>
    <url>/2021/05/26/randomized-algorithm/</url>
    <content><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>我们至今仍然不知道任何NP难问题的随机多项式算法。</p>
<p>我们只知道一些问题，满足：</p>
<ul>
<li>有高效的多项式随机算法</li>
<li>没有已知的多项式时间确定算法</li>
<li>不知道它是否属于 $P$</li>
</ul>
<p>随机算法和近似算法结合的合理性在于，算法输出错误结果的概率甚至小于确定性算法长时间运行下硬件出错的概率。</p>
<a id="more"></a>
<hr>
<h1 id="随机算法的分类"><a href="#随机算法的分类" class="headerlink" title="随机算法的分类"></a>随机算法的分类</h1><h2 id="一些概念"><a href="#一些概念" class="headerlink" title="一些概念"></a>一些概念</h2><p>我们可以把随机算法就看成是在运行中需要随机数的算法，并且后续运行需要这些随机数。</p>
<ul>
<li>$S_{A,x} = \{ C | C \text{ 是 }A \text{ 在 }x \text{ 上的一个随机计算}\}$。（样本空间）</li>
<li><p>$Prob $ 是 $S_{A,x} $ 的概率分布。</p>
</li>
<li><p>$Random_A(x)$ 是 $A $ 在 $x$ 上运行中的最大随机数位数。</p>
</li>
<li>$Random_A(n) = \max\{Random_A(x) | x \text{ 是大小为 } n \text{ 的输入}\}$。</li>
</ul>
<p>复杂度的衡量是重要的因为：</p>
<ul>
<li>生成伪随机数需要开销，一般和位数成正比</li>
<li>“去随机化”</li>
</ul>
<p>算法 $A$ 对于输入 $x$ 输出 $y$ 的概率 $Prob(A(x) = y)$ 是所有 $C$ 输出 $y$ 的 $Prob_{A , x}(C) $ 的和。随机算法的目标就是让输出正确的 $y$ 的 $Prob(A(x) = y)$ 尽可能大。</p>
<ul>
<li><p>$Time(C) $ 是 $C$ 的运行时间，则 $A $ 对于输入 $x$ 的期望时间复杂度是： </p>
<script type="math/tex; mode=display">Exp\text{-}Time_A(x) = E[Time] = \sum\limits_{C}Prob_{A,x}(C) \cdot Time(C)</script></li>
<li><p>$Exp\text{-}Time_A(n) = \max\{Random_A(x) | x \text{ 是大小为 } n \text{ 的输入}\}$。</p>
</li>
<li><p>有时我们也考虑最坏情况复杂度： $Time_A(x) = \max\{Time(C) | C \text{ 是 }A \text{ 在 }  x \text{ 上的运行} \}$。</p>
</li>
<li><p>$Time_A(n) = \max\{Time_A(x) | x \text{ 是大小为 } n \text{ 的输入} \}$。</p>
</li>
</ul>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><h3 id="拉斯维加斯算法（Las-Vegas-Algorithms）"><a href="#拉斯维加斯算法（Las-Vegas-Algorithms）" class="headerlink" title="拉斯维加斯算法（Las Vegas Algorithms）"></a>拉斯维加斯算法（Las Vegas Algorithms）</h3><p>定义一个算法 $A$ 是问题 $F$ 的拉斯维加斯算法：</p>
<ol>
<li><p>对于任意的输入 $x$，$Prob(A(x) = F(x)) = 1$，$F(x)$ 是问题 $F$ 对 $x$ 的解。</p>
<ul>
<li>这样 $A$ 的复杂度一般被认为是 $Exp\text{-}Time_A(n)$。</li>
</ul>
</li>
<li><p>我们允许输出 “?”（不能按时解决问题）。对任意的输入 $x$，$Prob(A(x) = F(x)) \geq \dfrac{1}{2}$ 且 </p>
<p><script type="math/tex">Prob(A(x)= ?)=  1 - Prob(A(x) = F(x)) \leq \dfrac{1}{2}</script>。</p>
<ul>
<li>这样 $A$ 的复杂度一般被认为是 $Time_A(n)$，因为一般需要运行到 $Time_A(x)$ 才能判断“?”。</li>
</ul>
</li>
</ol>
<h3 id="单错蒙特卡罗算法（One-Sided-Error-Monte-Carlo-Algorithm）"><a href="#单错蒙特卡罗算法（One-Sided-Error-Monte-Carlo-Algorithm）" class="headerlink" title="单错蒙特卡罗算法（One-Sided-Error Monte Carlo Algorithm）"></a>单错蒙特卡罗算法（One-Sided-Error Monte Carlo Algorithm）</h3><p>只对判定问题有用。</p>
<p>算法 $A$ 是语言 $L$ 的单错蒙特卡罗算法当：</p>
<ol>
<li>对任意的 $x \in L$，$Prob(A(x) = 1 ) \geq 1/2$；</li>
<li>对任意的 $x \notin L$，$Prob(A(x)  = 0) = 1$。</li>
</ol>
<p>所以单错蒙特卡罗算法不会误判错误的输入，但有小概率误判正确的输入，只有单方向错。</p>
<h3 id="双错蒙特卡算法（Two-Sided-Error-Monte-Carlo-Algorithm）"><a href="#双错蒙特卡算法（Two-Sided-Error-Monte-Carlo-Algorithm）" class="headerlink" title="双错蒙特卡算法（Two-Sided-Error Monte Carlo Algorithm）"></a>双错蒙特卡算法（Two-Sided-Error Monte Carlo Algorithm）</h3><p>$F$ 是一个计算问题，一个随机算法 $A$ 是双错蒙特卡算法当：</p>
<ul>
<li>存在一个实数 $0 &lt; \varepsilon \leq 1/2$，使得对任意的输入 $x$ ， $Prob(A(x) = F(x)) \geq \dfrac{1}{2}  + \varepsilon$。</li>
<li>让算法跑 $t$ 次，取出现次数至少是 $\lceil t / 2 \rceil$ 的结果，正确的概率很大。</li>
</ul>
<h3 id="无界错蒙特卡罗算法（Unbounded-Error-Monte-Carlo-Algorithm）"><a href="#无界错蒙特卡罗算法（Unbounded-Error-Monte-Carlo-Algorithm）" class="headerlink" title="无界错蒙特卡罗算法（Unbounded-Error Monte Carlo Algorithm）"></a>无界错蒙特卡罗算法（Unbounded-Error Monte Carlo Algorithm）</h3><p>$F$ 是一个计算问题，一个随机算法 $A$ 是无界错蒙特卡算法当：</p>
<ul>
<li>对任意输入 $x$，$Prob(A(x) = F(x)) &gt; \dfrac{1}{2}$。</li>
<li>缺点在于 $Prob(A(x) = F(x))$ 和 $\dfrac{1}{2}$ 可能趋近于 $0$ 当输入很大时。</li>
<li>为了从无界错蒙特卡罗算法获得一个随机算法满足 $Prob(A_{k(|n|)} (x) = F(x)) \geq 1 - \delta, 0 \leq \delta \leq \dfrac{1}{2}$，我们必须接受 $Time_{A_{k(n)}}(n) = O(2^{2Random_A(n)} \cdot Time_A(n))$</li>
</ul>
<h3 id="随机优化算法（Randomized-Optimization-Algorithm）"><a href="#随机优化算法（Randomized-Optimization-Algorithm）" class="headerlink" title="随机优化算法（Randomized Optimization Algorithm）"></a>随机优化算法（Randomized Optimization Algorithm）</h3><p>我们一般不考虑正确解出现的频率，而直接取最优解。所以一个 $k$ 轮算法中最优解的概率是</p>
<script type="math/tex; mode=display">
Prob(A_k(x) \in Output_U(x)) = 1 - [Prob(A(x) \notin Output_U(x))]^k,</script><p>其中 $A_k$ 指运行 $k$ 次随机优化算法 $A$。</p>
<ul>
<li>若 $Prob(A(x) \notin Output_U(x)) \leq \varepsilon, \varepsilon &lt; 1$，则 $A$ 和单错蒙特卡罗效率差不多。</li>
<li>若 $Prob(A(x) \in Output_U(x)) \geq 1 / p(|x|)$，$p$ 为多项式，则 $A$ 的效率也很可观。</li>
</ul>
<h2 id="随机近似算法"><a href="#随机近似算法" class="headerlink" title="随机近似算法"></a>随机近似算法</h2><ul>
<li>$U = (\Sigma_I, \Sigma_O, L, L_I,\mathcal{M}, cost, goal)$ 是一个优化问题，对任意的实数 $\delta &gt; 1$，一个随机算法 $A$ 叫做 $U$ 的<strong>随机$\delta$-近似算法</strong>，当满足对任意的 $x\in L_I$：<ul>
<li>$Prob(A(x) \in \mathcal{M}(x)) = 1$，</li>
<li>$Prob(R_A(x) \leq \delta)\geq 1/ 2$。</li>
</ul>
</li>
<li>对任意的函数 $f :\mathbb{N} \to \mathbb{R}^+$，$A$ 叫做 $U$ 的<strong>随机$f(n)$-近似算法</strong>，当满足对任意的 $x \in L_I$：<ul>
<li>$Prob(A(x) \in \mathcal{M}(x)) = 1$，</li>
<li>$Prob(R_A(x) \leq f(|x|)) \geq 1/2$。</li>
</ul>
</li>
<li>一个随机算法 $A$ 叫做 $U$ 的<strong>随机多项式时间近似方案（RPTAS）</strong>，如果存在一个函数 $p: L_I \times \mathbb{R}^+ \to \mathbb{N}$，使得对于任何输入 $(x, \delta) \in L_I \times \mathbb{R}^+$，满足：<ul>
<li>$Prob(A(x, \delta) \in  \mathcal{M}(x)) = 1$，对所有的选择 $A$ 都可以计算出一个可行解。</li>
<li>$Prob(\varepsilon_A(x, \delta) \leq \delta) \geq \dfrac{1}{2}$，一个可行解的相对误差不超过 $\delta$ 的概率大于 $1/2$。</li>
<li>$Time_A(x, \delta^{-1}) \leq p(|x| , \delta^{-1})$ 且 $p$ 是关于 $|x| $ 的多项式。</li>
</ul>
</li>
<li>类似的，如果 $p$ 是同时关于 $|x| $ 和 $\delta^{-1}$ 的多项式，$A$ 叫做 $U$ 的<strong>随机完全多项式时间近似方案（RFPTAS）</strong>。</li>
</ul>
<ul>
<li>$U = (\Sigma_I, \Sigma_O, L, L_I,\mathcal{M}, cost, goal)$ 是一个优化问题，对任意的实数 $\delta &gt; 1$，一个随机算法 $A$ 叫做 $U$ 的<strong>随机$\delta$-期望算法</strong>，当满足对任意的 $x\in L_I$：<ul>
<li>$Prob(A(x) \in \mathcal{M}(x)) = 1$，</li>
<li>$E[R_A(x)] \leq \delta$。</li>
</ul>
</li>
</ul>
<hr>
<h1 id="随机算法的设计范式"><a href="#随机算法的设计范式" class="headerlink" title="随机算法的设计范式"></a>随机算法的设计范式</h1><h2 id="挫败敌人（Foiling-an-Adversary）"><a href="#挫败敌人（Foiling-an-Adversary）" class="headerlink" title="挫败敌人（Foiling an Adversary）"></a>挫败敌人（Foiling an Adversary）</h2><ul>
<li>对手论证</li>
<li>随机算法可以看作是确定性算法上的概率分布，对手可以构造一小部分很强（大开销）的确定性算法的输入，但很难设计击败随机算法的输入。</li>
</ul>
<h2 id="丰富的证人？（Abundance-of-Witnesses）"><a href="#丰富的证人？（Abundance-of-Witnesses）" class="headerlink" title="丰富的证人？（Abundance of Witnesses）"></a>丰富的证人？（Abundance of Witnesses）</h2><h2 id="指纹？（Fingerprinting）"><a href="#指纹？（Fingerprinting）" class="headerlink" title="指纹？（Fingerprinting）"></a>指纹？（Fingerprinting）</h2><h2 id="随机样本（Random-Sampling）"><a href="#随机样本（Random-Sampling）" class="headerlink" title="随机样本（Random Sampling）"></a>随机样本（Random Sampling）</h2><h2 id="（Relaxation-and-Random-Rounding）"><a href="#（Relaxation-and-Random-Rounding）" class="headerlink" title="（Relaxation and Random Rounding）"></a>（Relaxation and Random Rounding）</h2>]]></content>
      <categories>
        <category>课程</category>
        <category>problem-solving</category>
      </categories>
  </entry>
  <entry>
    <title>scikit-learn</title>
    <url>/2021/08/02/scikit-learn/</url>
    <content><![CDATA[<p>本文主要是对 python 库 <code>sklearn</code> 的学习笔记。</p>
<p>参考网站：</p>
<p><a href="https://scikit-learn.org/stable/index.html">sklearn官方网站</a></p>
<p><a href="https://scikit-learn.org.cn/">官网汉化</a></p>
<p><a href="https://sklearn.apachecn.org/">一个中文文档</a></p>
<a id="more"></a>
<h1 id="scikit-learn-是啥"><a href="#scikit-learn-是啥" class="headerlink" title="scikit-learn 是啥"></a>scikit-learn 是啥</h1><p><code>scikit-learn</code> 是一个开源的机器学习库，它支持有监督和无监督的学习。它还提供了用于模型拟合，数据预处理，模型选择和评估以及许多其他实用程序的各种工具。</p>
<h1 id="拟合和预测"><a href="#拟合和预测" class="headerlink" title="拟合和预测"></a>拟合和预测</h1><p><code>sklearn</code> 提供了数十种内置的机器学习算法和模型，称为估算器。每个估算器可以使用其拟合方法拟合到一些数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>clf = RandomForestClassifier(random_state=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># clf 就是一个分类器(classifier)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>X = [[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],  <span class="comment">#2个样本，3个特征</span></span><br><span class="line"><span class="meta">... </span>     [<span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = [<span class="number">0</span>, <span class="number">1</span>]  <span class="comment">#每一个样本的类别</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>clf.fit(X, y)	<span class="comment"># 一般通过 .fit 方法来拟合</span></span><br></pre></td></tr></table></figure>
<p>我们可以直接调用这些模型，给定一些超参数，获得一个分类器/估算器。然后使用 <code>.fit</code> 方法来进行拟合，所有计算、优化过程都在这个方法中，（而且基本上这个库里的实现比自己手撸的要好）。</p>
<p>拟合方法通常有两个输入：</p>
<ul>
<li>样本矩阵（或设计矩阵）X。<code>X</code>的大小通常为<code>(n_samples, n_features)</code>，这意味着样本表示为行，特征表示为列。</li>
<li>目标值y是用于回归任务的真实数字，或者是用于分类的整数（或任何其他离散值）。对于无监督学习，<code>y</code>无需指定。<code>y</code>通常是1d数组，其中<code>i</code>对应于目标<code>X</code>的 第<code>i</code>个样本（行）。</li>
<li>虽然某些估算器可以使用其他格式（例如稀疏矩阵），但是通常，两者<code>X</code>和<code>y</code>预计都是numpy数组或等效的类似数组的数据类型。</li>
</ul>
<p>拟合完成之后，就可以直接用它预测啦。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>clf.predict(X)  <span class="comment"># 预测训练数据的标签</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>clf.predict([[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>]])  <span class="comment"># 预测新数据的标签</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title>tensorflow</title>
    <url>/2021/08/11/tensorflow/</url>
    <content><![CDATA[<p>本篇是对python库 <code>tensorflow</code> 的学习笔记。</p>
<p><a href="https://www.tensorflow.org/">tensorflow官方网站</a></p>
<a id="more"></a>
<h1 id="tensorflow-是啥"><a href="#tensorflow-是啥" class="headerlink" title="tensorflow 是啥"></a>tensorflow 是啥</h1><p><strong>TensorFlow</strong>是一个开源软件库，用于各种感知和语言理解任务的机器学习。</p>
<h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><h2 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h2><p><code>tensorflow</code> 最重要的数据类型就是<strong>张量</strong> <code>tf.tensor</code>，是具有统一类型的多维数组，其实和 <code>numpy.arrays</code> 和相似。</p>
<p>可以用 <code>tf.constant()</code> 输入一个 list 来创建一个张量。</p>
<p>张量有一些基本数学运算，和 <code>numpy</code> 差不多，不赘述。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">                 [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">                 [<span class="number">1</span>, <span class="number">1</span>]]) <span class="comment"># Could have also said `tf.ones([2,2])`</span></span><br><span class="line"></span><br><span class="line">print(a + b, <span class="string">&quot;\n&quot;</span>) <span class="comment"># element-wise addition</span></span><br><span class="line">print(a * b, <span class="string">&quot;\n&quot;</span>) <span class="comment"># element-wise multiplication</span></span><br><span class="line">print(a @ b, <span class="string">&quot;\n&quot;</span>) <span class="comment"># matrix multiplication</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Find the largest value</span></span><br><span class="line">print(tf.reduce_max(c))</span><br><span class="line"><span class="comment"># Find the index of the largest value</span></span><br><span class="line">print(tf.argmax(c))</span><br><span class="line"><span class="comment"># Compute the softmax</span></span><br><span class="line">print(tf.nn.softmax(c))</span><br></pre></td></tr></table></figure>
<p>张量有形状。下面是几个相关术语：</p>
<ul>
<li><strong>形状</strong>：张量的每个维度的长度（元素数量）。</li>
<li><strong>秩</strong>：张量的维度数量。标量的秩为 0，向量的秩为 1，矩阵的秩为 2。</li>
<li><strong>轴</strong>或<strong>维度</strong>：张量的一个特殊维度。</li>
<li><strong>大小</strong>：张量的总项数，即乘积形状向量</li>
</ul>
<p>索引，形状操作，广播等也不赘述。</p>
<h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><p><strong>变量</strong> <code>tf.Variable</code> 是用于表示程序处理的共享持久状态的推荐方法。变量通过 <code>tf.Variable</code> 类进行创建和跟踪<code>tf.Variable</code>。表示张量，对它执行运算可以改变其值。利用特定运算可以读取和修改此张量的值。更高级的库（如<code>tf.keras</code>）使用 <code>tf.Variable</code> 来存储模型参数。</p>
<p>可以用 <code>tf.Variable()</code> 来初始化一个变量，都和 <code>tf.constant()</code> 很相似。</p>
<p>可以使用 <code>tf.Variable.assign</code> 重新分配张量。调用 <code>assign</code>（通常）不会分配新张量，而会重用现有张量的内存。</p>
<p>虽然变量对微分很重要，但某些变量不需要进行微分。在创建时，通过将 <code>trainable</code> 设置为 False 可以关闭梯度。</p>
<p>为了提高性能，TensorFlow 会尝试将张量和变量放在与其 <code>dtype</code> 兼容的最快设备上。这意味着如果有 GPU，那么大部分变量都会放置在 GPU 上。</p>
<h2 id="自动微分"><a href="#自动微分" class="headerlink" title="自动微分"></a>自动微分</h2><p>这也是 <code>tensorflow</code> 最重要的功能之一，它使得神经网络不需要手动计算梯度来更新参数。</p>
<p>TensorFlow 为自动微分提供了 <a href="https://tensorflow.google.cn/api_docs/python/tf/GradientTape?hl=zh_cn"><code>tf.GradientTape</code></a> API ，根据某个函数的输入变量来计算它的导数。Tensorflow 会把 ‘tf.GradientTape’ 上下文中执行的所有操作都记录在一个磁带上 (“tape”)。 然后基于这个磁带和每次操作产生的导数，用反向微分法（”reverse mode differentiation”）来计算这些被“记录在案”的函数的导数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = tf.ones((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> t:</span><br><span class="line">  t.watch(x)</span><br><span class="line">  y = tf.reduce_sum(x)</span><br><span class="line">  z = tf.multiply(y, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Derivative of z with respect to the original input tensor x</span></span><br><span class="line">dz_dx = t.gradient(z, x)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">1</span>]:</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">1</span>]:</span><br><span class="line">    <span class="keyword">assert</span> dz_dx[i][j].numpy() == <span class="number">8.0</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># Use the tape to compute the derivative of z with respect to the</span></span><br><span class="line"><span class="comment"># intermediate value y.</span></span><br><span class="line">dz_dy = t.gradient(z, y)</span><br><span class="line"><span class="keyword">assert</span> dz_dy.numpy() == <span class="number">8.0</span></span><br></pre></td></tr></table></figure>
<p>默认情况下，调用 GradientTape.gradient() 方法时， GradientTape 占用的资源会立即得到释放。通过创建一个持久的梯度带 <code>tf.GradientTape(persistent=True)</code>，可以计算同个函数的多个导数。这样在磁带对象被垃圾回收时，就可以多次调用 ‘gradient()’ 方法。</p>
<p>由于磁带会记录所有执行的操作，Python 控制流（如使用 if 和 while 的代码段）自然得到了处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x, y</span>):</span></span><br><span class="line">  output = <span class="number">1.0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(y):</span><br><span class="line">    <span class="keyword">if</span> i &gt; <span class="number">1</span> <span class="keyword">and</span> i &lt; <span class="number">5</span>:</span><br><span class="line">      output = tf.multiply(output, x)</span><br><span class="line">  <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad</span>(<span class="params">x, y</span>):</span></span><br><span class="line">  <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> t:</span><br><span class="line">    t.watch(x)</span><br><span class="line">    out = f(x, y)</span><br><span class="line">  <span class="keyword">return</span> t.gradient(out, x)</span><br><span class="line"></span><br><span class="line">x = tf.convert_to_tensor(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> grad(x, <span class="number">6</span>).numpy() == <span class="number">12.0</span></span><br><span class="line"><span class="keyword">assert</span> grad(x, <span class="number">5</span>).numpy() == <span class="number">12.0</span></span><br><span class="line"><span class="keyword">assert</span> grad(x, <span class="number">4</span>).numpy() == <span class="number">4.0</span></span><br></pre></td></tr></table></figure>
<p>在 ‘GradientTape’ 上下文管理器中记录的操作会用于自动微分。如果导数是在上下文中计算的，导数的函数也会被记录下来。因此，同个 API 可以用于高阶导数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = tf.Variable(<span class="number">1.0</span>)  <span class="comment"># Create a Tensorflow variable initialized to 1.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> t:</span><br><span class="line">  <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> t2:</span><br><span class="line">    y = x * x * x</span><br><span class="line">  <span class="comment"># Compute the gradient inside the &#x27;t&#x27; context manager</span></span><br><span class="line">  <span class="comment"># which means the gradient computation is differentiable as well.</span></span><br><span class="line">  dy_dx = t2.gradient(y, x)</span><br><span class="line">d2y_dx2 = t.gradient(dy_dx, x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> dy_dx.numpy() == <span class="number">3.0</span></span><br><span class="line"><span class="keyword">assert</span> d2y_dx2.numpy() == <span class="number">6.0</span></span><br></pre></td></tr></table></figure>
<h1 id="tf-keras"><a href="#tf-keras" class="headerlink" title="tf.keras"></a>tf.keras</h1><p><code>keras</code> 是很经典的神经网络框架，也是很常用的，所以整理一下。</p>
<p>首先最重要的它有两个类 <code>Model</code> 和 <code>Sequential</code>，他们之间有很多联系和区别。简单来说 <code>Sequential</code> 只能往后叠层，<code>model</code> 可以搭建更复杂的模型。</p>
<p><a href="https://stackoverflow.com/questions/66879748/what-is-the-difference-between-tf-keras-model-and-tf-keras-sequential">这里</a>有一篇简述他们的区别的post。</p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>有两种初始化一个 <code>Model</code> 的方法。</p>
<p>一个是函数式 API：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs = tf.keras.Input(shape=(<span class="number">3</span>,))</span><br><span class="line">x = tf.keras.layers.Dense(<span class="number">4</span>, activation=tf.nn.relu)(inputs)</span><br><span class="line">outputs = tf.keras.layers.Dense(<span class="number">5</span>, activation=tf.nn.softmax)(x)</span><br><span class="line">model = tf.keras.Model(inputs=inputs, outputs=outputs)</span><br></pre></td></tr></table></figure>
<p>一个是通过子类，可以在 <code>__init__</code> 中定义层，通过 <code>call</code> 来前向传播：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">    self.dense1 = tf.keras.layers.Dense(<span class="number">4</span>, activation=tf.nn.relu)</span><br><span class="line">    self.dense2 = tf.keras.layers.Dense(<span class="number">5</span>, activation=tf.nn.softmax)</span><br><span class="line">    self.dropout = tf.keras.layers.Dropout(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, inputs, training=<span class="literal">False</span></span>):</span></span><br><span class="line">    x = self.dense1(inputs)</span><br><span class="line">    <span class="keyword">if</span> training:</span><br><span class="line">      x = self.dropout(x, training=training)</span><br><span class="line">    <span class="keyword">return</span> self.dense2(x)</span><br><span class="line"></span><br><span class="line">model = MyModel()</span><br></pre></td></tr></table></figure>
<h3 id="call"><a href="#call" class="headerlink" title="call"></a>call</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">call(</span><br><span class="line">    inputs, training=<span class="literal">None</span>, mask=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>进行一次前向传播。</p>
<h3 id="compile"><a href="#compile" class="headerlink" title="compile"></a>compile</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">compile</span>(</span><br><span class="line">    optimizer=<span class="string">&#x27;rmsprop&#x27;</span>, loss=<span class="literal">None</span>, metrics=<span class="literal">None</span>, loss_weights=<span class="literal">None</span>,</span><br><span class="line">    weighted_metrics=<span class="literal">None</span>, run_eagerly=<span class="literal">None</span>, steps_per_execution=<span class="literal">None</span>, **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>设置训练模型。</p>
<p>常用参数：</p>
<ul>
<li><code>optimizer</code>，使用的优化器</li>
<li><code>loss</code>，使用的损失函数</li>
<li><code>metrics</code>，在训练和测试中需要被计算的矩阵</li>
</ul>
<h3 id="evaluate"><a href="#evaluate" class="headerlink" title="evaluate"></a>evaluate</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">evaluate(</span><br><span class="line">    x=<span class="literal">None</span>, y=<span class="literal">None</span>, batch_size=<span class="literal">None</span>, verbose=<span class="number">1</span>, sample_weight=<span class="literal">None</span>, steps=<span class="literal">None</span>,</span><br><span class="line">    callbacks=<span class="literal">None</span>, max_queue_size=<span class="number">10</span>, workers=<span class="number">1</span>, use_multiprocessing=<span class="literal">False</span>,</span><br><span class="line">    return_dict=<span class="literal">False</span>, **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>返回损失值和矩阵值，在测试模式下。计算是 in batches 的。</p>
<ul>
<li><code>x</code>，输入数据</li>
<li><code>y</code>，目标数据</li>
<li><code>batch_size</code>，每次计算中每个 <code>batch</code> 的样本数量</li>
</ul>
<h3 id="fit"><a href="#fit" class="headerlink" title="fit"></a>fit</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit(</span><br><span class="line">    x=<span class="literal">None</span>, y=<span class="literal">None</span>, batch_size=<span class="literal">None</span>, epochs=<span class="number">1</span>, verbose=<span class="string">&#x27;auto&#x27;</span>,</span><br><span class="line">    callbacks=<span class="literal">None</span>, validation_split=<span class="number">0.0</span>, validation_data=<span class="literal">None</span>, shuffle=<span class="literal">True</span>,</span><br><span class="line">    class_weight=<span class="literal">None</span>, sample_weight=<span class="literal">None</span>, initial_epoch=<span class="number">0</span>, steps_per_epoch=<span class="literal">None</span>,</span><br><span class="line">    validation_steps=<span class="literal">None</span>, validation_batch_size=<span class="literal">None</span>, validation_freq=<span class="number">1</span>,</span><br><span class="line">    max_queue_size=<span class="number">10</span>, workers=<span class="number">1</span>, use_multiprocessing=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>训练模型。</p>
<p>会返回一个 <code>History</code> 对象，在每个epoch记录训练或验证损失和矩阵值的历史。</p>
<ul>
<li><code>epochs</code>，迭代次数。</li>
</ul>
<h3 id="get-layer"><a href="#get-layer" class="headerlink" title="get_layer"></a>get_layer</h3><p>根据名字或编号获得一个层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">get_layer(</span><br><span class="line">    name=<span class="literal">None</span>, index=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="predict"><a href="#predict" class="headerlink" title="predict"></a>predict</h3><p>对于输入生成输出预测。这是对于大规模数据预测的，in batch 的，对于小规模可以用 <code>call</code>。是不计正则化的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predict(</span><br><span class="line">    x, batch_size=<span class="literal">None</span>, verbose=<span class="number">0</span>, steps=<span class="literal">None</span>, callbacks=<span class="literal">None</span>, max_queue_size=<span class="number">10</span>,</span><br><span class="line">    workers=<span class="number">1</span>, use_multiprocessing=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h3><p>打印网络的框架。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">summary(</span><br><span class="line">    line_length=<span class="literal">None</span>, positions=<span class="literal">None</span>, print_fn=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>然后有一个有关网络输出维数的一个理解，<code>summary()</code> 都会显示维数是 <code>(None, ...)</code>，<a href="https://stackoverflow.com/questions/47240348/what-is-the-meaning-of-the-none-in-model-summary-of-keras">这里</a>有一个post 说的比较清楚，就是第一维其实是 batch_size。</p>
<h2 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h2><p>整个训练、预测方法和 <code>Model</code> 差不多，但差别就在于 <code>Sequential</code> 只能简单叠层，一般通过 <code>add</code> 或直接在初始化列表中设置。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Optionally, the first layer can receive an `input_shape` argument:</span></span><br><span class="line">model = tf.keras.Sequential()</span><br><span class="line">model.add(tf.keras.layers.Dense(<span class="number">8</span>, input_shape=(<span class="number">16</span>,)))</span><br><span class="line"><span class="comment"># Afterwards, we do automatic shape inference:</span></span><br><span class="line">model.add(tf.keras.layers.Dense(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<p>然后要注意的是不设置输入维度的话，这个模型就还没有建成（built），就不能 <code>summary</code> 等等，直到第一次训练或预测等，接收到了输入数据。或者用 <code>build</code> 进行手动延迟搭建。</p>
<h3 id="add"><a href="#add" class="headerlink" title="add"></a>add</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add(</span><br><span class="line">    layer</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>加一层。</p>
<h1 id="tf-data"><a href="#tf-data" class="headerlink" title="tf.data"></a>tf.data</h1><p>这个主要是用来构建数据集管道的。</p>
<h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>一个大数据集，提供高效的输入管道。符合几个模式：</p>
<ol>
<li>从输入数据创造源数据集。</li>
<li>应用数据集转换对数据进行预处理。</li>
<li>遍历数据集并处理元素。</li>
</ol>
<p>迭代的以流的方式进行，所以完整的数据集不需要载入内存。</p>
<h3 id="as-numpy-iterator"><a href="#as-numpy-iterator" class="headerlink" title="as_numpy_iterator"></a>as_numpy_iterator</h3><p>返回一个数据集的 numpy 的迭代器。用来监视数据中的内容。</p>
<p>如果是查看数据集元素的形状和类型，直接打印元素就可以。</p>
<h3 id="batch"><a href="#batch" class="headerlink" title="batch"></a>batch</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch(</span><br><span class="line">    batch_size, drop_remainder=<span class="literal">False</span>, num_parallel_calls=<span class="literal">None</span>, deterministic=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>将数据集中连续的元素打包成 batch。</p>
<h3 id="list-files"><a href="#list-files" class="headerlink" title="list_files"></a>list_files</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line">list_files(</span><br><span class="line">    file_pattern, shuffle=<span class="literal">None</span>, seed=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>创建一个包含匹配该模式的所有文件的数据集。</p>
<h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">map</span>(</span><br><span class="line">    map_func, num_parallel_calls=<span class="literal">None</span>, deterministic=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>把 <code>map_func</code> 作用到数据集每一个元素上。</p>
<h3 id="take"><a href="#take" class="headerlink" title="take"></a>take</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">take(</span><br><span class="line">    count</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>从数据集中取出 <code>count</code> 个组成一个数据集。</p>
<h2 id="experimental"><a href="#experimental" class="headerlink" title="experimental"></a>experimental</h2><h3 id="Counter"><a href="#Counter" class="headerlink" title="Counter"></a>Counter</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.data.experimental.Counter(</span><br><span class="line">    start=<span class="number">0</span>, step=<span class="number">1</span>, dtype=tf.dtypes.int64</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>创建一个从 <code>start</code> 以步长 <code>step</code> 计数的数据集。</p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title>mit机器学习cv</title>
    <url>/2021/08/10/mit-cv/</url>
    <content><![CDATA[<p>暑假上了mit的机器学习课程，虽然时间很短内容不多，但觉得第二部分cv的老师讲得很好，所以做一篇笔记。</p>
<a id="more"></a>
<h1 id="lecture-1"><a href="#lecture-1" class="headerlink" title="lecture 1"></a>lecture 1</h1><p>第一节课是对 deep learning 和 computer vision 的一个介绍。计算机视觉，就是指观察图像，像素，理解图像里出现了是什么，预测会发生什么。</p>
<h2 id="神经元（Perceptron）"><a href="#神经元（Perceptron）" class="headerlink" title="神经元（Perceptron）"></a>神经元（Perceptron）</h2><p>具体概念已经看了太多遍了，不再赘述。</p>
<p>对于优化的概念，$W \leftarrow W - \eta \frac{\partial J(W)}{\partial W}$，他解释的很好。这个偏导项就是代价函数随着 $W$ 的变化增长最快的方向，再加上一个小步长，就是移动一点点，因为是最小值所以是反方向移动，是负号。</p>
<h2 id="边缘检测-Edges"><a href="#边缘检测-Edges" class="headerlink" title="边缘检测 Edges"></a>边缘检测 Edges</h2><p>图上的边特征：</p>
<ul>
<li>图像梯度 Image gradient: $\nabla I = \left( \dfrac{\partial I}{\partial x}, \dfrac{\partial I}{\partial y} \right)$</li>
<li>图像导数的估算 Approximation image derivative: $\dfrac{\partial I}{\partial x} \simeq I(x, y) - I(x - 1, y)$</li>
<li>边缘强度 Edge strength: $E(x, y) = |\nabla I(x, y)|$</li>
<li>边缘取向 Edge orientation: $\theta(x, y) = \angle \nabla I = \arctan{\dfrac{\partial I /\partial y}{\partial I / \partial x}}$</li>
<li>边缘法线 Edge normal: $n = \dfrac{\nabla I}{|\nabla I|}$</li>
</ul>
<p>如果我们使用梯度算法，并绘制出边缘的强度，我们可以看到所有的边缘都暴露出来了。我们可以将这些微分方程或公式转换成图像强度的离散线性约束，这对于定义图像变化非常重要。</p>
<h2 id="卷积-Convolution"><a href="#卷积-Convolution" class="headerlink" title="卷积 Convolution"></a>卷积 Convolution</h2><p>卷积是一个操作两个函数的数学操作，输入两个函数输出一个（函数复合）。</p>
<p>通过滑动函数来计算，一个函数（kernel）划过另一个函数（template），在每一步把他们相乘再相加。</p>
<script type="math/tex; mode=display">
f[n] = h \circ g = \sum\limits_{k = 0}^{N - 1} h[n - k] g[k]</script><p>我们就可以用卷积来计算图像导数。</p>
<p>比如用</p>
<script type="math/tex; mode=display">
(E \otimes k)(x, y)=\sum\limits_{i=0}^{m-1} \sum\limits_{j=0}^{n-1} E(x+i, y+j) k(i, j)</script><p>能用来计算</p>
<script type="math/tex; mode=display">
\dfrac{\partial E(x, y)}{\partial x}=E(x+1, y)-E(x, y)</script><p>当 $k = [-1, 1]$。</p>
<p>然后我们可以对图像导数的定义再扩展，运用卷积计算更加复杂的一些特征。</p>
<p><img data-src="image_dev.png" alt></p>
<p>然后已知两个方向的导数，就可以用三角函数计算出任意方向的导数而不是真的创造任意方向的导数。</p>
<p>将导数可视化：</p>
<p><img data-src="Screenshot from 2021-08-10 18-51-05.png" alt></p>
<p>甚至我们可以创造不同类型的过滤器，不只是计算导数，实现更多的图像处理技术。</p>
<p><img data-src="Screenshot from 2021-08-10 18-57-36.png" alt></p>
<p><em>这是我对卷积和图像处理认识的第一步，我感到非常震撼，原来我们所在ps等图像处理软件上所做的处理，其原理真的就是矩阵对于像素的计算。不由得想到jyy的名言：计算机世界没有魔法(。</em></p>
<p>而我们在这节课学的是如何去学习卷积的参数而不是导数卷积，也就是说找到最好的 kernel。</p>
<p>我们可以把很多层卷基层堆积在一起，形成一个卷积块（我自己起的名字），维度是 $H \times W \times D$，$D$ 深度是过滤层的个数。</p>
<p>步幅（stride）。</p>
<p>卷基层的特点：</p>
<ul>
<li>空间不变性（Spatial invariance）</li>
<li>批量处理，高效并行</li>
<li>图像过滤</li>
<li>共享参数，高效</li>
<li>多种大小输入</li>
</ul>
<p><strong>gabor filters</strong></p>
<h2 id="池化-Pooling"><a href="#池化-Pooling" class="headerlink" title="池化 Pooling"></a>池化 Pooling</h2><p>Pooling 和 convolution 不一样的地方是，卷积在于从一块像素中提取信息，而 pooling 是为了将信息压缩，使得一块像素塌缩成一个单位，也是对数据的抽样，对信息流的限制。</p>
<p><strong>max pooling</strong></p>
<h2 id="结合起来"><a href="#结合起来" class="headerlink" title="结合起来"></a>结合起来</h2><p>我们就可以叠buff，一层conv一层relu一层pooling，一层conv一层relu一层pooling。。。实现一个端对端模型，一层层提取结构。</p>
<p>General CNN architecture:</p>
<p><img data-src="Screenshot from 2021-08-10 21-35-47.png" alt></p>
<h1 id="Lecture-2"><a href="#Lecture-2" class="headerlink" title="Lecture 2"></a>Lecture 2</h1><h2 id="CNN-Architectures"><a href="#CNN-Architectures" class="headerlink" title="CNN Architectures"></a>CNN Architectures</h2><ul>
<li>LeNet: LeCun et al. 1998. <ul>
<li>第一个视觉神经网络。</li>
</ul>
</li>
<li>AlexNet: Krizhevsky et al. NeurIPS 2012. <ul>
<li>两个信息流并行，更深度。</li>
</ul>
</li>
<li>GoogLeNet/Inception: Szegedy et al. CVPR 2015. <ul>
<li>Inception module</li>
<li>更小的卷积，更深，更精确。</li>
</ul>
</li>
<li>VGGNet: Simonyan et al. ICLR 2015.</li>
</ul>
<p><del>CV历史</del></p>
<h2 id="残差连接-Scaling-CNNs-residual-connections"><a href="#残差连接-Scaling-CNNs-residual-connections" class="headerlink" title="残差连接 Scaling CNNs: residual connections"></a>残差连接 Scaling CNNs: residual connections</h2><p>普通的卷积层的代价函数过于复杂，容易陷入局部最优，很难找到全局最优。简单的卷积加池化无法使模型更好。</p>
<p>我们需要学习残差的变化而不是实际的端对端函数。</p>
<h2 id="残差区块-Residual-blocks"><a href="#残差区块-Residual-blocks" class="headerlink" title="残差区块 Residual blocks"></a>残差区块 Residual blocks</h2><p><img data-src="Screenshot from 2021-08-10 22-47-54.png" alt></p>
<p>不是只学习从 $x$ 到输出，我们学习的是变化、残差，以达到输出的目的。它能够把问题变得更简单。</p>
<p>优点：</p>
<ul>
<li>更快的梯度传输。</li>
<li>只应用少量的残余值而不是整个函数值。</li>
<li>保持输入的结构。</li>
</ul>
<p>当输出和输入的结构不一样时，加一个权重层来投影到正确的维度。</p>
<p>有各种各样的残差连接。</p>
<ul>
<li>ResNet: He et al. CVPR 2016.<ul>
<li>使得机器图片识别准确性开始超过人类。</li>
<li>突破计算机可以训练的层数，$25 \to 150$。</li>
<li>代价函数更佳平滑，容易找到全局最优。</li>
</ul>
</li>
</ul>
<h2 id="数据集-Dataset"><a href="#数据集-Dataset" class="headerlink" title="数据集 Dataset"></a>数据集 Dataset</h2><p>深度学习不是什么都能学的，要仔细选择数据集，不能往里面“扔垃圾”。</p>
<blockquote>
<p>Garbage in, Garbage out.</p>
</blockquote>
<ul>
<li>MNIST</li>
<li>Image NET</li>
<li>CIFAR  10/100<ul>
<li>Facet: tool for vusualization of train data</li>
</ul>
</li>
<li>Object Net</li>
<li>MiniPlaces: scene recognition</li>
</ul>
<h1 id="Lecture-3"><a href="#Lecture-3" class="headerlink" title="Lecture 3"></a>Lecture 3</h1><p>本节主要讲图像序列，连续图像的处理。</p>
<h2 id="循环神经网络-Recurrent-neural-network"><a href="#循环神经网络-Recurrent-neural-network" class="headerlink" title="循环神经网络 Recurrent neural network"></a>循环神经网络 Recurrent neural network</h2><p>为了建模序列，我们需要：</p>
<ol>
<li>处理变化长度的序列。</li>
<li>跟踪长期的依赖关系。</li>
<li>保持信息的时间顺序。</li>
<li>在序列中的共享参数。</li>
</ol>
<p>原本我们的模型都是一对一的，现在我妈们可以考虑一对多、多对一、多对多预测，多对多分类。我们需要更新我们的模型，使得能够整合连续序列信息。</p>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>老师在这门课上并没有详细地讲解 RNN 的数学理论，简单介绍一下。</p>
<p>在 RNN 中，我们有时间 $t$ 这个概念，在每个时间点会有一个输入 $x_t$，然后有一个隐藏状态 $h_t$ 在每个时间点，状态序列就通过一个循环过程产生：</p>
<script type="math/tex; mode=display">
h_t = f_W(h_{t - 1}, x_t)</script><p>注意每个时间点所用的函数都是一样（保持时间对称性），也就是当前状态由前一个状态和当前输入决定，其实很像数电中状态机、时序的概念。</p>
<p>具体的，会由两个矩阵来线性组合出当前状态：</p>
<script type="math/tex; mode=display">
h_t = \tanh{(W_{hh}^Th_{t-1} + W_{xh}^{T}x_t)}\\
\hat{y}_t = W_{hy}^{T}h_t</script><p>这个 $\hat{y}$ 是对当前状态产生的一个输出，因为 $h$ 可以看作只是状态的一个编码。</p>
<p>然后其中的权重矩阵就会由学习得到。</p>
<p><img data-src="Screenshot from 2021-08-11 18-07-27.png" alt></p>
<p>如果我们考虑将所有时间的信息都记录下来，长距离的依赖会导致问题：</p>
<ul>
<li>需要记录的东西和内存会随着时间不断增加。</li>
<li>没有无限大的参数集来建模所有依赖。</li>
<li>RNN假设：当前隐藏层状态只依赖于前一个时间点的状态。</li>
<li>想法是建立隐藏状态来建模长距离的依赖。</li>
</ul>
<p>这些想法促使了我们需要一个有更好建模长距离依赖能力的新模型架构。</p>
<h3 id="长短时记忆单元-Long-short-term-memory-unit"><a href="#长短时记忆单元-Long-short-term-memory-unit" class="headerlink" title="长短时记忆单元 Long short term memory unit"></a>长短时记忆单元 Long short term memory unit</h3><p>也没有仔细描述 LSTM 的细节，主要思想是 not to forget &amp; learn to forget。</p>
<p>LSTM 使用<strong>门（gates）</strong> 来控制信息流：</p>
<ul>
<li><strong>遗忘</strong> 来减少无关信息</li>
<li><strong>储存</strong> 相关信息从当前输入</li>
<li>选择性 <strong>更新</strong> 单元状态</li>
<li><strong>输出</strong>一个过滤版本状态</li>
</ul>
<h2 id="RNNs-CNNs"><a href="#RNNs-CNNs" class="headerlink" title="RNNs + CNNs"></a>RNNs + CNNs</h2><p>核心思想就是 CNNs 来学习出2D图像的特征提取器，压缩后的特征喂入 RNNs 分析序列。</p>
<p><img data-src="Screenshot from 2021-08-11 18-47-07.png" alt></p>
<h2 id="CV中的应用"><a href="#CV中的应用" class="headerlink" title="CV中的应用"></a>CV中的应用</h2><h3 id="视频分类"><a href="#视频分类" class="headerlink" title="视频分类"></a>视频分类</h3><p><img data-src="Screenshot from 2021-08-11 21-44-11.png" alt></p>
<p>有多种整合特征的方法。</p>
<h3 id="图像标题-imgae-captioning"><a href="#图像标题-imgae-captioning" class="headerlink" title="图像标题 imgae captioning"></a>图像标题 imgae captioning</h3><p>目标是生成准确捕捉图像内容的句子。</p>
<ul>
<li>生成模型</li>
</ul>
<p><img data-src="Screenshot from 2021-08-11 21-44-40.png" alt></p>
<h3 id="动作预测"><a href="#动作预测" class="headerlink" title="动作预测"></a>动作预测</h3><p><img data-src="Screenshot from 2021-08-11 21-53-42.png" alt></p>
<h1 id="Lecture-4"><a href="#Lecture-4" class="headerlink" title="Lecture 4"></a>Lecture 4</h1><p>本章主要介绍<strong>图像生成模型（Generative Image Modeling）</strong>。</p>
<h2 id="生成模型-Generative-modeling"><a href="#生成模型-Generative-modeling" class="headerlink" title="生成模型 Generative modeling"></a>生成模型 Generative modeling</h2><p>在第二节课中我们学到了图像分类，我们思考能不能将这个一对一映射反过来，根据标签去生成图像。</p>
<p><strong>图像合成（image synthesis）</strong></p>
<p><strong>图像翻译（image translation）</strong></p>
<p>生成模型的目标就是：学习出一个代表并捕捉了一些控制相关数据的潜在概率分布深度学习模型。</p>
<p>在计算机视觉中：</p>
<ul>
<li>学习拟合一个数据流形（data manifold）。</li>
<li>从习得的分布产生新的样例。</li>
</ul>
<p>挑战：</p>
<ul>
<li>高维。</li>
<li>潜在分布的复杂性。</li>
</ul>
<p>所以我们用深度学习来建模因为它的数据驱动的拟合函数方式。</p>
<h2 id="模型架构-Architecture"><a href="#模型架构-Architecture" class="headerlink" title="模型架构 Architecture"></a>模型架构 Architecture</h2><p>分布转化（distribution transformers）</p>
<p><img data-src="Screenshot from 2021-08-12 18-36-59.png" alt></p>
<p>我们需要神经网络上非常聪明的方法，来实现端对端的动态模型。</p>
<p>我们从一个可以使低维的概率分布，习得一个神经网络模型，从原来的分布到目标数据的分布。在这个过程中我们可以生产落在目标数据中的合成图像，这就是生成模型的关键。</p>
<p><img data-src="Screenshot from 2021-08-12 18-43-11.png" alt></p>
<p>关于建立这种模型可以分为两类，一种是明确估计概率函数的，另一种是不明确地学习密度函数的，从隐式的学习到实现生成。我们将会主要讨论 <strong>VAE</strong> 和 <strong>GAN</strong>。</p>
<p><img data-src="Screenshot from 2021-08-12 18-46-16.png" alt></p>
<h3 id="变分自编码-Variational-autoencoders"><a href="#变分自编码-Variational-autoencoders" class="headerlink" title="变分自编码 Variational autoencoders"></a>变分自编码 Variational autoencoders</h3><p>我们是想要从真实的数据空间，模拟生成合成样本或真正数据空间的重构，能够概括数据的形态。</p>
<p><strong>VAE</strong> 先将真实数据编码成低维特征空间或隐空间（latent space），由一组隐变量编码而成，我们可以去学习这个特征空间而不需要任何监督。</p>
<p><strong>VAE</strong> 将问题分成两个部分，一个就是对于原来的高维的真实数据编码（encode）成低维的数据空间，从而来限制提取出有用的特征。基于此，我们要从这个低维的数据空间解码（decode）或习得一个重构的数据空间。</p>
<p>所以对于图像数据，编码和解码网络就可以由卷积结构来构建。</p>
<p><img data-src="Screenshot from 2021-08-12 18-57-17.png" alt></p>
<p>我们算出隐空间 $z$ 的概率分布，根据输入数据 $x$，它将被编码网络的权重矩阵参数化，我们用 $\phi$ 来表示。然后反向地我们可以做出解码网络，根据 $z$ 计算出 $x$，解码参数为 $\theta$。把两部分连起来。</p>
<p>训练的方法就是将两部分整合进代价函数，参数为 $\phi$ 和 $\theta$，一项是正则化项。</p>
<p>为了训练这个模型，我们需要预先给定一些概率分布，这取决于我们希望这些隐变量采取怎样的形态。我们会在每一个隐变量 $z$ 上加上一个高斯分布（Gaussian prior）。</p>
<p>为了不是绝对重建，我们还得想办法从 $z$ 中取样，我们要参数化 $z$，用均值和标准差根据之前的正太高斯分布，把标准差按照一个噪声因子进行缩放，该因子包含了随机性、概率元素和变分元素，使我们可以用 <strong>VAE</strong> 生成合成的数据实例。</p>
<p>更具体地说，当我们施加高斯分布，并将隐变量参数化为平均值和标准差并缩放一个噪声系数，这样子我们就可以算出代价函数，根据重建项和一个正则化项，其中正则化项可以得到我们根据隐变量学习解码出的结果和根据高斯分布指定的隐变量分布形态之间的差距。</p>
<script type="math/tex; mode=display">
\mathcal{L}(\phi, \theta, x) = \| x - \hat{x} \|^2 + D(q_\phi(z|x) | p(z))</script><script type="math/tex; mode=display">
z = \mu + \sigma \odot \varepsilon</script><p>这是一个很好的可跟踪的方法，来检查网络并理解网络到底学了什么信息。我们要做的，就是<strong>潜变量摄动（latent variable perturbation）</strong>就是改变 $z$ 然后观察输出结果，得到前变量的属性，是否相互独立等。</p>
<p>总结：</p>
<ol>
<li>压缩数据的表示到低维空间。</li>
<li>重建允许了无监督学习。</li>
<li>重参数化技巧来训练。</li>
<li>通过摄动解释潜变量。</li>
<li>生成新实例。</li>
</ol>
<h3 id="生成对抗网络-Generative-adversarial-networks"><a href="#生成对抗网络-Generative-adversarial-networks" class="headerlink" title="生成对抗网络 Generative adversarial networks"></a>生成对抗网络 Generative adversarial networks</h3><p>核心理念是：<br>我们先从完全高斯噪声作为初始的 $z$ 分布开始，去生成符合的图像数据。制定一条端到端的流水线，它们由两个网络组成，一个是<strong>生成器（generator）</strong>，一个是<strong>识别器（discriminator）</strong>，它们在一个敌对博弈中相互竞争。我们从一个高斯噪声输入开始，生成器生成合成图像，然后被喂到分类网络中，这个网路被训练去思考这个实例是真实的或人造的。整个过程通过迭代，训练识别器和生成器通过相互竞争实现优化。全局最优就是识别器识别不出来了。</p>
<p><img data-src="Screenshot from 2021-08-13 00-33-13.png" alt></p>
<p>从高斯噪声开始，生成器生成的合成图片被输入识别器，同时真实的图片也会被输入识别器，识别器被训练来判断输入的图像是真的还是假的，对于我们可以优化的损失这些损失都是交叉熵损失：</p>
<script type="math/tex; mode=display">
\arg \max\limits_{D} \mathbb{E}_{\mathbf{z}, \mathbf{x}}[\log D(G(\mathbf{z}))+\log (1-D(\mathbf{x}))</script><p>前一项描述的是识别器对于生成器的图像的真假判断的概率，后一项刻画的是一个给定的图片是否与一个真标签相关联。我们利用交叉熵损失和最大化分离目标来训练识别器。</p>
<script type="math/tex; mode=display">
\arg \min\limits_{G} \mathbb{E}_{\mathbf{z}, \mathbf{x}}[\log D(G(\mathbf{z}))+\log (1-D(\mathbf{x}))]</script><p>相反地，我们可以用同样的交叉熵，用最小化代替最大化，也就是说让它与识别器为敌。</p>
<p>我们可以通过内插，在高斯噪声中的两个数据点，实现由 GAN 生成的合成图像的扰动，观察其在学习到的数据空间流形中的距离，来调试 GAN。优点是保证了映射的条理性。</p>
<p>对传统 GAN 的优化：</p>
<ul>
<li><p>StyleGAN, Karras+ 2019</p>
</li>
<li><p>我看不懂（</p>
</li>
</ul>
<p><img data-src="Screenshot from 2021-08-13 00-54-23.png" alt></p>
<h2 id="Recent-Advances"><a href="#Recent-Advances" class="headerlink" title="Recent Advances"></a>Recent Advances</h2><ul>
<li>Paired translation<ul>
<li>pix2pix</li>
</ul>
</li>
<li>CycleGAN: domain transfer</li>
</ul>
<p><img data-src="Screenshot from 2021-08-13 01-05-39.png" alt></p>
<h1 id="Lecture-5"><a href="#Lecture-5" class="headerlink" title="Lecture 5"></a>Lecture 5</h1><p>本章主要讲的是神经绘图和渲染（Neural Graphics and Rendering）。</p>
<p><strong>神经渲染（Neural Rendering）</strong>：是将计算机图形学的物理知识和生成模型机器学习结合起来，通过融合不同的渲染到网络训练中。</p>
<ul>
<li>生成网络负责合成原始像素。</li>
<li>可解释的参数来控制输出。</li>
<li>光影，相机，位置，动作，结构。。</li>
</ul>
<h2 id="经典计算机图形学-Classical-computer-graphics"><a href="#经典计算机图形学-Classical-computer-graphics" class="headerlink" title="经典计算机图形学 Classical computer graphics"></a>经典计算机图形学 Classical computer graphics</h2><p><img data-src="Screenshot from 2021-08-13 16-28-23.png" alt></p>
<p>两个方向，一个forward从模型到图片的渲染，一个backward从图片到模型的重建。</p>
<p>forward 过程，对于一个模型，它可能有很多属性，光源、材料、相机等等，通过 CG 计算机绘图的函数，输出一张合理的从照相机视角看到的一个图片。</p>
<h3 id="渲染方程"><a href="#渲染方程" class="headerlink" title="渲染方程"></a>渲染方程</h3><p>有一个渲染方程不过我看不懂：</p>
<script type="math/tex; mode=display">
L_{\mathrm{o}}\left(\mathbf{p}, \omega_{\mathrm{o}}\right)=L_{\mathrm{e}}\left(\mathbf{p}, \omega_{\mathrm{o}}\right)+\int_{\Omega} L_{\mathrm{i}}\left(\mathbf{p}, \omega_{\mathrm{i}}\right) f_{\mathrm{r}}\left(\mathbf{p}, \omega_{\mathrm{i}}, \omega_{\mathrm{o}}\right)\left(\omega_{\mathrm{i}} \cdot \mathbf{n}\right) \mathrm{d} \omega_{\mathrm{i}}</script><p>简单理解就是等号左边是 Outgoing radiance，该点的出射光；右边第一项是该点的光源；积分项是入射光和一个 BRDF函数。总之这个方程非常强大，可以递归。</p>
<h3 id="BRDF"><a href="#BRDF" class="headerlink" title="BRDF"></a>BRDF</h3><p>它可以捕捉材料的变化，就可以处理反射特性，进行材料光泽的渲染。</p>
<h2 id="神经图像表示-Neural-scene-representations"><a href="#神经图像表示-Neural-scene-representations" class="headerlink" title="神经图像表示 Neural scene representations"></a>神经图像表示 Neural scene representations</h2><h3 id="语义渲染-Semantic-rendering"><a href="#语义渲染-Semantic-rendering" class="headerlink" title="语义渲染 Semantic rendering"></a>语义渲染 Semantic rendering</h3><p>思想是通过一种语义合理的方式，控制并合成场景的外观。</p>
<p><img data-src="Screenshot from 2021-08-13 17-01-47.png" alt></p>
<h3 id="pix2pix-生成模型"><a href="#pix2pix-生成模型" class="headerlink" title="pix2pix 生成模型"></a>pix2pix 生成模型</h3><p>通过不仅将生成器生成的合成图片喂入识别器，还喂入原始的输入，来让识别器判断真假。</p>
<p>这个模型工作地得很好，但它的问题是不能在高质量图片上表现得不错。</p>
<p>所以我们要增强模型的鲁棒性，渲染更高质量的图片。</p>
<ul>
<li>动态增长模型（progressive growing of generative models）</li>
<li>实时渲染</li>
</ul>
<h3 id="渲染风格的控制"><a href="#渲染风格的控制" class="headerlink" title="渲染风格的控制"></a>渲染风格的控制</h3><p>我们将输入编码到潜空间，单这个潜空间遵循更高维参数的分布，这些高阶参数可以用 <strong>KL分离损失</strong>来约束。</p>
<p>也就是说他们是概率的，我们可以在这个概率分布中取样，决定了渲染的风格，即使他们都是语义正确的。</p>
<p><img data-src="Screenshot from 2021-08-14 01-11-02.png" alt></p>
<h2 id="Implicit-neural-rendering"><a href="#Implicit-neural-rendering" class="headerlink" title="Implicit neural rendering"></a>Implicit neural rendering</h2><p>对于反向重建，是一个端对端的神经渲染训练。</p>
<p><img data-src="Screenshot from 2021-08-14 01-26-54.png" alt></p>
<h3 id="基于图像的复制-Image-based-reprojection"><a href="#基于图像的复制-Image-based-reprojection" class="headerlink" title="基于图像的复制 Image-based reprojection"></a>基于图像的复制 Image-based reprojection</h3><p>场景表示：RGB值+深度</p>
<h3 id="Point-based-rendering"><a href="#Point-based-rendering" class="headerlink" title="Point-based rendering"></a>Point-based rendering</h3><p><strong>神经隐式渲染（Neural implicit rendering）</strong>。</p>
<p><img data-src="Screenshot from 2021-08-14 01-41-48.png" alt></p>
<ul>
<li>如何得到数据？</li>
<li>什么是神经渲染的模型？</li>
</ul>
<h2 id="结合起来-1"><a href="#结合起来-1" class="headerlink" title="结合起来"></a>结合起来</h2><p><img data-src="Screenshot from 2021-08-14 01-57-13.png" alt></p>
<p>总之这节课所讲的内容，太过于前沿，模型太过于复杂，我基本上属于听不懂的状态，很难跟上，只能放一些slides的思路图。希望以后能懂orz。</p>
<h1 id="Lecture-6"><a href="#Lecture-6" class="headerlink" title="Lecture 6"></a>Lecture 6</h1><p>这节课主要讲神经视觉应用。</p>
<h2 id="目标检测-Object-detection"><a href="#目标检测-Object-detection" class="headerlink" title="目标检测 Object detection"></a>目标检测 Object detection</h2><p>目标检测指的是不仅输出图片的标签，还捕捉到这个目标在图像中的区块</p>
<h3 id="区域卷积神经网络-Regional-CNN"><a href="#区域卷积神经网络-Regional-CNN" class="headerlink" title="区域卷积神经网络 Regional CNN"></a>区域卷积神经网络 Regional CNN</h3><p>提取固定数量的区域，改变形状大小，通过 CNN 来分裂每个区域。</p>
<p>问题：</p>
<ul>
<li>训练缓慢因为每个图片要训练2000个区域。</li>
<li>不是实时的，每个测试图片有延迟。</li>
<li>区域的选择是经验性的，可能对于不同的训练集不能选到很好的区域。</li>
</ul>
<p>所以有了 Faster R-CNN，通过输入卷基层，只需要训练一次，数据驱动的区域选择。</p>
<p><img data-src="Screenshot from 2021-08-14 16-55-29.png" alt></p>
<h3 id="区域建议网络-Region-Proposal-Network"><a href="#区域建议网络-Region-Proposal-Network" class="headerlink" title="区域建议网络 Region Proposal Network"></a>区域建议网络 Region Proposal Network</h3><p>通过卷基层我们可以提取出一个 $m\times n$ 的特征映射（feature maps），图片中的每一个区域中心都可以被特征映射中的一个像素表示。</p>
<p>我们的 <strong>RPN</strong> 就是要找到 k 个目标块（anchor boxes）。</p>
<ul>
<li>辨别出块是是好的还是坏的。</li>
<li>在块上进行回归，来修改位置，宽和高。</li>
</ul>
<p>然后我们可以设计出代价函数：</p>
<script type="math/tex; mode=display">
L(\{p_i\}, \{ t_i\}) = \dfrac{1}{N_{cls}}\sum\limits_{i} L_{cls}(p_i, p_i^{*}) + \lambda \dfrac{1}{N_{reg}} p_i^{*} L_{reg} (t_i, t_i^{*})</script><p>第一部分是我们把区块中存在目标的真实概率分布与区块中存在目标的预测分布的差距最小化，第二部分是区块的真实位置和预测位置的回归损失。</p>
<h2 id="语义分割-Semantic-segmentation"><a href="#语义分割-Semantic-segmentation" class="headerlink" title="语义分割 Semantic segmentation"></a>语义分割 Semantic segmentation</h2><p>在高维上识别出每个像素是啥。</p>
<h3 id="全卷积神经网络-Fully-CNNs"><a href="#全卷积神经网络-Fully-CNNs" class="headerlink" title="全卷积神经网络 Fully CNNs"></a>全卷积神经网络 Fully CNNs</h3><p>在卷基层后不再是连到 Fully connected nn，而是通过 unpolling 等来up sampling，来增加视觉维度。</p>
<p><img data-src="Screenshot from 2021-08-14 22-45-32.png" alt></p>
<p>Unpooling 有几种：</p>
<ul>
<li>Nearest Neighbor</li>
<li>Bed of Nails</li>
</ul>
<p>代价函数我们用二元交叉熵：$- \sum\limits_{classes} y_{true} \log{(y_{pred})}$</p>
<p>当我们需要对边缘进行更精确的预测时，加大边缘的权重。</p>
<p>因为我们的编码器会把图像编码到更低维的特征空间，所以导致解码器很难捕捉到足够的特征。就有了<strong>跳跃连接（skip connect）</strong>，把编码部分的卷基层连接到解码部分对应的卷基层，避免了信息的损失。这个思想很类似残差网络。</p>
<p><img data-src="Screenshot from 2021-08-14 22-56-52.png" alt></p>
<h3 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h3><p><strong>U-net</strong> 就是一种通过大规模的跳跃连接来在学习过程中分享细节的架构。</p>
<p><img data-src="Screenshot from 2021-08-14 22-59-50.png" alt></p>
<h2 id="自监督学习-Self-supervised-learning"><a href="#自监督学习-Self-supervised-learning" class="headerlink" title="自监督学习 Self-supervised learning"></a>自监督学习 Self-supervised learning</h2><h3 id="深度估计-Depth-estimation"><a href="#深度估计-Depth-estimation" class="headerlink" title="深度估计 Depth estimation"></a>深度估计 Depth estimation</h3><p>在高维上识别出每个像素深度是多少。</p>
<p>因为深度是一个实数，所以是一个回归问题，我们一般用平方代价函数：$\text{MSE} = \dfrac{1}{N} \sum\limits_{i=1}^{N} (y_i - \hat{y}_i)^2$。</p>
<p>但是对于一个监督学习，我们需要知道真实的标签，也就是真实的深度，但这是很难办到的。我们需要一些其他的技术来得到深度。</p>
<h3 id="先验知识"><a href="#先验知识" class="headerlink" title="先验知识"></a>先验知识</h3><p>在没有标记数据的情况下，我们可以注入一些真实世界的先验知识。</p>
<ul>
<li><strong>视察（disparity）</strong>和深度成反比。</li>
<li>通过左右眼视察算出深度。</li>
</ul>
<p>我们就可以用这些知识来<strong>自监督</strong>。</p>
<h3 id="自监督"><a href="#自监督" class="headerlink" title="自监督"></a>自监督</h3><p><img data-src="Screenshot from 2021-08-14 23-32-38.png" alt></p>
<p>这是一个自监督单眼深度模型，我们用一个左眼摄像头作为输入，这很好得到。</p>
<p>我们要训练一个模型来预测我们希望看到的视察图，但这是没有办法被优化的，单纯只由网络预测。</p>
<p>但是模型和图的关系才是最重要的，我们通过把输入的左眼图和预测的视察图结合起来，就可以估算出右眼图。然后我们就可以用左右眼摄像机来训练这个网络，到达自我优化。</p>
<p>在测试的时候，我们只关心视察层，也就是输出的视察图是否准确。这太 amazing 了，这意味着我们只使用一张图片且没有任何标签，就可以估算深度。它运行地也非常快，因为我们只需要运行一半的网络。</p>
<p>没有传统的配对调整。</p>
<h1 id="Lecture-7"><a href="#Lecture-7" class="headerlink" title="Lecture 7"></a>Lecture 7</h1><p>本节主要讲了计算机视觉中的可解释性和不确定性。</p>
<h2 id="可解释性和可视化-Interpretability-and-Visualization"><a href="#可解释性和可视化-Interpretability-and-Visualization" class="headerlink" title="可解释性和可视化 Interpretability and Visualization"></a>可解释性和可视化 Interpretability and Visualization</h2><p>像素级的解释：</p>
<ul>
<li>哪些像素在图中是最重要的。</li>
<li>如果移除了这些像素预测将会发生变化。</li>
</ul>
<p>一个神经网络有百万个参数，如何判断它们控制哪些模式？</p>
<p>对于一个卷基层，我们可以把它可视化。我们要怎么把多个 feature maps 结合成一个可视化的特征？</p>
<h3 id="视觉反向传播VisualBackprop"><a href="#视觉反向传播VisualBackprop" class="headerlink" title="视觉反向传播VisualBackprop"></a>视觉反向传播VisualBackprop</h3><ul>
<li>平均（average）</li>
<li><p>扩大（upscale）</p>
</li>
<li><p>相乘（multiply）</p>
</li>
</ul>
<p><img data-src="Screenshot from 2021-08-15 18-45-33.png" alt></p>
<p>我们最终会得到一个可视化掩模（mask），和我们的输入图像在相同的维度上。</p>
<p>通过这种方式我们可以生成输入图像中不同像素特性的可视化。</p>
<h3 id="类激活映射-Class-Activation-Maps"><a href="#类激活映射-Class-Activation-Maps" class="headerlink" title="类激活映射 Class Activation Maps"></a>类激活映射 Class Activation Maps</h3><p>思考更新参数时 $W \leftarrow W - \eta \dfrac{\partial J(W, x, y)}{\partial W}$，一个参数的改变是如何增加我们的损失。</p>
<p><strong>CAM</strong> 的想法是让图像通过一系列的卷积层，从而产生一系列的特征向量。然后我们要做的是有一个<strong>全局平均池化操作（Global average pooling）</strong>，所有的特征映射被聚合，也就是说这个向量的每个分量就是所有特征向量的平均值。 然后我们可以考虑网络上每一个权重的聚合值，其最终输出，然后把这些权重乘以激活映射，生成最终的聚合激活映射。</p>
<p><img data-src="Screenshot from 2021-08-15 22-06-36.png" alt></p>
<p>它捕获了图像的各个区域，导致一系列的激活和特征映射，有助于网络的决策。</p>
<p>这个 GAP 会完全减少空间的信息，只保留了深度维度，导致了特征映射和类别之间的对应。</p>
<h2 id="不确定性估计-Uncertainty-estimation"><a href="#不确定性估计-Uncertainty-estimation" class="headerlink" title="不确定性估计 Uncertainty estimation"></a>不确定性估计 Uncertainty estimation</h2><p>不确定性的种类</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Known Knowns</td>
<td style="text-align:center">Known Unknows</td>
</tr>
<tr>
<td style="text-align:center">Unknown Known</td>
<td style="text-align:center">Unknown Unknows</td>
</tr>
</tbody>
</table>
</div>
<p>最主要两个方面的不确定性：</p>
<ul>
<li>任意的不确定性（aleatoric）<ul>
<li>数据的不确定性</li>
<li>描述了输入数据的信心</li>
<li>输入有噪声时高</li>
</ul>
</li>
<li>认知的不确定性（epistemic）<ul>
<li>模型的不确定性</li>
<li>描述了预测的信心（confidence）</li>
<li>当缺少训练数据时很高</li>
<li>通过增加数据来减少</li>
</ul>
</li>
</ul>
<p><strong>Aleatoric uncertainty</strong> 可以直接通过神经网络来学习。</p>
<p><strong>Epistemic uncertainty</strong> 很难用神经网络学习，我们不去训练确定性网络，而训练<strong>贝叶斯神经网络（Bayesian NN）</strong>。</p>
<h3 id="BNN"><a href="#BNN" class="headerlink" title="BNN"></a>BNN</h3><p>对网络中每一个权值建立概率分布模型，然后根据正态分布将这些权重参数化，我们要训练这个模型来预测一个平均值、一个不变值，它定义了网络上的概率分布。这意味着我们要学习网络权值的后验（posterior）概率分布，$P(W | X, Y) = \dfrac{P(Y|X,W) P(W)}{P(Y|X)}$。</p>
<p>但是这在分析上是不可解的，所以我们试图近似。</p>
<p><strong>dropout</strong>，每次前向传播中随机选择一些权重剔除，按照伯努利分布 $z_{w,t} \sim Bernoulli(p), \forall w  \in W$。</p>
<p><img data-src="Screenshot from 2021-08-15 23-13-44.png" alt></p>
<h1 id="Lecture-8"><a href="#Lecture-8" class="headerlink" title="Lecture 8"></a>Lecture 8</h1><p>本章最后一节，主要讨论计算机视觉的进展，回顾一下这门课所学。</p>
<p>主要是解决偏差的方法。</p>
<ul>
<li>Adversarial debiasing</li>
<li>Debiasing variational autoencoder</li>
</ul>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
  </entry>
  <entry>
    <title>吴恩达深度学习笔记</title>
    <url>/2021/08/04/DL-AN/</url>
    <content><![CDATA[<p><a href="https://www.bilibili.com/video/BV1FT4y1E74V?from=search&amp;seid=7469215768123017337">视频指路</a></p>
<a id="more"></a>
<hr>
<h1 id="Lesson-1-1"><a href="#Lesson-1-1" class="headerlink" title="Lesson 1.1"></a>Lesson 1.1</h1><p>本章是对深度学习神经网络的一个介绍，课程的概览。</p>
<h1 id="Lesson-1-2"><a href="#Lesson-1-2" class="headerlink" title="Lesson 1.2"></a>Lesson 1.2</h1><p>本章主要讲了一下 logisitic 回归，梯度下降，在ML中已经学习了。</p>
<p>然后介绍了 <code>python</code> 中的一些知识，向量化、广播、<code>numpy</code>、jupyter notebook等，都可以 <strong>rtfm</strong>，不在此赘述。</p>
<p>唯一有一点就是符号上的表示，andrew 喜欢用 $m$ 表示样本个数，$n$ 表示特征数。我习惯 mit 的课教的 $n$ 表示样本数，$d$ 表示特征维数。</p>
<h1 id="Lesson-1-3"><a href="#Lesson-1-3" class="headerlink" title="Lesson 1.3"></a>Lesson 1.3</h1><p>本章算是一个神经网络的引入，介绍一些基本概念，浅层神经网络（shallow neural network）。</p>
<p><img data-src="L1_week3_6.png" alt></p>
<p>每个神经单元就是这么个计算过程，有输入 $x$，权重 $w$，偏移 $b$，激活函数（activation function） $h$。</p>
<ul>
<li>$z = w^T x + b$</li>
<li>$a = h(z)$</li>
</ul>
<p>符号： $a_1^{<a href="1">1</a>} $ ，右上角中括号表示层数，右上角括号中表示第几个样本，右下角表示该层第几个神经元。</p>
<p>对于一个输入样本，避免一层内的 for 循环，向量化计算：</p>
<script type="math/tex; mode=display">
z^{[i]} = W^{[i]} a^{[i-1]} + b^{[i]}\\
a^{[i]} = h(z^{[i]})</script><p>$W^{[i]}$ 是第 $i$ 层每个神经元的权重排成的矩阵。</p>
<p>举例图示：</p>
<script type="math/tex; mode=display">
\left[
        \begin{array}{c}
        z^{[1]}_{1}\\
        z^{[1]}_{2}\\
        z^{[1]}_{3}\\
        z^{[1]}_{4}\\
        \end{array}
        \right]
         =
    \overbrace{
    \left[
        \begin{array}{c}
        ...W^{[1]T}_{1}...\\
        ...W^{[1]T}_{2}...\\
        ...W^{[1]T}_{3}...\\
        ...W^{[1]T}_{4}...
        \end{array}
        \right]
        }^{W^{[1]}}
        *
    \overbrace{
    \left[
        \begin{array}{c}
        x_1\\
        x_2\\
        x_3\\
        \end{array}
        \right]
        }^{input}
        +
    \overbrace{
    \left[
        \begin{array}{c}
        b^{[1]}_1\\
        b^{[1]}_2\\
        b^{[1]}_3\\
        b^{[1]}_4\\
        \end{array}
        \right]
        }^{b^{[1]}}</script><p>对于所有样本，避免 for 遍历样本，向量化计算：</p>
<script type="math/tex; mode=display">
Z^{[i]} = W^{[i]}A^{[i-1]} + b^{[i]}\\
A^{[i]} = h(Z^{[i]})</script><p>其中 $Z^{[i]},A^{[i]}$ 是第 $i$ 层所有样本输出排成的矩阵。</p>
<p>激活函数：<strong>sigmoid</strong>（主要在二分类）, <strong>tanh</strong>（比 sigmoid 常用），<strong>ReLU</strong>（在神经网络中很常用），Leaky ReLU。</p>
<p>前两个有梯度消失的风险。</p>
<p>这里提到一个很重要的问题就是为什么要使用非线性函数而不是直接 $a = z$。因为全用线性激活函数（identity）会使神经网络退化成一个单层模型。</p>
<h2 id="反向传播-Back-Propagation"><a href="#反向传播-Back-Propagation" class="headerlink" title="反向传播 Back Propagation"></a>反向传播 Back Propagation</h2><p>也就是神经网络的梯度下降（Gradient Descent）。比较重要，理解一下推倒，本质上是函数求导的链式法则。</p>
<p>代价函数：</p>
<script type="math/tex; mode=display">
J(W, b) = \dfrac{1}{m}\sum\limits_{i= 1}^{m}L(\hat{y}, y)</script><p>当参数初始化成某些值后，每次梯度下降都会循环计算以下预测值：$\hat{y}^{(i)},(i=1,2,…,m)$。</p>
<p>有 $dW^{[i]} = \dfrac{\partial J}{\partial W^{[i]}}$, $d b^{[i]} = \dfrac{\partial J}{\partial b^{[i]}}$。</p>
<p>在梯度下降时每一次更新：$W^{[i]}\implies{W^{[i]} - \eta dW^{[i]}},b^{[i]}\implies{b^{[i]} -\eta db^{[i]}}$, $\eta$ 为步长。</p>
<p>反向传播时，就是一个链式求导：</p>
<script type="math/tex; mode=display">
\underbrace{
    \left.
    \begin{array}{l}
    x \\
    w \\
    b 
    \end{array}
    \right\}
    }_{dw=dz \cdot x, db =dz}
    \impliedby \underbrace{z=w^Tx+b}_{dz=da\cdot g^{'}(z),
    g(z)=\sigma(z),
    \frac{dL}{dz}} = \frac{dL}{da} \cdot \frac{da}{dz},
    \frac{d}{ dz} g(z)=g^{'}(z)
    \impliedby \underbrace{a = \sigma(z) 
    \impliedby L(a,y)}_{da=\frac{d}{da}L\left(a,y \right)=(-y\log{\alpha} - (1 - y)\log(1 - a))^{'}=-\frac{y}{a} + \frac{1 - y}{1 - a} }</script><p>所以有：</p>
<p>$dz^{[L]} = A^{[L]} - Y$</p>
<p>$dW^{[i]} = \dfrac{1}{m} dZ^{[i]}A^{[i-1]T}$</p>
<p>$db^{[i]} = \dfrac{1}{m}$ <code>np.sum(dZ^&#123;[i]&#125;, axis=1)​</code></p>
<script type="math/tex; mode=display">
dz^{[i]} = \underbrace{W^{[i + 1]T} dz^{[i+1]}}_{(n^{[i]},m)}\quad \times  \underbrace{g^{[i]'}}_{activation \; function \; of \; hidden \; layer}\times  \quad\underbrace{(z^{[i]})}_{(n^{[1]},m)}</script><ul>
<li>随机初始化，不要初始化成相同的参数。</li>
</ul>
<h1 id="Lesson-1-4"><a href="#Lesson-1-4" class="headerlink" title="Lesson 1.4"></a>Lesson 1.4</h1><p>本章介绍深层神经网络，主要就是把前一章讲的只有两层的网络更推广一下，而我们在上一章其实已经推广过了。</p>
<h2 id="为什么使用深层表示？"><a href="#为什么使用深层表示？" class="headerlink" title="为什么使用深层表示？"></a>为什么使用深层表示？</h2><p>深度神经网络的这许多隐藏层中，较早的前几层能学习一些低层次的简单特征，等到后几层，就能把简单的特征结合起来，去探测更加复杂的东西。</p>
<p>深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。</p>
<blockquote>
<p>说实话，我认为“深度学习”这个名字挺唬人的，这些概念以前都统称为有很多隐藏层的神经网络，但是深度学习听起来多高大上，太深奥了，对么？这个词流传出去以后，这是神经网络的重新包装或是多隐藏层神经网络的重新包装，激发了大众的想象力。    ——Andrew</p>
</blockquote>
<h2 id="搭建神经网络块"><a href="#搭建神经网络块" class="headerlink" title="搭建神经网络块"></a>搭建神经网络块</h2><p>其实就是对于每一层，权重矩阵，偏移值，激活函数，在前向传播的时候缓存（cache）好 $z,a$ 等值，用反向传播时计算 $dW,db$ 等。</p>
<p>就放一张老师的板书吧（</p>
<p><img data-src="network.png" alt="building blocks"></p>
<h2 id="参数和超参数"><a href="#参数和超参数" class="headerlink" title="参数和超参数"></a>参数和超参数</h2><p>算法中的<strong>learning rate</strong> $a$（学习率）、<strong>iterations</strong>(梯度下降法循环的数量)、$L$（隐藏层数目）、$n^{[l]}$（隐藏层单元数目）、<strong>choice of activation function</strong>（激活函数的选择）都需要你来设置，这些数字实际上控制了最后的参数$W$和$b$的值，所以它们被称作超参数（Hyperparameter）。</p>
<p>如何寻找超参数：走<strong>Idea—Code—Experiment—Idea</strong>这个循环，尝试各种不同的参数，实现模型并观察是否成功，然后再迭代。</p>
<blockquote>
<p>应用深度学习领域，一个很大程度基于经验的过程，凭经验的过程通俗来说，就是试直到你找到合适的数值。</p>
<p>如果你所解决的问题需要很多年时间，只要经常试试不同的超参数，勤于检验结果，看看有没有更好的超参数数值，相信你慢慢会得到设定超参数的直觉，知道你的问题最好用什么数值。</p>
</blockquote>
<hr>
<h1 id="Lesson-2-1"><a href="#Lesson-2-1" class="headerlink" title="Lesson 2.1"></a>Lesson 2.1</h1><p>本章主要讲改善神经网络，超参数调试、正则化等内容。</p>
<ul>
<li>当我们有百万量级以上的数据，可以拿 99% 以上的数据来进行训练，几万条用来交叉验证（dev）和测试就可以了。</li>
</ul>
<h2 id="方差-偏差"><a href="#方差-偏差" class="headerlink" title="方差/偏差"></a>方差/偏差</h2><ul>
<li>高偏差（<strong>high bias</strong>），欠拟合（<strong>underfitting</strong>）。</li>
<li>高方差（<strong>high variance</strong>），过拟合（<strong>overfitting</strong>）。</li>
</ul>
<p>通过训练集和验证集误差判断：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Training set error</th>
<th style="text-align:center">Dev set error</th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1%</td>
<td style="text-align:center">15%</td>
<td style="text-align:center">high variance</td>
</tr>
<tr>
<td style="text-align:center">15%</td>
<td style="text-align:center">16%</td>
<td style="text-align:center">high bias</td>
</tr>
<tr>
<td style="text-align:center">15%</td>
<td style="text-align:center">30%</td>
<td style="text-align:center">high variance &amp; bias</td>
</tr>
<tr>
<td style="text-align:center">0.5%</td>
<td style="text-align:center">1%</td>
<td style="text-align:center">low variance &amp; bias</td>
</tr>
</tbody>
</table>
</div>
<h2 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h2><blockquote>
<p>初始模型训练完成后，我首先要知道算法的偏差高不高，如果偏差较高，试着评估训练集或训练数据的性能。如果偏差的确很高，甚至无法拟合训练集，那么你要做的就是选择一个新的网络，比如含有更多隐藏层或者隐藏单元的网络，或者花费更多时间来训练网络，或者尝试更先进的优化算法，后面我们会讲到这部分内容。</p>
</blockquote>
<ul>
<li>只要正则适度，通常构建一个更大的网络便可以，在不影响方差的同时减少偏差，而采用更多数据通常可以在不过多影响偏差的同时减少方差。</li>
<li>训练网络，选择网络或者准备更多数据，现在我们有工具可以做到在减少偏差或方差的同时，不对另一方产生过多不良影响。</li>
</ul>
<h2 id="正则化（Regularization）"><a href="#正则化（Regularization）" class="headerlink" title="正则化（Regularization）"></a>正则化（Regularization）</h2><p>深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，正则化通常有助于避免过拟合或减少你的网络误差。</p>
<p>就是在代价函数里加一个正则化项，一般用 $\dfrac{\lambda}{2m}$乘以$w$范数的平方,其中$\left| w \right|_2^2$是$w$的欧几里德范数的平方，$L2$ 正则化。</p>
<p>神经网络中的正则项就是为$\dfrac{\lambda }{2m}\sum\limits_{l = 1}^{L}| W^{[l]}|^{2}$，我们称${||W^{\left[l\right]}||}^{2}$为范数平方，这个矩阵范数${||W^{\left[l\right]}||}^{2}$（即平方范数），被定义为矩阵中所有元素的平方求和。该矩阵范数被称作“弗罗贝尼乌斯范数”，用下标$F$标注。</p>
<p>带正则化的梯度下降中，对$W^{[l]}$的偏导数，把$W^{[l]}$替换为$W^{[l]}$减去学习率乘以$dW$。现在我们要做的就是给$dW$加上这一项$\dfrac {\lambda}{m}W^{[l]}$，然后计算这个更新项，使用新定义的$dW^{[l]}$，它的定义含有相关参数代价函数导数和，以及最后添加的额外正则项。</p>
<script type="math/tex; mode=display">
\begin{aligned}
W^{[l]} :&= W^{[l]}  - \alpha \times \left[(\text{from backpap}) + \dfrac{\lambda}{m}W^{[l]}\right]\\ &= (1 - \frac{\alpha \lambda}{m}) W^{[l]} - \alpha \times (\text{from backpap})
\end{aligned}</script><p><em>正则化预防过拟合的原因：极限思想，当lambda很大权重为0，退化成欠拟合，有个right fit 的中间态。</em></p>
<h2 id="dropout-正则化"><a href="#dropout-正则化" class="headerlink" title="dropout 正则化"></a>dropout 正则化</h2><p>就是随机消掉一些神经元。。。</p>
<ul>
<li><strong>inverted dropout</strong>（反向随机失活）<ul>
<li>首先要定义向量$d$，$d^{[3]}$表示网络第三层的<strong>dropout</strong>向量：<code>d3 = np.random.rand(a3.shape[0],a3.shape[1])</code> 。</li>
<li>然后看它是否小于某数，我们称之为<strong>keep-prob</strong>，<strong>keep-prob</strong>是一个具体数字，它表示保留某个隐藏单元的概率。</li>
<li>接下来要做的就是从第三层中获取$a^{[3]}$，$a^{[3]}$含有要计算的激活函数，$a^{[3]}$等于上面的$a^{[3]}$乘以 $d^{[3]}$，就是把 $d$ 中 0 对应位置的数归零。（$d$ 实际上是一个布尔数组）</li>
<li>最后，我们向外扩展$a^{[3]}$，用它除以<strong>keep-prob</strong>参数。</li>
</ul>
</li>
</ul>
<p>显然在测试阶段，我们不使用<strong>dropout</strong>。要同时在 <strong>fpp</strong> 和 <strong>bpp</strong> 中使用 dropout。</p>
<p><em>dropout 预防过拟合的原因，dropout 的功能类似于$L2$正则化，与$L2$正则化不同的是应用方式不同会带来一点点小变化，甚至更适用于不同的输入范围。</em></p>
<ul>
<li><p>如果你担心某些层比其它层更容易发生过拟合，可以把某些层的<strong>keep-prob</strong>值设置得比其它层更低，缺点是为了使用交叉验证，你要搜索更多的超级参数，另一种方案是在一些层上应用<strong>dropout</strong>，而有些层不用<strong>dropout</strong>，应用<strong>dropout</strong>的层只含有一个超级参数，就是<strong>keep-prob</strong>。</p>
</li>
<li><p>它在其它领域应用得比较少，主要存在于计算机视觉领域，因为我们通常没有足够的数据，所以一直存在过拟合。</p>
</li>
<li><strong>dropout</strong>一大缺点就是代价函数$J$不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。</li>
</ul>
<h2 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h2><ul>
<li>数据扩增</li>
<li><strong>early stopping</strong><ul>
<li>缺点是不能独立处理梯度下降和优化代价函数。</li>
</ul>
</li>
</ul>
<h2 id="归一化输入（Normalizing）"><a href="#归一化输入（Normalizing）" class="headerlink" title="归一化输入（Normalizing）"></a>归一化输入（Normalizing）</h2><p>训练神经网络，其中一个加速训练的方法就是归一化输入。归一化需要两个步骤：</p>
<ol>
<li>零均值<ul>
<li>$\mu = \frac{1}{m}\sum\limits_{i =1}^{m}x^{(i)}$，它是一个向量，$x$ 等于每个训练数据 $x$ 减去 $\mu$，意思是移动训练集，直到它完成零均值化。 </li>
</ul>
</li>
<li>归一化方差<ul>
<li>$ \sigma^{2}= \frac{1}{m}\sum\limits_{i =1}^{m}(x^{(i)})^{2} $，$\sigma^{2}$是一个向量，它的每个特征都有方差，把所有数据除以向量$\sigma^{2}$。</li>
</ul>
</li>
</ol>
<h2 id="梯度消失-梯度爆炸（Vanishing-Exploding-gradients）"><a href="#梯度消失-梯度爆炸（Vanishing-Exploding-gradients）" class="headerlink" title="梯度消失/梯度爆炸（Vanishing / Exploding gradients）"></a>梯度消失/梯度爆炸（Vanishing / Exploding gradients）</h2><p>神经网络层数多了，激活函数就会以指数级递增或递减。</p>
<h2 id="神经网络权重的初始化"><a href="#神经网络权重的初始化" class="headerlink" title="神经网络权重的初始化"></a>神经网络权重的初始化</h2><p>针对梯度消失/爆炸，有一个方案就是更谨慎地选择随机初始化参数。</p>
<p>$z = w_{1}x_{1} + w_{2}x_{2} + \ldots +w_{n}x_{n}$，为了预防$z$值过大或过小，希望每项值更小，最合理的方法就是设置$w_{i}=\frac{1}{n}$，$n$表示神经元的输入特征数量。</p>
<p>实际上，你要做的就是设置某层权重矩阵 <code>W[l] = np.random.randn(shape) * np.sqrt(1 / n[l-1])</code>，$n^{[l - 1]}$ 就是我喂给第$l$层神经单元的数量（即第$l-1$层神经元数量）。</p>
<p>如果你是用的是<strong>Relu</strong>激活函数，而不是$\frac{1}{n}$，方差设置为$\frac{2}{n}$，效果会更好。</p>
<p>对于<strong>tanh</strong>函数来说，用$\sqrt{\frac{1}{n^{[l-1]}}}$。</p>
<h2 id="梯度的数值逼近-Numerical-approximation-of-gradients"><a href="#梯度的数值逼近-Numerical-approximation-of-gradients" class="headerlink" title="梯度的数值逼近 Numerical approximation of gradients"></a>梯度的数值逼近 Numerical approximation of gradients</h2><p>就是导数定义，双边误差，即$\frac{f\left(\theta + \varepsilon \right) - f(\theta -\varepsilon)}{2\varepsilon}$。</p>
<p>先将所有的参数 $W, b$ 展开成一个大向量 $\theta$，在<strong>bpp</strong>中，算完梯度之后所有的梯度 $dW, db$ 就是 $d\theta$。</p>
<p>然后比较 $d\theta_{\text{approx}}\left[i \right] = \frac{J\left( \theta_{1},\theta_{2},\ldots\theta_{i} + \varepsilon,\ldots \right) - J\left( \theta_{1},\theta_{2},\ldots\theta_{i} - \varepsilon,\ldots \right)}{2\varepsilon}$ 和 $d\theta[i]$ 的值接不接近。</p>
<p>就计算它们的欧式距离再归一化，$\dfrac{||d\theta_{\text{approx}} -d\theta||_{2}}{||d\theta_{\text{approx}}||_2 + ||d\theta||_2}$。计算得到的值为$10^{-7}$或更小，这就很好；如果它的值在$10^{-5}$范围内，就要小心了，也许这个值没问题，但再次检查这个向量的所有项，确保没有一项误差过大，可能这里有<strong>bug</strong>。如果比$10^{-3}$大很多，就会很担心是否存在<strong>bug</strong>，这时应该仔细检查所有$\theta$项，看是否有一个具体的$i$值，使得$d\theta_{\text{approx}}\left[i \right]$与$ d\theta[i]$大不相同，并用它来追踪一些求导计算是否正确。</p>
<h2 id="梯度检验的注意事项"><a href="#梯度检验的注意事项" class="headerlink" title="梯度检验的注意事项"></a>梯度检验的注意事项</h2><ol>
<li>不要在训练中使用梯度检验，它只用于调试。</li>
<li>如果算法的梯度检验失败，要检查所有项，检查每一项，并试着找出<strong>bug</strong>。</li>
<li>在实施梯度检验时，如果使用正则化，请注意正则项。</li>
<li>梯度检验不能与<strong>dropout</strong>同时使用，因为每次迭代过程中，<strong>dropout</strong>会随机消除隐藏层单元的不同子集，难以计算<strong>dropout</strong>在梯度下降上的代价函数$J$。</li>
</ol>
<h1 id="Lesson-2-2"><a href="#Lesson-2-2" class="headerlink" title="Lesson 2.2"></a>Lesson 2.2</h1><p>本节课主要讲优化算法，也就是我们如何更新参数。</p>
<h2 id="Mini-batch-梯度下降"><a href="#Mini-batch-梯度下降" class="headerlink" title="Mini-batch 梯度下降"></a>Mini-batch 梯度下降</h2><p>你可以把训练集分割为小一点的子集训练，这些子集被取名为<strong>mini-batch</strong>，每个子集记作 $X^{\{i\}}$。就是把原来梯度下降时代入整个训练集改成代入一个mini-batch，然后多梯度下降几次。</p>
<p>使用<strong>mini-batch</strong>梯度下降法，如果作出成本函数在整个过程中的图，则并不是每次迭代都是下降的，特别是在每次迭代中，你要处理的是$X^{\{t\}}$和$Y^{\{ t\}}$。如果要作出成本函数$J$的图，你很可能会看到这样的结果，走向朝下，但有更多的噪声。</p>
<p>需要决定的变量之一是<strong>mini-batch</strong>的大小。首先，如果训练集较小，直接使用<strong>batch</strong>梯度下降法。样本数目较大的话，一般的<strong>mini-batch</strong>大小为64到512，考虑到电脑内存设置和使用的方式，如果<strong>mini-batch</strong>大小是2的$n$次方，代码会运行地快一些。</p>
<h2 id="指数加权平均数-Exponentially-weighted-averages"><a href="#指数加权平均数-Exponentially-weighted-averages" class="headerlink" title="指数加权平均数 Exponentially weighted averages"></a>指数加权平均数 Exponentially weighted averages</h2><p>递推式：</p>
<script type="math/tex; mode=display">
v_t = \beta v_{t - 1} + (1 - \beta)\theta_t</script><p>如果我们将其展开，这就是一个加权平均，是从 $0$ 到 $t$ 每个 $\theta_i$ 的平均，越远权重越小。</p>
<p>考虑 $\beta^{x} = \frac{1}{e}$，这个算的大约就是 $x$ 天的平均数。</p>
<p>指数加权平均数公式的好处之一在于，它占用极少内存，电脑内存中只占用一行数字而已，然后把最新数据代入公式，不断覆盖就可以了。</p>
<p>不过有可能会遇到 <strong>偏差修正（bias corrections）的问题</strong></p>
<p>因为我们取 $v_0 = 0$，所以会使得 $i$ 较小时 $v_i$ 所占权重都很小，估算不准确。</p>
<p>我们可以在估测初期，不用 $v_t$，而是 $\dfrac{v_t}{1 - \beta^t}$。</p>
<p>不过在机器学习中，在计算指数加权平均数的大部分时候，大家不在乎执行偏差修正。</p>
<h2 id="动量梯度下降法-Gradient-descent-with-Momentum"><a href="#动量梯度下降法-Gradient-descent-with-Momentum" class="headerlink" title="动量梯度下降法 Gradient descent with Momentum"></a>动量梯度下降法 Gradient descent with Momentum</h2><p>有一种算法叫做<strong>Momentum</strong>，或者叫做动量梯度下降法，运行速度几乎总是快于标准的梯度下降算法，基本的想法就是计算梯度的指数加权平均数，并利用该梯度更新你的权重。</p>
<p>计算动量：</p>
<script type="math/tex; mode=display">
v_{dW} = \beta v_{dW} + (1 - \beta) dW</script><script type="math/tex; mode=display">
v_{db} = \beta v_{db} + (1 - \beta) db</script><p>再更新参数：</p>
<script type="math/tex; mode=display">
W := W - \alpha v_{dW}</script><script type="math/tex; mode=display">
b := b - \alpha v_{db}</script><p>这样就可以减缓梯度下降的幅度。<em>它们能够最小化碗状函数，这些微分项，想象它们为从山上往下滚的一个球，提供了加速度，<strong>Momentum</strong>项相当于速度。</em></p>
<p>所以有两个超参数，学习率 $a$ 以及参数 $\beta$，$\beta$ 控制着指数加权平均数。$\beta$ 最常用的值是0.9，是很棒的鲁棒数。</p>
<p>有一个版本是 $v_{dW} = \beta v_{dW} + dW$，本质上没有区别。</p>
<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p><strong>root mean square prop</strong>算法，它也可以加速梯度下降，通过加快损失下降的方向，减缓无关方向，减少摆动。</p>
<script type="math/tex; mode=display">
S_{dW}= \beta S_{dW} + (1 -\beta) (dW)^{2}</script><script type="math/tex; mode=display">
S_{db}= \beta S_{db} + (1 - \beta)(db)^{2}</script><p>再更新参数：</p>
<script type="math/tex; mode=display">
W:= W -\alpha \dfrac{dW}{\sqrt{S_{dW}}}</script><script type="math/tex; mode=display">
b:=b -\alpha \dfrac{db}{\sqrt{S_{db}}}</script><p>为了确保数值稳定，在实际操练的时候，要在分母上加上一个很小很小的$\varepsilon$，$\varepsilon$是多少没关系，$10^{-8}$是个不错的选择.</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>把前面两个缝起来。</p>
<p>首先初始化：$v_{dW} = 0$，$S_{dW} =0$，$v_{db} = 0$，$S_{db} =0$。</p>
<script type="math/tex; mode=display">
v_{dW}= \beta_{1}v_{dW} + ( 1 - \beta_{1})dW</script><script type="math/tex; mode=display">
v_{db}= \beta_{1}v_{db} + ( 1 -\beta_{1} )db</script><script type="math/tex; mode=display">
S_{dW}=\beta_{2}S_{dW} + ( 1 - \beta_{2})(dW)^{2}</script><script type="math/tex; mode=display">
S_{db} =\beta_{2}S_{db} + ( 1 - \beta_{2} )(db)^{2}</script><p>一般使用<strong>Adam</strong>算法的时候，要计算偏差修正，$v_{dW}^{\text{corrected}}$，修正也就是在偏差修正之后：</p>
<script type="math/tex; mode=display">
v_{dW}^{\text{corrected}}= \dfrac{v_{dW}}{1 - \beta_{1}^{t}}</script><script type="math/tex; mode=display">
v_{db}^{\text{corrected}} =\dfrac{v_{db}}{1 -\beta_{1}^{t}}</script><script type="math/tex; mode=display">
S_{dW}^{\text{corrected}} =\dfrac{S_{dW}}{1 - \beta_{2}^{t}}</script><script type="math/tex; mode=display">
S_{db}^{\text{corrected}} =\dfrac{S_{db}}{1 - \beta_{2}^{t}}</script><p>最后更新权重：</p>
<script type="math/tex; mode=display">
W:= W - \dfrac{\alpha v_{dW}^{\text{corrected}}}{\sqrt{S_{dW}^{\text{corrected}}} +\varepsilon}</script><script type="math/tex; mode=display">
b:=b - \frac{\alpha v_{\text{db}}^{\text{corrected}}}{\sqrt{S_{\text{db}}^{\text{corrected}}} +\varepsilon}</script><p><strong>Adam</strong> 是一种极其常用的学习算法，被证明能有效适用于不同神经网络，适用于广泛的结构。</p>
<p>本算法中有很多超参数，超参数学习率$a$很重要，也经常需要调试，你可以尝试一系列值，然后看哪个有效。$\beta_{1}$常用的缺省值为0.9。超参数$\beta_{2}$，<strong>Adam</strong>论文作者，也就是<strong>Adam</strong>算法的发明者，推荐使用0.999。关于$\varepsilon$的选择其实没那么重要，<strong>Adam</strong>论文的作者建议$\varepsilon$为$10^{-8}$。</p>
<h2 id="学习率衰减-Learning-rate-decay"><a href="#学习率衰减-Learning-rate-decay" class="headerlink" title="学习率衰减 Learning rate decay"></a>学习率衰减 Learning rate decay</h2><p>慢慢减少$a$的本质在于，在学习初期，你能承受较大的步伐，但当开始收敛的时候，小一些的学习率能让你步伐小一些。</p>
<p>$a= \dfrac{1}{1 + decayrate \times \text{epoch}\text{-num}}a_{0}$，（<strong>decay-rate</strong>称为衰减率，<strong>epoch-num</strong>为代数，$\alpha_{0}$为初始学习率），注意这个衰减率是另一个需要调整的超参数。</p>
<p>人们用到的其它公式有$a =\dfrac{k}{\sqrt{\text{epoch-num}}}a_{0}$或者$a =\dfrac{k}{\sqrt{t}}a_{0}$（$t$为<strong>mini-batch</strong>的数字）。</p>
<h2 id="局部最优问题-Local-optima"><a href="#局部最优问题-Local-optima" class="headerlink" title="局部最优问题 Local optima"></a>局部最优问题 Local optima</h2><p>在深度学习研究早期，人们总是担心优化算法会困在极差的局部最优，不过随着深度学习理论不断发展，我们对局部最优的理解也发生了改变。</p>
<p>事实上，如果你要创建一个神经网络，通常梯度为零的点并不是这个图中的局部最优点，实际上成本函数的零梯度点，通常是鞍点。</p>
<p>意思就是一个最优点需要所有维度都是极小，在现在高维特征下很难遇到，所以碰到的所谓局部最优一般都不是最优，都可以跑出来。</p>
<h1 id="Lesson-2-3"><a href="#Lesson-2-3" class="headerlink" title="Lesson 2.3"></a>Lesson 2.3</h1><p>本章主要讲了超参数调试，归一化和深度学习框架。</p>
<h2 id="调试处理-Tuning-process"><a href="#调试处理-Tuning-process" class="headerlink" title="调试处理 Tuning process"></a>调试处理 Tuning process</h2><p>画格子取点。</p>
<p>超参数范围，对数尺。</p>
<p><strong>Pandas</strong>，小模型，每天逐渐变化超参数。</p>
<p><strong>Caviar</strong>，大模型，多个参数同时跑几天。</p>
<h2 id="归一化激活函数-Normalizing-activation"><a href="#归一化激活函数-Normalizing-activation" class="headerlink" title="归一化激活函数 Normalizing activation"></a>归一化激活函数 Normalizing activation</h2><p>就是 <strong>Batch 归一化</strong>，会使你的参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定。</p>
<p>实践中，经常做的是归一化$z^{[i]}$。</p>
<script type="math/tex; mode=display">
\mu = \dfrac{1}{m} \sum\limits_i z^{(i)}</script><script type="math/tex; mode=display">
\sigma^2 = \dfrac{1}{m} \sum\limits_i (z_i - \mu)^2</script><script type="math/tex; mode=display">
z_{norm}^{(i)} = \dfrac{z^{(i)} - \mu}{\sqrt{\sigma^2 + \varepsilon}}</script><p>所以现在我们已把这些$z$值标准化，化为含平均值0和标准单位方差，所以$z$的每一个分量都含有平均值0和方差1，但我们不想让隐藏单元总是含有平均值0和方差1，也许隐藏单元有了不同的分布会有意义，所以我们所要做的就是计算 $\tilde{z}^{(i)} = \gamma z_{norm}^{(i)} + \beta$，这里$\gamma$和$\beta$是你模型的学习参数，作用是可以随意设置${\tilde{z}}^{(i)}$的平均值。</p>
<p><em>应用Batch归一化了一些隐藏单元值中的平均值和方差，不过训练输入和这些隐藏单元值的一个区别是，你也许不想隐藏单元值必须是平均值0和方差1。它真正的作用是，使隐藏单元值的均值和方差标准化，即$z^{(i)}$有固定的均值和方差，均值和方差可以是0和1，也可以是其它值，它是由$\gamma$和$\beta$两参数控制的。</em></p>
<p>至于如何将 <em>BN</em> 层放进神经网络——tensorflow(</p>
<h2 id="Batch-Norm-为什么有用"><a href="#Batch-Norm-为什么有用" class="headerlink" title="Batch Norm 为什么有用"></a>Batch Norm 为什么有用</h2><p>通过归一化所有的输入特征值$x$，以获得类似范围的值，可以加速学习，不仅仅对于这里的输入值，还有隐藏单元的值。</p>
<p><strong>Batch</strong>归一化有效的第二个原因是，它可以使权重比你的网络更滞后或更深层。对于网络的泛化能力，浅层网络不一定能做的很好，所以我们尝试改变数据的分布，有个有点怪的名字“<strong>Covariate shift</strong>”。如果你已经学习了$x$到$y$ 的映射，如果$x$ 的分布改变了，那么你可能需要重新训练你的学习算法。这种做法同样适用于，如果真实函数由$x$ 到$y$ 映射保持不变。</p>
<p>按我的理解，就是前一层对于后一层的影响。当前一层的分布变化之后，后一层就要面临 <strong>Covariate shift</strong> 的问题，就会不稳定。而 Batch 归一化做的就是它减少了这些隐藏值分布变化的数量，使分布更稳定，神经网络的之后层就会有更坚实的基础。它限制了在前层的参数更新，会影响数值分布的程度，在后一层看到的这种情况，因此得到学习。</p>
<p><em>即使使输入分布改变了一些，它会改变得更少。它做的是当前层保持学习，当改变时，迫使后层适应的程度减小了，你可以这样想，它减弱了前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习，稍稍独立于其它层，这有助于加速整个网络的学习。</em></p>
<p>还有轻微的正则化的作用。因为均值和方差有一点小噪音，因为它只是由一小部分数据估计得出的。</p>
<h2 id="Softmax-回归"><a href="#Softmax-回归" class="headerlink" title="Softmax 回归"></a>Softmax 回归</h2><p>有一种<strong>logistic</strong>回归的一般形式，叫做<strong>Softmax</strong>回归，能让你在试图识别某一分类时做出预测，或者说是多种分类中的一个，不只是识别两个分类。</p>
<script type="math/tex; mode=display">
t=e^{z^{[l]}}</script><script type="math/tex; mode=display">
a_{i}^{[l]} = \dfrac{t_{i}}{\sum\limits_i t_{i}}</script><p>输出的 $a_i$ 就是第 $i$ 类的概率。</p>
<p>训练 softmax 分类器用的损失    函数一般是 $L(\hat{y},y ) = - \sum\limits_{j}y_{j}\log{\hat{y}_{j}}$。</p>
<h2 id="深度学习框架"><a href="#深度学习框架" class="headerlink" title="深度学习框架"></a>深度学习框架</h2><ul>
<li>Caffe/Caffe2</li>
<li>NCTK</li>
<li>DL4J</li>
<li>Keras</li>
<li>Lasagne</li>
<li>mxnet</li>
<li>PaddlePaddle</li>
<li>TensorFlow</li>
<li>Theano</li>
<li>Torch</li>
</ul>
<h1 id="Lesson-3-1"><a href="#Lesson-3-1" class="headerlink" title="Lesson 3.1"></a>Lesson 3.1</h1><p>本章主要介绍机器学习策略。很多事思想意识上的东西，可能很多照搬原话（</p>
<h2 id="正交化-Orthogonalization"><a href="#正交化-Orthogonalization" class="headerlink" title="正交化 Orthogonalization"></a>正交化 Orthogonalization</h2><p>意思大概就是把所有特征正交到几个相互独立的特征上。</p>
<blockquote>
<p>在机器学习中，如果你可以观察你的系统，然后说这一部分是错的，它在训练集上做的不好、在开发集上做的不好、它在测试集上做的不好，或者它在测试集上做的不错，但在现实世界中不好，这就很好。必须弄清楚到底是什么地方出问题了，然后我们刚好有对应的旋钮，或者一组对应的旋钮，刚好可以解决那个问题，那个限制了机器学习系统性能的问题。</p>
</blockquote>
<h2 id="单一数字评估指标-Single-number-evaluation-metric"><a href="#单一数字评估指标-Single-number-evaluation-metric" class="headerlink" title="单一数字评估指标 Single number evaluation metric"></a>单一数字评估指标 Single number evaluation metric</h2><p>无论是调整超参数，或者是尝试不同的学习算法，或者在搭建机器学习系统时尝试不同手段，如果你有一个单实数评估指标，进展会快得多，它可以快速告诉你，新尝试的手段比之前的手段好还是差。所以当团队开始进行机器学习项目时，推荐为问题设置一个单实数评估指标（F1 分数，平均值）。</p>
<p>有一个开发集，加上单实数评估指标，迭代速度肯定会很快，它可以加速改进机器学习算法的迭代过程。</p>
<h2 id="满足和优化指标-Satisficing-and-optimizing-metrics"><a href="#满足和优化指标-Satisficing-and-optimizing-metrics" class="headerlink" title="满足和优化指标 Satisficing and optimizing metrics"></a>满足和优化指标 Satisficing and optimizing metrics</h2><p>要把顾及到的所有事情组合成单实数评估指标有时并不容易，在那些情况里，有时候设立满足和优化指标是很重要的。</p>
<p>优化指标，想要准确度最大化，想做的尽可能准确。</p>
<p>满足指标，意思是它必须足够好，达到标准之后，不那么在乎这指标有多好。</p>
<p>通过定义优化和满足指标，就可以提供一个明确的方式，去选择“最好的”分类器。</p>
<h2 id="训练-开发-测试集划分-train-dev-test-distributions"><a href="#训练-开发-测试集划分-train-dev-test-distributions" class="headerlink" title="训练/开发/测试集划分 train/dev/test distributions"></a>训练/开发/测试集划分 train/dev/test distributions</h2><p>开发（<strong>dev</strong>）集也叫做开发集（<strong>development set</strong>），有时称为保留交叉验证集（<strong>hold out cross validation set</strong>）。然后，机器学习中的工作流程是，尝试很多思路，用训练集训练不同的模型，然后使用开发集来评估不同的思路，然后选择一个，然后不断迭代去改善开发集的性能，直到最后可以得到一个令你满意的成本，然后你再用测试集去评估。</p>
<p>让开发集和测试集来自同一分布，通过将所有数据随机洗牌。</p>
<p>大小 98:1:1.</p>
<p><em>处理机器学习问题时，应该把它切分成独立的步骤。一步是弄清楚如何定义一个指标来衡量你想做的事情的表现，然后我们可以分开考虑如何改善系统在这个指标上的表现。要把机器学习任务看成两个独立的步骤，用目标这个比喻，第一步就是设定目标。所以要定义你要瞄准的目标，这是完全独立的一步，这是你可以调节的一个旋钮。如何设立目标是一个完全独立的问题，把它看成是一个单独的旋钮，可以调试算法表现的旋钮，如何精确瞄准，如何命中目标，定义指标是第一步。</em></p>
<p><em>然后第二步要做别的事情，在逼近目标的时候，也许学习算法针对某个成本函数优化，要最小化训练集上的损失。可以做的其中一件事是，修改这个，为了引入权重，也许最后需要修改这个归一化常数。</em></p>
<p>如何定义$J$并不重要，关键在于正交化的思路。将定义指标看成一步，然后在定义了指标之后，你才能想如何优化系统来提高这个指标评分。比如改变你神经网络要优化的成本函数$J$。</p>
<blockquote>
<p>所以我的建议是，即使你无法定义出一个很完美的评估指标和开发集，你直接快速设立出来，然后使用它们来驱动你们团队的迭代速度。如果在这之后，你发现选的不好，你有更好的想法，那么完全可以马上改。对于大多数团队，我建议最好不要在没有评估指标和开发集时跑太久，因为那样可能会减慢你的团队迭代和改善算法的速度。</p>
</blockquote>
<h2 id="为什么是人的表现-Why-human-level-performance"><a href="#为什么是人的表现-Why-human-level-performance" class="headerlink" title="为什么是人的表现 Why human-level performance?"></a>为什么是人的表现 Why human-level performance?</h2><p>在过去的几年里，更多的机器学习团队一直在讨论如何比较机器学习系统和人类的表现。</p>
<blockquote>
<p>我认为有两个主要原因，首先是因为深度学习系统的进步，机器学习算法突然变得更好了。在许多机器学习的应用领域已经开始见到算法已经可以威胁到人类的表现了。其次，事实证明，当你试图让机器做人类能做的事情时，可以精心设计机器学习系统的工作流程，让工作流程效率更高，所以在这些场合，比较人类和机器是很自然的，或者你要让机器模仿人类的行为。</p>
</blockquote>
<p>当这个算法表现比人类更好时，进展和精确度的提升就变得更慢了。也许它还会越来越好，但是在超越人类水平之后，它还可以变得更好，但性能增速，准确度上升的速度这个斜率，会变得越来越平缓，我们都希望能达到理论最佳性能水平。随着时间的推移，当继续训练算法时，可能模型越来越大，数据越来越多，但是性能无法超过某个理论上限，这就是所谓的贝叶斯最优错误率（<strong>Bayes optimal error</strong>）。</p>
<p>所以贝叶斯最优错误率一般认为是理论上可能达到的最优错误率，就是说没有任何办法设计出一个$x$到$y$的函数，让它能够超过一定的准确度。</p>
<h2 id="可避免偏差-Avoidable-bias"><a href="#可避免偏差-Avoidable-bias" class="headerlink" title="可避免偏差 Avoidable bias"></a>可避免偏差 Avoidable bias</h2><p>在之前的课程关于偏差和方差的讨论中，我们主要假设有一些任务的贝叶斯错误率几乎为0。根据定义，人类水平错误率比贝叶斯错误率高一点，因为贝叶斯错误率是理论上限，但人类水平错误率离贝叶斯错误率不会太远。</p>
<p>对贝叶斯错误率的估计和训练错误率之间的差值称为可避免偏差。这个训练错误率和开发错误率之前的差值，就大概说明你的算法在方差问题上还有多少改善空间。</p>
<p><em>理解偏差和方差，那么在人类可以做得很好的任务中，你可以估计人类水平的错误率，你可以使用人类水平错误率来估计贝叶斯错误率。所以你到贝叶斯错误率估计值的差距，告诉你可避免偏差问题有多大，可避免偏差问题有多严重，而训练错误率和开发错误率之间的差值告诉你方差上的问题有多大，你的算法是否能够从训练集泛化推广到开发集。</em></p>
<p>要达到超越人类的表现往往不容易，但如果有足够多的数据，已经有很多深度学习系统，在单一监督学习问题上已经超越了人类的水平，所以这对在开发的应用是有意义的。</p>
<h1 id="Lesson-3-2"><a href="#Lesson-3-2" class="headerlink" title="Lesson 3.2"></a>Lesson 3.2</h1><p>本章还是机器学习策略。</p>
<h2 id="误差分析-Error-analysis"><a href="#误差分析-Error-analysis" class="headerlink" title="误差分析 Error analysis"></a>误差分析 Error analysis</h2><p>进行错误分析，应该找一组错误样本，可能在你的开发集里或者测试集里，观察错误标记的样本，看看假阳性（<strong>false positives</strong>）和假阴性（<strong>false negatives</strong>），统计属于不同错误类型的错误数量。在这个过程中，可能会得到启发，归纳出新的错误类型。</p>
<p>通过统计不同错误标记类型占总数的百分比，可以帮你发现哪些问题需要优先解决，或者给你构思新优化方向的灵感。</p>
<h2 id="清楚标记错误的数据-Cleaning-up-incorrectly-labeled-data"><a href="#清楚标记错误的数据-Cleaning-up-incorrectly-labeled-data" class="headerlink" title="清楚标记错误的数据 Cleaning up incorrectly labeled data"></a>清楚标记错误的数据 Cleaning up incorrectly labeled data</h2><p>首先，考虑训练集，事实证明，深度学习算法对于训练集中的随机错误是相当健壮的（<strong>robust</strong>）。标记出错的样本，只要这些错误样本离随机错误不太远，有时可能做标记的人没有注意或者不小心，按错键了，如果错误足够随机，那么放着这些错误不管可能也没问题，而不要花太多时间修复它们。</p>
<p>如果这些标记错误严重影响了在开发集上评估算法的能力，那么就应该去花时间修正错误的标签。但是，如果它们没有严重影响到用开发集评估成本偏差的能力，那么可能就不应该花宝贵的时间去处理。</p>
<ul>
<li>对开发集和测试集做同样的处理来确保他们保持同样的分布。</li>
<li>同时检验算法判断正确和错误的样本。</li>
<li>训练集和开发/测试集可能来自不同分布。</li>
</ul>
<blockquote>
<p>首先，深度学习研究人员有时会喜欢这样说：“我只是把数据提供给算法，我训练过了，效果拔群”。这话说出了很多深度学习错误的真相，更多时候，我们把数据喂给算法，然后训练它，并减少人工干预，减少使用人类的见解。但我认为，在构造实际系统时，通常需要更多的人工错误分析，更多的人类见解来架构这些系统，尽管深度学习的研究人员不愿意承认这点。</p>
<p>其次，不知道为什么，我看一些工程师和研究人员不愿意亲自去看这些样本，也许做这些事情很无聊，坐下来看100或几百个样本来统计错误数量，但我经常亲自这么做。当我带领一个机器学习团队时，我想知道它所犯的错误，我会亲自去看看这些数据，尝试和一部分错误作斗争。我想就因为花了这几分钟，或者几个小时去亲自统计数据，真的可以帮你找到需要优先处理的任务，我发现花时间亲自检查数据非常值得，所以我强烈建议你们这样做，如果你在搭建你的机器学习系统的话，然后你想确定应该优先尝试哪些想法，或者哪些方向。</p>
</blockquote>
<p><del>大佬之所以是大佬</del>。</p>
<h2 id="迁移学习-Transfer-learning"><a href="#迁移学习-Transfer-learning" class="headerlink" title="迁移学习 Transfer learning"></a>迁移学习 Transfer learning</h2><p>深度学习中，最强大的理念之一就是，有的时候神经网络可以从一个任务中习得知识，并将这些知识应用到另一个独立的任务中。所以例如，也许你已经训练好一个神经网络，能够识别像猫这样的对象，然后使用那些知识，或者部分习得的知识去帮助您更好地阅读x射线扫描图，这就是所谓的迁移学习。</p>
<p>具体来说，在第一阶段训练过程中，当进行图像识别任务训练时，可以训练神经网络的所有常用参数，所有的权重，所有的层，然后就得到了一个能够做图像识别预测的网络。在训练了这个神经网络后，要实现迁移学习，现在要做的是，把数据集换成新的$(x,y)$对，现在这些变成放射科图像，而$y$是想要预测的诊断，要做的是初始化最后一层的权重，在这个新数据集上重新训练网络。</p>
<p>经验规则是，如果有一个小数据集，就只训练输出层前的最后一层，或者也许是最后一两层。但是如果有很多数据，那么也许可以重新训练网络中的所有参数。如果重新训练神经网络中的所有参数，那么这个在图像识别数据的初期训练阶段，有时称为预训练（<strong>pre-training</strong>），因为在用图像识别数据去预先初始化，或者预训练神经网络的权重。然后，如果以后更新所有权重，有时这个过程叫微调（<strong>fine tuning</strong>）。</p>
<p>迁移学习起作用的场合是，在迁移来源问题中你有很多数据，但迁移目标问题你没有那么多数据。</p>
<ul>
<li>任务 A 和 B 有相同的输入 $x$。</li>
<li>A 有比 B 更多的数据。</li>
<li>A 的低级特征对于 B 的学习有帮助。</li>
</ul>
<h2 id="多任务学习-Multi-task-learning"><a href="#多任务学习-Multi-task-learning" class="headerlink" title="多任务学习 Multi-task learning"></a>多任务学习 Multi-task learning</h2><p>在迁移学习中，你的步骤是串行的，你从任务$A$里学习只是然后迁移到任务$B$。在多任务学习中，你是同时开始学习的，试图让单个神经网络同时做几件事情，然后希望这里每个任务都能帮到其他所有任务。</p>
<p>输出 $y^{(i)}$ 不再是一个标签，而是几个标签，表示不同的任务输出，网络的最后一层变成矩阵 $Y$。损失函数就是所有的输出损失求和。</p>
<p>多任务学习什么时候有意义：</p>
<ul>
<li><p>训练的一组任务可以共用低层次特征。</p>
</li>
<li><p>每个任务的数据量很接近。</p>
</li>
<li>可以训练一个足够大的神经网络，同时做好所有的工作。</li>
</ul>
<h2 id="端到端的深度学习-End-to-end-deep-learning"><a href="#端到端的深度学习-End-to-end-deep-learning" class="headerlink" title="端到端的深度学习 End-to-end deep learning"></a>端到端的深度学习 End-to-end deep learning</h2><p>终于知道所说的端到端是啥意思了。。。</p>
<p>简而言之，以前有一些数据处理系统或者学习系统，它们需要多个阶段的处理。那么端到端深度学习就是忽略所有这些不同的阶段，用单个神经网络代替它。</p>
<p>它表现可以很好，也可以简化系统架构，不需要搭建那么多手工设计的单独组件，但它也不是灵丹妙药，并不是每次都能成功。</p>
<p>优点：</p>
<ul>
<li><p>只通过数据，而不引入更多的人为概念。（Let the data speak）</p>
</li>
<li><p>更少的手工设计的组件。</p>
</li>
</ul>
<p>缺点：</p>
<ul>
<li>需要大量的数据。</li>
<li>排除了可能有用的手工设计的组件。</li>
</ul>
<h1 id="Lesson-5-1"><a href="#Lesson-5-1" class="headerlink" title="Lesson 5.1"></a>Lesson 5.1</h1><p>本节主要讲序列模型（Sequence Models）。</p>
<h2 id="数学符号-Notation"><a href="#数学符号-Notation" class="headerlink" title="数学符号 Notation"></a>数学符号 Notation</h2><p>$x^{<t>}$：序列的中间位置，$t$ 索引。</t></p>
<p>$T_x$：输入序列的长度。</p>
<p>$T_y$：输出序列的长度。</p>
<p><strong>one-hot</strong> 法表示字典中的每一个单词。</p>
<p><strong>Unknow Word</strong> 伪造单词，&lt;<strong>UNK</strong>&gt;。</p>
<h2 id="循环神经网络-Recurrent-Neural-Network"><a href="#循环神经网络-Recurrent-Neural-Network" class="headerlink" title="循环神经网络 Recurrent Neural Network"></a>循环神经网络 Recurrent Neural Network</h2><p>简单来说，在时间步 $t$ 中，模型不仅是用 $x^{<t>}$ 来预测 $\hat{y}^{<t>}$，而且还用了前一个时间步中的信息，也就是时间步 $t-1$ 的激活值会传递到 $t$。</t></t></p>
<p>有一个零时刻的初始激活值 $a^{<0>} = \mathbf{0}$。</0></p>
<p>循环神经网络是从左向右扫描数据，同时每个时间步的参数也是共享的。$W_{\text{ax}}$ 表示管理着从 $x^{<1>}$ 到隐藏层的连接的一系列参数，每个时间步使用的都是相同的参数 $W_{\text{ax}}$。而激活值也就是水平联系是由参数 $W_{aa}$ 决定的，同时每一个时间步都使用相同的参数 $W_{aa}$，同样的输出结果由 $W_{\text{ya}}$ 决定。</1></p>
<p><img data-src="rnn.png" alt></p>
<p>这个循环神经网络的一个缺点就是它只使用了这个序列中之前的信息来做出预测。</p>
<p>计算示例（FPP）：</p>
<p>$a^{&lt; t &gt;} = g_{1}(W_{aa}a^{&lt; t - 1 &gt;} + W_{ax}x^{&lt; t &gt;} + b_{a})$</p>
<p>$\hat y^{&lt; t &gt;} = g_{2}(W_a^{&lt; t &gt;} + b_{y})$</p>
<p>循环神经网络用的激活函数经常是<strong>tanh</strong>。</p>
<p>简化符号：</p>
<p>$a^{<t>} =g(W_{a}\left\lbrack a^{&lt; t-1 &gt;},x^{<t>} \right\rbrack +b_{a})$</t></t></p>
<p>$\hat y^{&lt; t &gt;} = g(W_{y}a^{&lt; t &gt;} +b_{y})$</p>
<p>所以我们定义$W_{a}$的方式是将矩阵$W_{aa}$和矩阵$W_{ax}$水平并列放置，$[ W_{aa}\vdots W_{ax}]=W_{a}$。</p>
<p>用这个符号（$\left\lbrack a^{&lt; t - 1 &gt;},x^{&lt; t &gt;}\right\rbrack$）的意思是将这两个向量堆在一起，用这个符号表示，即$\begin{bmatrix}a^{&lt; t-1 &gt;} \\ x^{&lt; t &gt;} \\\end{bmatrix}$。</p>
<p><img data-src="rnn-f.png" alt></p>
<h2 id="通过时间的反向传播-Backpropagation-through-time"><a href="#通过时间的反向传播-Backpropagation-through-time" class="headerlink" title="通过时间的反向传播 Backpropagation through time"></a>通过时间的反向传播 Backpropagation through time</h2><p><img data-src="rnn_cell_backprop.png" alt></p>
<h2 id="不同类型的循环神经网络"><a href="#不同类型的循环神经网络" class="headerlink" title="不同类型的循环神经网络"></a>不同类型的循环神经网络</h2><ul>
<li>多对多</li>
<li>多对一</li>
<li>一对一</li>
<li>一对多</li>
</ul>
<h2 id="语言模型和序列生成-Sequence-generation"><a href="#语言模型和序列生成-Sequence-generation" class="headerlink" title="语言模型和序列生成 Sequence generation"></a>语言模型和序列生成 Sequence generation</h2><p>所以语言模型所做的就是，预测某个特定的句子它出现的概率是多少。</p>
<p>为了使用<strong>RNN</strong>建立出这样的模型，首先需要一个训练集，包含一个很大的英文文本语料库（<strong>corpus</strong>）或者其它的语言，用于构建模型的语言的语料库。语料库是自然语言处理的一个专有名词，就是很长的或者说数量众多的英文句子组成的文本。</p>
<p>就是根据前面所给的单词，在下一个时间步预测出下一个单词的分布。</p>
<h2 id="新序列采样-Sampling-novel-sequences"><a href="#新序列采样-Sampling-novel-sequences" class="headerlink" title="新序列采样 Sampling novel sequences"></a>新序列采样 Sampling novel sequences</h2><p>就是把每一个时间步的输出传递到下一个时间步，直到 <strong>EOS</strong>。</p>
<h2 id="循环神经网络的梯度消失-Vanishing-gradients-with-RNNs"><a href="#循环神经网络的梯度消失-Vanishing-gradients-with-RNNs" class="headerlink" title="循环神经网络的梯度消失 Vanishing gradients with RNNs"></a>循环神经网络的梯度消失 Vanishing gradients with RNNs</h2><p>一个很深很深的网络，对这个网络从左到右做前向传播然后再反向传播。如果这是个很深的神经网络，从输出$\hat y$得到的梯度很难传播回去，很难影响靠前层的权重，很难影响前面层的计算。</p>
<p><strong>RNN</strong> 一个时间步的输出，基本上很难受到序列靠前的输入的影响，这是因为不管输出是什么，不管是对的，还是错的，这个区域都很难反向传播到序列的前面部分，也因此网络很难调整序列前面的计算。</p>
<h2 id="门控循环单元-Gated-Recurrent-Unit"><a href="#门控循环单元-Gated-Recurrent-Unit" class="headerlink" title="门控循环单元 Gated Recurrent Unit"></a>门控循环单元 Gated Recurrent Unit</h2><p><strong>GRU</strong> 能够改变 <strong>RNN</strong> 的隐藏层，使其可以更好地捕捉深层连接，并改善了梯度消失问题。</p>
<p>有个新的变量称为$c$，代表细胞（<strong>cell</strong>），即记忆细胞，记忆细胞的作用是提供了记忆的能力。实际上用的就是激活值 $c^{<t>} = a^{<t>}$。</t></t></p>
<p>在每个时间步，将用一个候选值重写记忆细胞，用 <strong>tanh</strong> 来激活。</p>
<script type="math/tex; mode=display">
\tilde{c}^{<t>} = \tanh{\left( W_c [c^{<t-1>}, x^{<t>}] + b_c \right)}</script><p>在<strong>GRU</strong>中真正重要的思想是有一个门，叫做$\Gamma_{u}$，这是个下标为 $u$ 的大写希腊字母 $\Gamma$，$u$ 代表更新（update），这是一个0到1之间的值。</p>
<script type="math/tex; mode=display">
\Gamma_{u}= \sigma(W_{u}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{u})</script><p>然后门决定是否要真的更新它。</p>
<script type="math/tex; mode=display">
c^{<t>} = \Gamma_{u} \times \tilde{c}^{<t>} +\left( 1- \Gamma_{u} \right)*c^{<t-1>}</script><p>因为$\Gamma_{u}$很接近0，可能是0.000001或者更小，这就不会有梯度消失的问题了。因为$\Gamma_{u}$很接近0，这就是说$c^{<t>}$几乎就等于$c^{<t-1>}$，而且$c^{<t>}$的值也很好地被维持了，即使经过很多很多的时间步。</t></t-1></t></p>
<p>对于完整的 <strong>GRU</strong>，还有一个门 $\Gamma_r$，$r$ 表示相关性（relevance），这个 $\Gamma_{r}$ 门告诉你计算出的下一个$c^{<t>}$的候选值${\tilde{c}}^{<t>}$跟$c^{<t-1>}$有多大的相关性。</t-1></t></t></p>
<script type="math/tex; mode=display">
\Gamma_{r}= \sigma(W_{r}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack + b_{r})</script><blockquote>
<p>这是多年来研究者们试验过很多很多不同可能的方法来设计这些单元，去尝试让神经网络有更深层的连接，去尝试产生更大范围的影响，还有解决梯度消失的问题，<strong>GRU</strong>就是其中一个研究者们最常使用的版本，也被发现在很多不同的问题上也是非常健壮和实用的。你可以尝试发明新版本的单元，只要你愿意。但是<strong>GRU</strong>是一个标准版本，也就是最常使用的。</p>
</blockquote>
<h2 id="长短期记忆-Long-short-term-memory"><a href="#长短期记忆-Long-short-term-memory" class="headerlink" title="长短期记忆 Long short term memory"></a>长短期记忆 Long short term memory</h2><script type="math/tex; mode=display">
\tilde{c}^{<t>} = \tanh{(W_c\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack) + b_c}</script><script type="math/tex; mode=display">
\Gamma_{u}= \sigma(W_{u}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{u})</script><script type="math/tex; mode=display">
\Gamma_{f}= \sigma(W_{f}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{f})</script><script type="math/tex; mode=display">
\Gamma_{o}= \sigma(W_{o}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{o})</script><script type="math/tex; mode=display">
c^{<t>} = \Gamma_u \times \tilde{c}^{<t>} + \Gamma_f \times c^{<t-1>}</script><script type="math/tex; mode=display">
a^{<t>} = \Gamma_o \times c^{<t>}</script><p>常用的版本可能是门值不仅取决于$a^{<t-1>}$和$x^{<t>}$，有时候也可以偷窥一下$c^{<t-1>}$的值（上图编号13所示），这叫做“窥视孔连接”（<strong>peephole connection</strong>）。</t-1></t></t-1></p>
<p>只要正确地设置了遗忘门和更新门，<strong>LSTM</strong>是相当容易把$c^{<0>}$的值一直往下传递到右边。这就是为什么<strong>LSTM</strong>和<strong>GRU</strong>非常擅长于长时间记忆某个值，对于存在记忆细胞中的某个值，即使经过很长很长的时间步。</0></p>
<p><img data-src="LSTM.png" alt></p>
<p><strong>GRU</strong>的优点是这是个更加简单的模型，所以更容易创建一个更大的网络，而且它只有两个门，在计算性上也运行得更快，然后它可以扩大模型的规模。</p>
<p>但是<strong>LSTM</strong>更加强大和灵活，因为它有三个门而不是两个。<strong>LSTM</strong>在历史进程上是个更优先的选择。</p>
<h2 id="双向循环神经网络-Bidirectional-RNN"><a href="#双向循环神经网络-Bidirectional-RNN" class="headerlink" title="双向循环神经网络 Bidirectional RNN"></a>双向循环神经网络 Bidirectional RNN</h2><p>就是正着跑一遍再反着跑一遍，是一个无环图（Acyclic graph）。</p>
<p><img data-src="brnn.png" alt></p>
<p>会有 $\hat y^{<t>} =g(W_{g}\left\lbrack \overrightarrow{a}^{&lt; t &gt;},\overleftarrow{a}^{&lt; t &gt;} \right\rbrack +b_{y})$。</t></p>
<p>通过这些改变，就可以用一个用<strong>RNN</strong>或<strong>GRU</strong>或<strong>LSTM</strong>构建的模型，并且能够预测任意位置，即使在句子的中间，因为模型能够考虑整个句子的信息。这个双向<strong>RNN</strong>网络模型的缺点就是需要完整的数据的序列，才能预测任意位置。</p>
<h2 id="深层循环神经网络-Deep-RNNs"><a href="#深层循环神经网络-Deep-RNNs" class="headerlink" title="深层循环神经网络 Deep RNNs"></a>深层循环神经网络 Deep RNNs</h2><p>要学习非常复杂的函数，通常我们会把<strong>RNN</strong>的多个层堆叠在一起构建更深的模型。</p>
<p>可以堆好几个隐藏层 $a^{[i]}$。</p>
<p>可以输出后再叠非循环层。</p>
<p><img data-src="drnn.png" alt></p>
<p>由于深层的<strong>RNN</strong>训练需要很多计算资源，需要很长的时间，尽管看起来没有多少循环层，这个也就是在时间上连接了三个深层的循环层，看不到多少深层的循环层，不像卷积神经网络一样有大量的隐含层。</p>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
  </entry>
  <entry>
    <title>python学习笔记</title>
    <url>/2021/01/28/python-learning/</url>
    <content><![CDATA[<p><strong><em>持续更新中</em></strong><br>参考: <a href="https://www.liaoxuefeng.com/wiki/1016959663602400">廖雪峰python教程</a></p>
<h1 id="Python-基础"><a href="#Python-基础" class="headerlink" title="Python 基础"></a>Python 基础</h1><h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><ul>
<li>整数(Python可以处理任意大)，浮点数，字符串(<code>&#39;&#39;</code>或<code>&quot;&quot;</code>括起来的，<code>\</code>转义)，布尔值(<code>True</code> or <code>False</code>)，空值(<code>None</code>).</li>
<li>在Python中，等号<code>=</code>是赋值语句，可以把任意数据类型赋值给变量，同一个变量可以反复赋值，而且可以是不同类型的变量(动态语言)。</li>
<li>Python3用Unicode编码，提供<code>ord()</code>函数获取字符的整数表示，<code>chr()</code>函数把编码转换为对应的字符。</li>
<li>把<code>str</code>变为以字节为单位的<code>bytes</code>型，带<code>b</code>前缀的的单引号或双引号表示。<code>encode()</code>, <code>decode()</code></li>
<li>格式化字符串规则和c语言类似。</li>
</ul>
<a id="more"></a>
<h3 id="list列表"><a href="#list列表" class="headerlink" title="list列表"></a>list列表</h3><ul>
<li>[ ]表示list</li>
<li>[ ]下标索引，从0开始，负数表示倒数第几个</li>
<li>list中数据类型可以不同，可以嵌套</li>
</ul>
<figure class="highlight python"><figcaption><span>list</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="built_in">len</span>(mylist) <span class="comment">#获得list元素的个数 </span></span><br><span class="line">mylist.append(<span class="string">&#x27;a&#x27;</span>) <span class="comment"># 往list中追加元素到末尾</span></span><br><span class="line">mylist.insert(<span class="number">1</span>, <span class="string">&#x27;a&#x27;</span>) <span class="comment"># 把元素插入到指定位置</span></span><br><span class="line">mylist.pop() <span class="comment"># 删除list末尾的元素</span></span><br><span class="line">mylist.pop(<span class="number">1</span>) <span class="comment"># 删除指定位置的元素</span></span><br><span class="line"><span class="comment">#pop返回值为被删除的元素</span></span><br><span class="line"><span class="keyword">del</span> mylist[<span class="number">1</span>] <span class="comment">#使用del可以删除任何位置的列表元素，条件是知道索引</span></span><br><span class="line">mylist.remove() <span class="comment">#按值删除元素，只删除第一个指定的值</span></span><br><span class="line"><span class="comment"># `pop和`del`的选择：删除后是否还要使用该元素</span></span><br><span class="line">mylist.sort() <span class="comment">#永久排序</span></span><br><span class="line">mylist.sort(reverse = <span class="literal">True</span>) <span class="comment">#倒序排序</span></span><br><span class="line"><span class="built_in">sorted</span>(mylist) <span class="comment">#暂时排序，也可以加入倒序参数</span></span><br><span class="line">mylist.reverse() <span class="comment">#永久倒置</span></span><br><span class="line">mylist[<span class="number">1</span>:<span class="number">3</span>] <span class="comment">#list切片，同MATLAB，首尾可缺省</span></span><br><span class="line">copy_list = mylist[:] <span class="comment">#通过切片赋值list</span></span><br></pre></td></tr></table></figure>
<h3 id="tuple元组"><a href="#tuple元组" class="headerlink" title="tuple元组"></a>tuple元组</h3><ul>
<li><code>( )</code>表示元组</li>
<li>tuple和list非常类似，但是tuple一旦初始化就<strong>不能修改</strong>，类似enum</li>
<li>定义一个元素就加个<code>,</code></li>
<li><code>tuple</code>的不变是<strong>指向不变</strong>，即给元组变量赋值是合法的</li>
</ul>
<h2 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h2><h3 id="if-else"><a href="#if-else" class="headerlink" title="if-else"></a>if-else</h3><ul>
<li>不要括号，冒号换行，缩进</li>
<li><code>else if</code>可以缩写<code>elif</code></li>
<li><code>if a in mylist</code>判断元素是否在列表(很自然语言)</li>
</ul>
<h3 id="for"><a href="#for" class="headerlink" title="for"></a>for</h3><ul>
<li><code>for x in ...</code>按list或tuple迭代</li>
<li><code>range(m,n,step)</code>函数生成从m开始到n的整数序列，步长为step，<code>m</code>缺省为0，<code>step</code>缺省为1</li>
</ul>
<h3 id="while"><a href="#while" class="headerlink" title="while"></a>while</h3><ul>
<li><code>break</code>, <code>continue</code>正常用</li>
</ul>
<h3 id="dict字典"><a href="#dict字典" class="headerlink" title="dict字典"></a>dict字典</h3><ul>
<li>key-value, 查找速度很快；类似map, hash table</li>
<li>key必须是<strong>不可变</strong>对象</li>
<li><code>&#123;&#39;key&#39;: value, ...&#125;</code>创建</li>
</ul>
<figure class="highlight python"><figcaption><span>dict</span></figcaption><table><tr><td class="code"><pre><span class="line">d.get(<span class="string">&#x27;Thomas&#x27;</span>,-<span class="number">1</span>) <span class="comment">#寻找某个关键字的值，-1是规定的不存在时的返回值，缺省时返回None，Python交互环境不显示结果</span></span><br><span class="line">d.pop(<span class="string">&#x27;Bob&#x27;</span>) <span class="comment">#删除一个关键字</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key,value <span class="keyword">in</span> d.items(): <span class="comment"># 使用items()方法可以访问所有条目</span></span><br><span class="line">    print(key + <span class="string">&quot;:&quot;</span> + value)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> d.keys(): <span class="comment"># 使用keys()方法可以访问所有的关键字</span></span><br><span class="line">    print(key.title())</span><br></pre></td></tr></table></figure>
<h3 id="set集合"><a href="#set集合" class="headerlink" title="set集合"></a>set集合</h3><ul>
<li>无序，不重复元素，不能放入可变对象</li>
</ul>
<figure class="highlight python"><figcaption><span>set</span></figcaption><table><tr><td class="code"><pre><span class="line">s = <span class="built_in">set</span>([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">s.add(<span class="number">4</span>) <span class="comment">#添加元素</span></span><br><span class="line">s.remove(<span class="number">4</span>) <span class="comment">#删除元素</span></span><br><span class="line">s1 &amp; s2 <span class="comment">#交集</span></span><br><span class="line">s1 | s2 <span class="comment">#并集</span></span><br></pre></td></tr></table></figure>
<ul>
<li>不可变对象：调用对象自身的方法不会改变该对象自身的内容，而是会创建新的对象并返回，如str</li>
<li>可变对象：恰恰相反，如list</li>
</ul>
<h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1><p>Python内置函数: <a href="http://docs.python.org/3/library/functions.html#abs">python手册</a><br>函数名就是指向函数对象的引用，可以当变量用(太骚了)。</p>
<figure class="highlight python"><figcaption><span>define_function</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nop</span>():</span></span><br><span class="line">  <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>funct_with_arg_check</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_abs</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(x,(<span class="built_in">int</span>, <span class="built_in">float</span>)): <span class="comment">#数据类型检查</span></span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">&#x27;bad operand type&#x27;</span>) <span class="comment">#异常处理(后续会提到)</span></span><br><span class="line">    <span class="keyword">if</span> x &gt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>multi_return_value</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">move</span>(<span class="params">x, y, step, angle=<span class="number">0</span></span>):</span></span><br><span class="line">    nx = x + step * math.cos(angle)</span><br><span class="line">    ny = y - step * math.sin(angle)</span><br><span class="line">    <span class="keyword">return</span> nx, ny</span><br></pre></td></tr></table></figure>
<p>返回多值其实是返回一个tuple, 这点比c好用多了!</p>
<h2 id="默认参数"><a href="#默认参数" class="headerlink" title="默认参数"></a>默认参数</h2><p><code>def power(x, n=2)</code>缺省时取默认值。</p>
<ul>
<li>必选参数在前，可选参数在后</li>
<li>变化大的参数在前，变化小的参数在后，降低调用难度</li>
<li>默认参数必须指向<strong>不变对象</strong></li>
</ul>
<h2 id="可变参数"><a href="#可变参数" class="headerlink" title="可变参数"></a>可变参数</h2><p>允许你传入0个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。<br>参数前加<code>*</code>，可以传入任意个参数。list或tuple前加<code>*</code>表示把list或tuple的所有元素作为可变参数。<br><figure class="highlight python"><figcaption><span>changeable_args</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal</span>(<span class="params">*numbers</span>):</span> <span class="comment">#加*表示可变参数</span></span><br><span class="line">    ans = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> numbers:</span><br><span class="line">        ans += n * n</span><br><span class="line">    <span class="keyword">return</span> ans   </span><br><span class="line"><span class="comment">#传参</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cal(<span class="number">1</span>,<span class="number">2</span>) <span class="comment">#传递变量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cal(*number) <span class="comment">#传递list和tuple</span></span><br></pre></td></tr></table></figure></p>
<h2 id="关键字参数"><a href="#关键字参数" class="headerlink" title="关键字参数"></a>关键字参数</h2><p>允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict。<br>参数前加<code>**</code>，可以传入任意个关键字参数，dict前加<code>**</code>表示把这个dict的所有key-value作为参数。</p>
<h2 id="命名关键字参数"><a href="#命名关键字参数" class="headerlink" title="命名关键字参数"></a>命名关键字参数</h2><p>限制关键字名字。</p>
<ul>
<li>需要一个特殊分隔符<code>*</code>, 后面跟命名关键字参数。如果有可变参数，就不需要<code>*</code></li>
<li>必须传入参数名，否则报错；可以设置默认缺省值</li>
</ul>
<figure class="highlight python"><figcaption><span>key_args</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">person</span>(<span class="params">name, age, **kw</span>):</span> <span class="comment">#表示接受关键字参数`kw`</span></span><br><span class="line">    print(<span class="string">&#x27;name:&#x27;</span>, name, <span class="string">&#x27;age:&#x27;</span>, age, <span class="string">&#x27;other:&#x27;</span>, kw)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>person(<span class="string">&#x27;Michael&#x27;</span>, <span class="number">30</span>) <span class="comment">#可以只传入必选参数</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>person(<span class="string">&#x27;Adam&#x27;</span>, <span class="number">45</span>, gender=<span class="string">&#x27;M&#x27;</span>, job=<span class="string">&#x27;Engineer&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>person(<span class="string">&#x27;Adam&#x27;</span>, <span class="number">45</span>, **extra) <span class="comment">#将现成的dict作为参数</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">person</span>(<span class="params">name, age, **kw</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;city&#x27;</span> <span class="keyword">in</span> kw: <span class="comment">#有city参数</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;job&#x27;</span> <span class="keyword">in</span> kw: <span class="comment">#有job参数</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">person</span>(<span class="params">name, age, *, city, job</span>):</span> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">person</span>(<span class="params">name, age, *args, city, job</span>):</span></span><br></pre></td></tr></table></figure>
<p>所有这些参数都能组合使用，理论上任意函数都可以通过<code>f(*args,**kw)</code>调用。不要使用太多组合，保持接口的可理解性。</p>
<h2 id="递归函数"><a href="#递归函数" class="headerlink" title="递归函数"></a>递归函数</h2><p>Python没有尾递归优化。</p>
<h1 id="高级特性"><a href="#高级特性" class="headerlink" title="高级特性"></a>高级特性</h1><h2 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h2><p>有点类似verilog里面的索引，但更高级而且范围不同。</p>
<figure class="highlight python"><figcaption><span>slice</span></figcaption><table><tr><td class="code"><pre><span class="line">L[head : tail :step] <span class="comment">#范围[head,tail),[head,tail-1]</span></span><br><span class="line">L[:<span class="number">3</span>] <span class="comment">#取出从第0个到第3个元素</span></span><br><span class="line">L[-<span class="number">2</span>:] <span class="comment">#取出倒数第2个到最后1个</span></span><br><span class="line">L[<span class="number">1</span>:<span class="number">3</span>] <span class="comment">#取出从第1个到第3个元素</span></span><br><span class="line">L[:<span class="number">10</span>:<span class="number">2</span>] <span class="comment">#前10个数，每2个取一个</span></span><br><span class="line">L[::<span class="number">5</span>] <span class="comment">#所有数，每5个取一个</span></span><br><span class="line">L[:] <span class="comment">#原样复制一个list</span></span><br><span class="line"><span class="string">&#x27;ABCDEFG&#x27;</span>[<span class="number">1</span>:<span class="number">3</span>] <span class="comment">#字符串也可以看成List</span></span><br></pre></td></tr></table></figure>
<h2 id="迭代"><a href="#迭代" class="headerlink" title="迭代"></a>迭代</h2><p>啥都能迭代</p>
<figure class="highlight python"><figcaption><span>iteration</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, value <span class="keyword">in</span> <span class="built_in">enumerate</span>([<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;C&#x27;</span>]):</span><br><span class="line">    print(i, value)</span><br><span class="line"><span class="comment"># enumerate可以将list变成索引-元素对，相当于数组</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> [(<span class="number">1</span>,<span class="number">1</span>),(<span class="number">2</span>,<span class="number">4</span>),(<span class="number">3</span>,<span class="number">9</span>)]:</span><br><span class="line">    print(x,y)</span><br><span class="line"><span class="comment">#同时对两个变量进行迭代</span></span><br></pre></td></tr></table></figure>
<p>通过collections模块的Iterable类型判断一个对象是否可迭代<br><figure class="highlight python"><figcaption><span>iterable</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> Iterable</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">isinstance</span>(<span class="string">&#x27;abc&#x27;</span>, Iterable) <span class="comment"># str是否可迭代</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure></p>
<h2 id="列表生成式"><a href="#列表生成式" class="headerlink" title="列表生成式"></a>列表生成式</h2><p>太自然语言了。。。<br><figure class="highlight python"><figcaption><span>listgenerator</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>[x * x <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>) <span class="keyword">if</span> x % <span class="number">2</span> == <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> os <span class="comment"># 导入os模块，模块的概念后面讲到</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[d <span class="keyword">for</span> d <span class="keyword">in</span> os.listdir(<span class="string">&#x27;.&#x27;</span>)] <span class="comment"># os.listdir可以列出文件和目录</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[m + n <span class="keyword">for</span> m <span class="keyword">in</span> <span class="string">&#x27;ABC&#x27;</span> <span class="keyword">for</span> n <span class="keyword">in</span> <span class="string">&#x27;XYZ&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[x <span class="keyword">if</span> x % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">else</span> -x <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>)] <span class="comment">#if在前在后的区别</span></span><br></pre></td></tr></table></figure></p>
<h2 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h2><p>一边循环一边计算的机制，generator。列表生成式的<code>[]</code>改成<code>()</code></p>
<ul>
<li>list保存的是数据，generator保存的是算法。</li>
<li>使用<code>next()</code>获得generator的下一个返回值。</li>
<li>generator是可迭代对象。</li>
<li>函数定义中包含关键字<code>yield</code>，就是一个generator。执行流程和函数不一样，遇到<code>yield</code>返回，再次执行从上次<code>yield</code>继续。</li>
<li>遇到return或执行到函数体的最后一句，generator结束，for循环结束。</li>
</ul>
<figure class="highlight python"><figcaption><span>generator</span></figcaption><table><tr><td class="code"><pre><span class="line">g = (x * x <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)) <span class="comment">#把list的[]变成()就可以得到生成器</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span>(<span class="params"><span class="built_in">max</span></span>):</span></span><br><span class="line">    n, a, b = <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span>(n &lt; <span class="built_in">max</span>):</span><br><span class="line">        <span class="keyword">yield</span> b</span><br><span class="line">        a, b = b, a + b</span><br><span class="line">        <span class="comment">#相当于(a,b) = (b, a+b)</span></span><br><span class="line">        n = n + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;done&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h2><ul>
<li>可以直接作用于<code>for</code>循环的对象统称为可迭代对象<code>iterable</code>, 使用<code>isinstance( , Iterable)</code>判断。(list, dict, str, generator)</li>
<li>可以被<code>next()</code>调用并返回下个值对象称为迭代器<code>iterator</code>, 使用<code>isinstance( , Iterator)</code>判断。(generator)</li>
<li><code>iter()</code>可以把<code>iterable</code>变成<code>iterator</code></li>
</ul>
<p>Python的Iterator对象表示的是一个数据流，Iterator对象可以被next()函数调用并不断返回下一个数据，直到没有数据时抛出StopIteration错误。可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过next()函数实现按需计算下一个数据，所以Iterator的计算是惰性的，只有在需要返回下一个数据时它才会计算。</p>
<h1 id="函数式编程"><a href="#函数式编程" class="headerlink" title="函数式编程"></a>函数式编程</h1><p>函数式编程就是一种抽象程度很高的编程范式，纯粹的函数式编程语言编写的函数没有变量，因此，任意一个函数，只要输入是确定的，输出就是确定的，这种纯函数我们称之为没有副作用。而允许使用变量的程序设计语言，由于函数内部的变量状态不确定，同样的输入，可能得到不同的输出，因此，这种函数是有副作用的。(yls好像讲过这个owo)</p>
<p>函数式编程的一个特点就是，允许把函数本身作为参数传入另一个函数，还允许返回一个函数！(函数名也是变量)</p>
<h2 id="高阶函数-High-order-function"><a href="#高阶函数-High-order-function" class="headerlink" title="高阶函数 High-order-function"></a>高阶函数 High-order-function</h2><p>变量可以指向函数，函数的参数能接收变量，一个函数就可以接收另一个函数作为参数，这种函数就称之为<strong>高阶函数</strong>。</p>
<h3 id="map"><a href="#map" class="headerlink" title="map()"></a>map()</h3><p>接收两个参数，一个是函数，一个是<code>Iterable</code>，函数依次作用到每个元素，结果作为新的<code>Iterator</code>返回。<br><figure class="highlight python"><figcaption><span>map</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">str</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]))</span><br><span class="line">[<span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>, <span class="string">&#x27;4&#x27;</span>, <span class="string">&#x27;5&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;7&#x27;</span>, <span class="string">&#x27;8&#x27;</span>, <span class="string">&#x27;9&#x27;</span>]</span><br></pre></td></tr></table></figure></p>
<h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce()"></a>reduce()</h3><p>接收两个参数，一个是函数，一个是<code>Iterable</code>, 函数必须接受两个参数，<code>reduce()</code>把结果继续和序列的下一个元素做累积计算，效果为<code>reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4)</code><br><figure class="highlight python"><figcaption><span>str2int</span></figcaption><table><tr><td class="code"><pre><span class="line">num = &#123;<span class="string">&#x27;0&#x27;</span>:<span class="number">0</span>, <span class="string">&#x27;1&#x27;</span>:<span class="number">1</span>, <span class="string">&#x27;2&#x27;</span>:<span class="number">2</span>, <span class="string">&#x27;3&#x27;</span>:<span class="number">3</span>, <span class="string">&#x27;4&#x27;</span>:<span class="number">4</span>, <span class="string">&#x27;5&#x27;</span>:<span class="number">5</span>, <span class="string">&#x27;6&#x27;</span>:<span class="number">6</span>, <span class="string">&#x27;7&#x27;</span>:<span class="number">7</span>, <span class="string">&#x27;8&#x27;</span>:<span class="number">8</span>, <span class="string">&#x27;9&#x27;</span>:<span class="number">9</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">str2num</span>(<span class="params">s</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fn</span>(<span class="params">x,y</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x * <span class="number">10</span> + y</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">char2num</span>(<span class="params">s</span>):</span></span><br><span class="line">        <span class="keyword">return</span> num[s]</span><br><span class="line">    <span class="keyword">return</span> reduce(fn, <span class="built_in">map</span>(char2num, s))</span><br></pre></td></tr></table></figure></p>
<h3 id="filter"><a href="#filter" class="headerlink" title="filter()"></a>filter()</h3><p>接收一个函数和一个序列，把函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。<br><figure class="highlight python"><figcaption><span>primes</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_odd_iter</span>():</span></span><br><span class="line">    n = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        n = n + <span class="number">2</span></span><br><span class="line">        <span class="keyword">yield</span> n</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_not_divisible</span>(<span class="params">n</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span> x: x % n &gt; <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">primes</span>():</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="number">2</span></span><br><span class="line">    it = _odd_iter() <span class="comment"># 初始序列</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        n = <span class="built_in">next</span>(it) <span class="comment"># 返回序列的第一个数</span></span><br><span class="line">        <span class="keyword">yield</span> n</span><br><span class="line">        it = <span class="built_in">filter</span>(_not_divisible(n), it) <span class="comment"># 构造新序列</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>filter()</code>的作用是从一个序列中筛出符合条件的元素。由于<code>filter()</code>使用了惰性计算，所以只有在取<code>filter()</code>结果的时候，才会真正筛选并每次返回下一个筛出的元素。</li>
</ul>
<h3 id="sorted"><a href="#sorted" class="headerlink" title="sorted()"></a>sorted()</h3><p>接受一个<code>key</code>函数来实现自定义排序，<code>reverse</code>决定正着还是反着排。<br><code>sorted([&#39;bob&#39;, &#39;about&#39;, &#39;Zoo&#39;, &#39;Credit&#39;], key=str.lower, reverse=True)</code></p>
<h2 id="返回函数"><a href="#返回函数" class="headerlink" title="返回函数"></a>返回函数</h2><p>函数作为返回值的函数。</p>
<ul>
<li>闭包(closure): 内部函数可以引用外部函数的参数和局部变量，返回时，相关参数和局部变量都保存在返回的函数中。</li>
<li>返回的函数不会立刻执行，而是等到调用了才执行。</li>
<li><strong>返回函数不要引用任何循环变量，或者后续会发生变化的变量。</strong></li>
</ul>
<h2 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h2><p>不需要显式地定义函数，关键字<code>lambda</code>表示匿名函数，冒号前的变量表示参数。</p>
<ul>
<li>只能有一个表达式，不用写<code>return</code>，返回值就是该表达式的结果。</li>
</ul>
<h2 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h2><p>在代码运行期间动态增加功能的方式，称之为“装饰器”（Decorator），本质上就是一个返回函数的高阶函数。</p>
<ul>
<li>函数有一个<code>__name__</code>属性，可以获得函数的名字。</li>
</ul>
<figure class="highlight python"><figcaption><span>decorator</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decorator</span>(<span class="params">func</span>):</span></span><br><span class="line"><span class="meta">        @functools.wraps(<span class="params">func</span>)  </span><span class="comment">#将原始函数的属性复制到wapper中</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args, **kw</span>):</span></span><br><span class="line">            print(<span class="string">&#x27;%s %s():&#x27;</span> % (text, func.__name__))   <span class="comment">#函数调用之外的作用</span></span><br><span class="line">            <span class="keyword">return</span> func(*args, **kw)    <span class="comment">#调用原函数</span></span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="keyword">return</span> decorator</span><br><span class="line"></span><br><span class="line"><span class="meta">@log </span><span class="comment"># @语法，相当于执行now=log(now)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">now</span>():</span></span><br><span class="line">    print(<span class="string">&quot;2021-1-29&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>now()</span><br><span class="line">call now():</span><br><span class="line"><span class="number">2021</span>-<span class="number">1</span>-<span class="number">29</span></span><br></pre></td></tr></table></figure>
<p>在面向对象（OOP）的设计模式中，decorator被称为装饰模式。OOP的装饰模式需要通过继承和组合来实现，而Python除了能支持OOP的decorator外，直接从语法层次支持decorator。Python的decorator可以用函数实现，也可以用类实现。<br>decorator可以增强函数的功能，定义起来虽然有点复杂，但使用起来非常灵活和方便。</p>
<h2 id="偏函数"><a href="#偏函数" class="headerlink" title="偏函数"></a>偏函数</h2><p>把一个函数的某些参数固定住(设置一个参数的默认值)，返回一个新的函数，可以直接调用这个新的函数。<code>functools.partial(f, args, **kw)</code><br><code>int2 = functools.partial(int, base=2)</code></p>
<h1 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h1><ul>
<li>一个<code>.py</code>文件就是一个模块。</li>
<li>提高了代码的可维护性。编写程序的时候，经常引用其他模块，包括Python内置的模块和来自第三方的模块。</li>
<li>避免了函数名冲突，尽量不要与内置函数名字冲突。自己创建模块时要注意命名，一定不能和Python自带的模块名称冲突。</li>
<li>为了避免模块名冲突，Python又引入了按目录来组织模块的方法，称为包（Package）。<ul>
<li>模块名为”包名.模块名”</li>
<li>每个包必须有一个<code>__init__.py</code>文件</li>
<li>包可以再套娃，多级层次包结构</li>
</ul>
</li>
</ul>
<h2 id="标准模块文件"><a href="#标准模块文件" class="headerlink" title="标准模块文件"></a>标准模块文件</h2><figure class="highlight python"><figcaption><span>std-module</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27; a test module &#x27;</span></span><br><span class="line"></span><br><span class="line">__author__ = <span class="string">&#x27;Michael Liao&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br></pre></td></tr></table></figure>
<ul>
<li>第1行和第2行是标准注释，第1行注释可以让这个文件直接在Unix/Linux/Mac上运行，第2行注释表示.py文件本身使用标准UTF-8编码。</li>
<li>第4行是一个字符串，表示模块的文档注释，任何模块代码的第一个字符串都被视为模块的文档注释。</li>
<li>第6行使用<strong>author</strong>变量把作者写进去，这样当你公开源代码后别人就可以瞻仰你的大名。</li>
</ul>
<h2 id="导入模块"><a href="#导入模块" class="headerlink" title="导入模块"></a>导入模块</h2><p><code>import sys</code>获得变量sys指向该模块，利用sys这个变量，就可以访问sys模块的所有功能。</p>
<ul>
<li>模块有一个argv变量，用list存储了命令行的所有参数。argv至少有一个元素，因为第一个参数永远是该.py文件的名称。</li>
<li>命令行中变量<code>__name__ == __main__</code></li>
</ul>
<h2 id="作用域"><a href="#作用域" class="headerlink" title="作用域"></a>作用域</h2><ul>
<li>正常的函数和变量名是公开的(public)，可以直接被引用。</li>
<li>类似<code>__xxx__</code>这样的变量是特殊变量，可以被直接引用，但是有特殊用途，我们自己的变量一般不要用这种变量名。</li>
<li>类似<code>_xxx</code>和<code>__xxx</code>这样的函数或变量就是非公开的（private），不应该被直接引用，只应该在模块内部引用。</li>
<li>这是一种非常有用的代码封装和抽象的方法，外部不需要引用的函数全部定义成private，只有外部需要引用的函数才定义为public。</li>
</ul>
<h2 id="安装第三方模块"><a href="#安装第三方模块" class="headerlink" title="安装第三方模块"></a>安装第三方模块</h2><p>在Python中，安装第三方模块，是通过包管理工具pip完成的。<br><code>pip install [模块名]</code></p>
<h1 id="面向对象编程-OOP"><a href="#面向对象编程-OOP" class="headerlink" title="面向对象编程 OOP"></a>面向对象编程 OOP</h1><ul>
<li>在Python中，所有数据类型都可以视为对象，当然也可以自定义对象。自定义的对象数据类型就是面向对象中的<strong>类</strong>（Class）的概念。</li>
<li>对象拥有<strong>属性</strong>（Property）, 调用对象对应的关联函数，我们称之为对象的<strong>方法</strong>（Method）。</li>
</ul>
<h2 id="定义类"><a href="#定义类" class="headerlink" title="定义类"></a>定义类</h2><p><code>class [Class name](object):</code></p>
<p><code>class</code>后面紧接着是类名，类名通常是大写开头的单词，紧接着是(object)，表示该类是从哪个类继承下来的。通常，如果没有合适的继承类，就使用object类，这是所有类最终都会继承的类。</p>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><ul>
<li>创建实例：类名+()</li>
<li>可以自由地给一个实例变量绑定属性</li>
<li>通过<code>__init__</code>方法在创建实例时把一些我们认为必须绑定的属性强制填写进去。<ul>
<li>和普通的函数相比，在类中定义的函数只有一点不同，就是<strong>第一个</strong>参数永远是实例变量<code>self</code>(cpp的<code>this</code>)，调用时不用传递</li>
</ul>
</li>
</ul>
<figure class="highlight python"><figcaption><span>class-definition</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name, score</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.score = score</span><br><span class="line"><span class="comment"># 创建实例</span></span><br><span class="line">bart = Student(<span class="string">&#x27;Bart Simpson&#x27;</span>, <span class="number">59</span>)</span><br></pre></td></tr></table></figure>
<p>数据封装，类方法，和cpp类似</p>
<h2 id="访问限制"><a href="#访问限制" class="headerlink" title="访问限制"></a>访问限制</h2><ul>
<li>私有变量：名称前加两个下划线<code>__xxx</code>，外部无法访问(其实只是Python解释器把它解释为了另一个名字)。</li>
<li>双下划线开头双下划线结尾的，是特殊变量，特殊变量是可以直接访问的，不是private变量。</li>
<li>单下划线开头的，“虽然我可以被访问，但是，请把我视为私有变量，不要随意访问”。</li>
</ul>
<h2 id="继承和多态"><a href="#继承和多态" class="headerlink" title="继承和多态"></a>继承和多态</h2><ul>
<li>子类继承父类的方法，相同的方法子类覆盖父类。</li>
<li>多态：对于一个变量，我们只需要知道它是父类型，无需确切地知道它的子类型，就可以放心地调用某个方法，而具体调用的方法是作用在对象上，由运行时该对象的确切类型决定。<ul>
<li>这就是多态真正的威力：调用方只管调用，不管细节，而当我们新增一种子类时，只要确保方法编写正确，不用管原来的代码是如何调用的。</li>
<li>开闭原则：对扩展开放，允许新增子类；对修改封闭：不需要修改依赖父类型的函数。</li>
</ul>
</li>
<li>比java等静态语言更开放的，不要求严格的继承体系，调用方法时只要保证对象有这样的方法。<ul>
<li>这就是动态语言的“鸭子类型”，它并不要求严格的继承体系，一个对象只要“看起来像鸭子，走起路来像鸭子”，那它就可以被看做是鸭子。</li>
</ul>
</li>
</ul>
<h2 id="获得对象信息"><a href="#获得对象信息" class="headerlink" title="获得对象信息"></a>获得对象信息</h2><ul>
<li><code>type()</code>函数，返回对应的Class类型。<ul>
<li><code>int</code>, <code>str</code>可以直接做数据类型关键字。</li>
<li><code>types</code>模块中定义有各种数据类型的常量。</li>
</ul>
</li>
<li><code>isinstance()</code>函数，判断是否是某种类型。<ul>
<li>总是优先使用<code>isinstance()</code>判断类型，可以将指定类型及其子类“一网打尽”。</li>
</ul>
</li>
<li><code>dir()</code>函数，获得一个对象的所有属性和方法，返回一个包含字符串的list。<ul>
<li>配合<code>getattr()</code>、<code>setattr()</code>以及<code>hasattr()</code>，我们可以直接操作一个对象的状态</li>
</ul>
</li>
</ul>
<figure class="highlight python"><figcaption><span>class</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">hasattr</span>(obj, <span class="string">&#x27;x&#x27;</span>) <span class="comment"># 有属性&#x27;x&#x27;吗？</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>obj.x</span><br><span class="line"><span class="number">9</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">hasattr</span>(obj, <span class="string">&#x27;y&#x27;</span>) <span class="comment"># 有属性&#x27;y&#x27;吗？</span></span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">setattr</span>(obj, <span class="string">&#x27;y&#x27;</span>, <span class="number">19</span>) <span class="comment"># 设置一个属性&#x27;y&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">hasattr</span>(obj, <span class="string">&#x27;y&#x27;</span>) <span class="comment"># 有属性&#x27;y&#x27;吗？</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">getattr</span>(obj, <span class="string">&#x27;y&#x27;</span>) <span class="comment"># 获取属性&#x27;y&#x27;</span></span><br><span class="line"><span class="number">19</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>obj.y <span class="comment"># 获取属性&#x27;y&#x27;</span></span><br><span class="line"><span class="number">19</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">hasattr</span>(obj, <span class="string">&#x27;power&#x27;</span>) <span class="comment"># 有属性&#x27;power&#x27;吗？</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">getattr</span>(obj, <span class="string">&#x27;power&#x27;</span>) <span class="comment"># 获取属性&#x27;power&#x27;</span></span><br><span class="line">&lt;bound method MyObject.power of &lt;__main__.MyObject <span class="built_in">object</span> at <span class="number">0x10077a6a0</span>&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fn = <span class="built_in">getattr</span>(obj, <span class="string">&#x27;power&#x27;</span>) <span class="comment"># 获取属性&#x27;power&#x27;并赋值到变量fn</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fn <span class="comment"># fn指向obj.power</span></span><br><span class="line">&lt;bound method MyObject.power of &lt;__main__.MyObject <span class="built_in">object</span> at <span class="number">0x10077a6a0</span>&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fn() <span class="comment"># 调用fn()与调用obj.power()是一样的</span></span><br><span class="line"><span class="number">81</span></span><br></pre></td></tr></table></figure>
<p>类似<code>__xxx__</code>的属性和方法在Python中都是有特殊用途的，比如<code>__len__</code>方法返回长度。在Python中，如果你调用<code>len()</code>函数试图获取一个对象的长度，实际上，在<code>len()</code>函数内部，它自动去调用该对象的<code>__len__()</code>方法</p>
<ul>
<li>只有在不知道对象具体信息时，才会去获取对象的信息</li>
</ul>
<figure class="highlight python"><figcaption><span>good-example</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readImage</span>(<span class="params">fp</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(fp, <span class="string">&#x27;read&#x27;</span>):</span><br><span class="line">        <span class="keyword">return</span> readData(fp)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"><span class="comment"># 假设我们希望从文件流fp中读取图像，我们首先要判断该fp对象是否存在read方法，如果存在，则该对象是一个流，如果不存在，则无法读取</span></span><br></pre></td></tr></table></figure>
<h2 id="实例属性和类属性"><a href="#实例属性和类属性" class="headerlink" title="实例属性和类属性"></a>实例属性和类属性</h2><ul>
<li>给实例绑定属性的方法是通过实例变量，或者通过self变量。</li>
<li>直接在class中定义属性，这种属性是<strong>类属性</strong>，归类所有，类的所有实例都可以访问到，共享。</li>
</ul>
<figure class="highlight python"><figcaption><span>class-property</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    name = <span class="string">&#x27;Student&#x27;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>千万不要对实例属性和类属性使用相同的名字，因为相同名称的实例属性将屏蔽掉类属性，但是当你删除实例属性后，再使用相同的名称，访问到的将是类属性。</li>
</ul>
<h2 id="slots"><a href="#slots" class="headerlink" title="__slots__"></a>__slots__</h2><p>实例可以绑定属性和方法。</p>
<ul>
<li>绑定方法需要使用<code>types</code>模块的<code>Methodtype</code>方法。</li>
<li>对其他实例是不起作用的。<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> types <span class="keyword">import</span> MethodType</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.set_age = MethodType(set_age, s) <span class="comment"># 给实例绑定一个方法</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.set_age(<span class="number">25</span>) <span class="comment"># 调用实例方法</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s.age <span class="comment"># 测试结果</span></span><br><span class="line"><span class="number">25</span></span><br></pre></td></tr></table></figure></li>
<li>为了给所有实例都绑定方法，可以给class绑定方法，所有实例均可调用。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">set_score</span>(<span class="params">self, score</span>):</span></span><br><span class="line"><span class="meta">... </span>    self.score = score</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Student.set_score = set_score</span><br></pre></td></tr></table></figure>
<ul>
<li><code>__slot__</code>可以限制实例的属性。<ul>
<li>仅对当前类实例起作用，对继承的子类是不起作用的。</li>
<li>除非在子类中也定义<code>__slots__</code>，这样，子类实例允许定义的属性就是自身的<code>__slots__</code>加上父类的<code>__slots__</code>。</li>
</ul>
</li>
</ul>
<h2 id="property"><a href="#property" class="headerlink" title="@property"></a>@property</h2><p>装饰器，实现比较复杂，把一个getter方法变成属性，创建另一个装饰器<code>@score.setter</code>，负责把一个setter方法变成属性赋值。<br><figure class="highlight python"><figcaption><span>@property</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._score  <span class="comment"># getter属性</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @score.setter</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span>(<span class="params">self, value</span>):</span> <span class="comment"># setter属性</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(value, <span class="built_in">int</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;score must be an integer!&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> value &lt; <span class="number">0</span> <span class="keyword">or</span> value &gt; <span class="number">100</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;score must between 0 ~ 100!&#x27;</span>)</span><br><span class="line">        self._score = value</span><br></pre></td></tr></table></figure></p>
<ul>
<li>@property使得对实例属性操作时，通过getter和setter方法来实现。</li>
<li>只定义getter方法就相当于定义了一个只读属性。</li>
</ul>
<h2 id="多重继承"><a href="#多重继承" class="headerlink" title="多重继承"></a>多重继承</h2><p>一个子类就可以同时获得多个父类的所有功能。</p>
<ul>
<li>MixIn: 通过多重继承实现，除主线外的其他继承关系。</li>
<li>为了更好地看出继承关系，可以把主线外的继承类命名<code>xxxMixIn</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span>(<span class="params">Mammal, RunnableMixIn, CarnivorousMixIn</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h2 id="定制类"><a href="#定制类" class="headerlink" title="定制类"></a>定制类</h2><p>Python的class中有许多形如<code>__xxx__()</code>这样有特殊用途的函数，可以帮助我们定制类。</p>
<h3 id="str"><a href="#str" class="headerlink" title="__str__"></a>__str__</h3><p><code>__str__</code>方法可以改变类的实例的打印方式，返回用户看到的字符串，<code>__repr__</code>返回程序开发者看到的字符串，用于调试。<br><figure class="highlight python"><figcaption><span>__str__</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name</span>):</span></span><br><span class="line"><span class="meta">... </span>        self.name = name</span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line"><span class="meta">... </span>        <span class="keyword">return</span> <span class="string">&#x27;Student object (name: %s)&#x27;</span> % self.name</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(Student(<span class="string">&#x27;Michael&#x27;</span>))</span><br><span class="line">Student <span class="built_in">object</span> (name: Michael)</span><br><span class="line"><span class="comment"># 原来是&lt;__main__.Student object at 0x109afb190&gt;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="iter"><a href="#iter" class="headerlink" title="__iter__"></a>__iter__</h3><p>使得该对象可以被用于for循环。<br>该方法返回一个迭代对象，然后，Python的for循环就会不断调用该迭代对象的<code>__next__()</code>方法拿到循环的下一个值，直到遇到<code>StopIteration</code>错误时退出循环。<br><figure class="highlight python"><figcaption><span>__iter__</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fib</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.a, self.b = <span class="number">0</span>, <span class="number">1</span> <span class="comment"># 初始化两个计数器a，b</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self <span class="comment"># 实例本身就是迭代对象，故返回自己</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.a, self.b = self.b, self.a + self.b <span class="comment"># 计算下一个值</span></span><br><span class="line">        <span class="keyword">if</span> self.a &gt; <span class="number">100000</span>: <span class="comment"># 退出循环的条件</span></span><br><span class="line">            <span class="keyword">raise</span> StopIteration()</span><br><span class="line">        <span class="keyword">return</span> self.a <span class="comment"># 返回下一个值</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> n <span class="keyword">in</span> Fib():</span><br><span class="line"><span class="meta">... </span>    print(n)</span><br><span class="line">...</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line">...</span><br><span class="line"><span class="number">46368</span></span><br><span class="line"><span class="number">75025</span></span><br></pre></td></tr></table></figure></p>
<h3 id="getitem"><a href="#getitem" class="headerlink" title="__getitem__"></a>__getitem__</h3><p>表现得像list那样按照下标取出元素。<br><figure class="highlight python"><figcaption><span>__getitem__</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fib</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">isinstance</span>(n, <span class="built_in">int</span>)): <span class="comment"># n是索引</span></span><br><span class="line">            a,b = <span class="number">1</span>,<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">                a,b = b,a+b</span><br><span class="line">            <span class="keyword">return</span> a</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">isinstance</span>(n, <span class="built_in">slice</span>)): <span class="comment"># n是切片</span></span><br><span class="line">            start = n.start</span><br><span class="line">            stop = n.start</span><br><span class="line">            <span class="keyword">if</span> start <span class="keyword">is</span>  <span class="literal">None</span>:</span><br><span class="line">                start = <span class="number">0</span></span><br><span class="line">            a,b = <span class="number">1</span>,<span class="number">1</span></span><br><span class="line">            L = []</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(stop):</span><br><span class="line">                <span class="keyword">if</span> x &gt;= start:</span><br><span class="line">                    L.append(a)</span><br><span class="line">                a,b = b,a+b</span><br><span class="line">            <span class="keyword">return</span> L</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = Fib()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f[<span class="number">0</span>]</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f[<span class="number">100</span>]</span><br><span class="line"><span class="number">573147844013817084101</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f[<span class="number">0</span>:<span class="number">5</span>]</span><br><span class="line">[<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>]</span><br></pre></td></tr></table></figure></p>
<ul>
<li>以上<code>__getitem__()</code>方法没有对步长和负数做处理，因此要正确实现一个<code>__getitem__()</code>还是有很多工作要做的</li>
<li>如果把对象看成dict，<code>__getitem__()</code>的参数也可能是一个可以作key的object，例如str，与之对应的还有<code>__setitem__</code>和<code>__delitem__</code></li>
</ul>
<h3 id="getattr"><a href="#getattr" class="headerlink" title="__getattr__"></a>__getattr__</h3><p>动态返回一个属性，当调用不存在的属性时，会试图调用<code>__getattr__(self,属性)</code>来尝试获得属性。</p>
<ul>
<li>只有在没有找到属性的情况下才会调用<strong>getattr</strong>，已有的属性是直接获取的</li>
</ul>
<figure class="highlight python"><figcaption><span>__getattr__</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__getattr__</span>(<span class="params">self, attr</span>):</span></span><br><span class="line">    <span class="keyword">if</span> attr==<span class="string">&#x27;age&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">lambda</span>: <span class="number">25</span></span><br><span class="line">    <span class="keyword">raise</span> AttributeError(<span class="string">&#x27;\&#x27;Student\&#x27; object has no attribute \&#x27;%s\&#x27;&#x27;</span> % attr)</span><br></pre></td></tr></table></figure>
<h3 id="call"><a href="#call" class="headerlink" title="__call__"></a>__call__</h3><p>使实例自身能被当作函数调用。<br><figure class="highlight python"><figcaption><span>__call__</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self</span>):</span></span><br><span class="line">        print(<span class="string">&#x27;My name is %s.&#x27;</span> % self.name)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s = Student(<span class="string">&#x27;Michael&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>s()</span><br><span class="line">My name <span class="keyword">is</span> Michael</span><br></pre></td></tr></table></figure></p>
<ul>
<li>通过<code>callable()</code>函数判断一个对象是否是“可调用”对象。</li>
</ul>
<h2 id="枚举类"><a href="#枚举类" class="headerlink" title="枚举类"></a>枚举类</h2><figure class="highlight python"><figcaption><span>enum</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> enum <span class="keyword">import</span> Enum</span><br><span class="line">Month = Enum(<span class="string">&#x27;Month&#x27;</span>, (<span class="string">&#x27;Jan&#x27;</span>, <span class="string">&#x27;Feb&#x27;</span>, <span class="string">&#x27;Mar&#x27;</span>, <span class="string">&#x27;Apr&#x27;</span>, <span class="string">&#x27;May&#x27;</span>, <span class="string">&#x27;Jun&#x27;</span>, <span class="string">&#x27;Jul&#x27;</span>, <span class="string">&#x27;Aug&#x27;</span>, <span class="string">&#x27;Sep&#x27;</span>, <span class="string">&#x27;Oct&#x27;</span>, <span class="string">&#x27;Nov&#x27;</span>, <span class="string">&#x27;Dec&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>这样我们就获得了Month类型的枚举类，可以直接使用<code>Month.Jan</code>来引用一个常量，或者枚举它的所有成员<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> name, member <span class="keyword">in</span> Month.__members__.items():</span><br><span class="line">    print(name, <span class="string">&#x27;=&gt;&#x27;</span>, member, <span class="string">&#x27;,&#x27;</span>, member.value)</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>value</code>属性则是自动赋给成员的int常量，默认从1开始计数。</li>
</ul>
<p>如果需要更精确地控制枚举类型，可以从Enum派生出自定义类<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> enum <span class="keyword">import</span> Enum, unique</span><br><span class="line"></span><br><span class="line"><span class="meta">@unique </span><span class="comment"># @unique装饰器可以帮助我们检查保证没有重复值。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Weekday</span>(<span class="params">Enum</span>):</span></span><br><span class="line">    Sun = <span class="number">0</span> <span class="comment"># Sun的value被设定为0</span></span><br><span class="line">    Mon = <span class="number">1</span></span><br><span class="line">    Tue = <span class="number">2</span></span><br><span class="line">    Wed = <span class="number">3</span></span><br><span class="line">    Thu = <span class="number">4</span></span><br><span class="line">    Fri = <span class="number">5</span></span><br><span class="line">    Sat = <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 访问</span></span><br><span class="line">Weekday.Mon</span><br><span class="line">Weekday[<span class="string">&#x27;Tue&#x27;</span>]</span><br><span class="line">Weekday(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<ul>
<li>既可以用成员名称引用枚举常量，又可以直接根据value的值获得枚举常量。</li>
</ul>
<h2 id="元类"><a href="#元类" class="headerlink" title="元类"></a>元类</h2><h3 id="type"><a href="#type" class="headerlink" title="type()"></a>type()</h3><ul>
<li>动态语言的函数和类不是编译时定义的，而是运行时动态创建的。</li>
<li><code>type()</code>函数可以查看一个类型或变量的类型, 又可以创建出新的类型。</li>
<li>要创建一个class对象，<code>type()</code>依次传入3个参数<ul>
<li>class的名称</li>
<li>继承的父类集合，注意Python支持多重继承，如果只有一个父类，别忘了tuple的单元素写法</li>
<li>class的方法名称与函数绑定</li>
</ul>
</li>
</ul>
<figure class="highlight python"><figcaption><span>type()</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">fn</span>(<span class="params">self, name=<span class="string">&#x27;world&#x27;</span></span>):</span> <span class="comment"># 先定义函数</span></span><br><span class="line"><span class="meta">... </span>    print(<span class="string">&#x27;Hello, %s.&#x27;</span> % name)</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Hello = <span class="built_in">type</span>(<span class="string">&#x27;Hello&#x27;</span>, (<span class="built_in">object</span>,), <span class="built_in">dict</span>(hello=fn)) <span class="comment"># 创建Hello class</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>h = Hello()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>h.hello()</span><br><span class="line">Hello, world.</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="built_in">type</span>(Hello))</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">type</span>&#x27;&gt;</span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">print</span>(<span class="params"><span class="built_in">type</span>(<span class="params">h</span>)</span>)</span></span><br><span class="line"><span class="class">&lt;<span class="title">class</span> &#x27;<span class="title">__main__</span>.<span class="title">Hello</span>&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="metaclass"><a href="#metaclass" class="headerlink" title="metaclass()"></a>metaclass()</h2><p>据说很复杂，等用到了再来看orz</p>
<h1 id="错误、调试和测试"><a href="#错误、调试和测试" class="headerlink" title="错误、调试和测试"></a>错误、调试和测试</h1><h2 id="try"><a href="#try" class="headerlink" title="try"></a>try</h2><ul>
<li>当我们认为某些代码可能会出错时，就可以用try来运行这段代码。</li>
<li>如果执行出错，则后续代码不会继续执行，而是直接跳转至错误处理代码，即except语句块，执行完except后，如果有finally语句块，则执行finally语句块，至此，执行完毕。</li>
<li>如果发生了不同类型的错误，可以有多个except来捕获不同类型的错误。</li>
<li>如果没有错误发生，可以在except语句块后面加一个else，当没有错误发生时，会自动执行else语句。</li>
</ul>
<figure class="highlight python"><figcaption><span>try</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    print(<span class="string">&#x27;try...&#x27;</span>)</span><br><span class="line">    r = <span class="number">10</span> / <span class="built_in">int</span>(<span class="string">&#x27;2&#x27;</span>)</span><br><span class="line">    print(<span class="string">&#x27;result:&#x27;</span>, r)</span><br><span class="line"><span class="keyword">except</span> ValueError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">&#x27;ValueError:&#x27;</span>, e)</span><br><span class="line"><span class="keyword">except</span> ZeroDivisionError <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">&#x27;ZeroDivisionError:&#x27;</span>, e)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">&#x27;no error!&#x27;</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    print(<span class="string">&#x27;finally...&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;END&#x27;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>Python的错误其实也是class，所有的错误类型都继承自BaseException，所以在使用except时需要注意的是，它不但捕获该类型的错误，还把其子类也“一网打尽”。</li>
<li>常见的错误类型和继承关系看<a href="https://docs.python.org/3/library/exceptions.html#exception-hierarchy">这里</a></li>
<li>可以跨越多层调用, 不需要在每个可能出错的地方去捕获错误，只要在合适的层次去捕获错误就可以了</li>
</ul>
<h2 id="调用栈"><a href="#调用栈" class="headerlink" title="调用栈"></a>调用栈</h2><p>出错的时候，一定要分析错误的调用栈信息，才能定位错误的位置。</p>
<h2 id="记录错误"><a href="#记录错误" class="headerlink" title="记录错误"></a>记录错误</h2><p>Python内置的<code>logging</code>模块可以非常容易地记录错误信息。</p>
<ul>
<li>程序打印完错误信息后会继续执行，并正常退出。</li>
<li>通过配置，<code>logging</code>还可以把错误记录到日志文件里，方便事后排查。</li>
</ul>
<h2 id="抛出错误"><a href="#抛出错误" class="headerlink" title="抛出错误"></a>抛出错误</h2><ul>
<li>根据需要，可以定义一个错误的类，选择好继承关系，用<code>raise</code>语句抛出一个错误的实例。</li>
<li>只有在必要的时候才定义我们自己的错误类型。如果可以选择Python已有的内置的错误类型，尽量使用Python内置的错误类型。</li>
<li>当前函数不知道应该怎么处理该错误，继续往上抛，让顶层调用者去处理。</li>
<li>在<code>except</code>中<code>raise</code>一个Error，还可以把一种类型的错误转化成另一种类型</li>
</ul>
<figure class="highlight python"><figcaption><span>raise-err</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>(<span class="params">s</span>):</span></span><br><span class="line">    n = <span class="built_in">int</span>(s)</span><br><span class="line">    <span class="keyword">if</span> n==<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&#x27;invalid value: %s&#x27;</span> % s)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">10</span> / n</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bar</span>():</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        foo(<span class="string">&#x27;0&#x27;</span>)</span><br><span class="line">    <span class="keyword">except</span> ValueError <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">&#x27;ValueError!&#x27;</span>)</span><br><span class="line">        <span class="keyword">raise</span>  <span class="comment"># 不带参数，就会把当前错误原样抛出</span></span><br><span class="line"></span><br><span class="line">bar()</span><br></pre></td></tr></table></figure>
<h2 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h2><ul>
<li><code>print</code>调试法</li>
<li><code>assert</code>调试法<ul>
<li><code>-O</code>可以关闭<code>assert</code></li>
</ul>
</li>
</ul>
<figure class="highlight python"><figcaption><span>assert</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span>(<span class="params">s</span>):</span></span><br><span class="line">    <span class="keyword">assert</span> n != <span class="number">0</span>, <span class="string">&#x27;n is zero!&#x27;</span></span><br><span class="line">    <span class="comment">#表达式应该为True，否则输出AssertionError+后接的字符串</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>logging</code>调试法<ul>
<li>可以指定记录信息的级别，有<code>DEBUG</code> &gt; <code>INFO</code> &gt; <code>WARNING</code> &gt; <code>ERROR</code>等几个级别。这样一来，你可以放心地输出不同级别的信息，也不用删除，最后统一控制输出哪个级别的信息。</li>
<li>另一个好处是通过简单的配置，一条语句可以同时输出到不同的地方，比如console和文件。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><figcaption><span>logging</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logging.basicConfig(level=logging.INFO)</span><br><span class="line">logging.basicConfig(level=logging.INFO) <span class="comment"># 配置</span></span><br><span class="line">logging.info(<span class="string">&#x27;n = %d&#x27;</span> % n) <span class="comment"># 输出信息</span></span><br></pre></td></tr></table></figure>
<ul>
<li>pdb调试法<ul>
<li>命令和gdb差不多</li>
<li><code>pdb.set_trace()</code>设置断点，在运行时程序会自动暂停并进入pdb</li>
</ul>
</li>
</ul>
<figure class="highlight python"><figcaption><span>pdb</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pdb</span><br><span class="line"></span><br><span class="line">s = <span class="string">&#x27;0&#x27;</span></span><br><span class="line">n = <span class="built_in">int</span>(s)</span><br><span class="line">pdb.set_trace() <span class="comment"># 运行到这里会自动暂停</span></span><br><span class="line">print(<span class="number">10</span> / n)</span><br></pre></td></tr></table></figure>
<ul>
<li>IDE调试法</li>
</ul>
<h2 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h2><ul>
<li>对拍(?), difftest(?), 就是通过样例对设计进行正确性检验。</li>
<li>单元测试的测试用例要覆盖常用的输入组合、边界条件和异常。</li>
<li><code>unittest</code>模块提供了方便测试的流程<ul>
<li>单元测试时，我们需要编写一个测试类，从unittest.TestCase继承。</li>
<li>以test开头的方法就是测试方法，不以test开头的方法不被认为是测试方法，测试的时候不会被执行。</li>
<li>运行单元测试，<code>if __name__ == &#39;__main__&#39;:\unittest.main()</code> 或 <code>python -m unittest xxx.py</code>(推荐)</li>
</ul>
</li>
</ul>
<figure class="highlight python"><figcaption><span>unittest</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> unittest</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mydict <span class="keyword">import</span> Dict</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestDict</span>(<span class="params">unittest.TestCase</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_init</span>(<span class="params">self</span>):</span></span><br><span class="line">        d = Dict(a=<span class="number">1</span>, b=<span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">        self.assertEqual(d.a, <span class="number">1</span>)  <span class="comment"># 断言函数返回的结果与1相等</span></span><br><span class="line">        self.assertEqual(d.b, <span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">        self.assertTrue(<span class="built_in">isinstance</span>(d, <span class="built_in">dict</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_key</span>(<span class="params">self</span>):</span></span><br><span class="line">        d = Dict()</span><br><span class="line">        d[<span class="string">&#x27;key&#x27;</span>] = <span class="string">&#x27;value&#x27;</span></span><br><span class="line">        self.assertEqual(d.key, <span class="string">&#x27;value&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_attr</span>(<span class="params">self</span>):</span></span><br><span class="line">        d = Dict()</span><br><span class="line">        d.key = <span class="string">&#x27;value&#x27;</span></span><br><span class="line">        self.assertTrue(<span class="string">&#x27;key&#x27;</span> <span class="keyword">in</span> d)</span><br><span class="line">        self.assertEqual(d[<span class="string">&#x27;key&#x27;</span>], <span class="string">&#x27;value&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_keyerror</span>(<span class="params">self</span>):</span></span><br><span class="line">        d = Dict()</span><br><span class="line">        <span class="keyword">with</span> self.assertRaises(KeyError): <span class="comment"># 期待抛出指定类型的Error</span></span><br><span class="line">            value = d[<span class="string">&#x27;empty&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_attrerror</span>(<span class="params">self</span>):</span></span><br><span class="line">        d = Dict()</span><br><span class="line">        <span class="keyword">with</span> self.assertRaises(AttributeError):</span><br><span class="line">            value = d.empty</span><br></pre></td></tr></table></figure>
<h2 id="文档测试-Doctest"><a href="#文档测试-Doctest" class="headerlink" title="文档测试 Doctest"></a>文档测试 Doctest</h2><ul>
<li>直接提取注释中的代码并执行测试。</li>
<li>严格按照Python交互式命令行的输入和输出来判断测试结果是否正确。只有测试异常的时候，可以用…表示中间一大段烦人的输出。</li>
<li>只有在命令行直接运行时，才执行doctest。</li>
</ul>
<h1 id="IO编程"><a href="#IO编程" class="headerlink" title="IO编程"></a>IO编程</h1><p>IO编程中，Stream（流）是一个很重要的概念，可以把流想象成一个水管，数据就是水管里的水，但是只能单向流动。Input Stream就是数据从外面（磁盘、网络）流进内存，Output Stream就是数据从内存流到外面去。<br>同步IO, 异步IO</p>
<h2 id="读文件"><a href="#读文件" class="headerlink" title="读文件"></a>读文件</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = <span class="built_in">open</span>(<span class="string">&#x27;/Users/michael/test.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="comment"># 传入文件名和标示符, &#x27;r&#x27;表示读</span></span><br><span class="line"><span class="comment"># 文件不存在会抛出IOError错误</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.read()</span><br><span class="line"><span class="string">&#x27;Hello, world!&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.close() <span class="comment"># 记得关</span></span><br></pre></td></tr></table></figure>
<p>为了保证无论是否出错都能正确地关闭文件，我们可以使用<code>try ... finally</code>来实现:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    f = <span class="built_in">open</span>(<span class="string">&#x27;/path/to/file&#x27;</span>, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    print(f.read())</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="keyword">if</span> f:</span><br><span class="line">        f.close()</span><br></pre></td></tr></table></figure><br>太繁琐, 和上面一样，会自动调用<code>f.close()</code>:<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/path/to/file&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    print(f.read())</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>read(size)</code>一次最多读取size个字节的内容</li>
<li><code>readline()</code>每次读取一行</li>
<li><code>readlines()</code>一次读取所有内容并按行返回list</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">    print(line.strip()) <span class="comment"># 把末尾的&#x27;\n&#x27;删掉</span></span><br></pre></td></tr></table></figure>
<p>像<code>open()</code>函数返回的这种有个<code>read()</code>方法的对象，在Python中统称为file-like Object。除了file外，还可以是内存的字节流，网络流，自定义流等等。file-like Object不要求从特定类继承，只要写个<code>read()</code>方法就行。<br><code>StringIO</code>就是在内存中创建的file-like Object，常用作临时缓冲</p>
<ul>
<li>默认都是读取文本文件，并且是UTF-8编码的文本文件。要读取二进制文件，比如图片、视频等等，用’rb’模式打开文件</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = <span class="built_in">open</span>(<span class="string">&#x27;/Users/michael/test.jpg&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.read()</span><br><span class="line"><span class="string">b&#x27;\xff\xd8\xff\xe1\x00\x18Exif\x00\x00...&#x27;</span> <span class="comment"># 十六进制表示的字节</span></span><br></pre></td></tr></table></figure>
<ul>
<li>读取默认使用UTF-8编码，需要编码转换的时候要给open()传入encoding参数,errors参数表示出现错误后怎么处理，一般选择忽略</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = <span class="built_in">open</span>(<span class="string">&#x27;/Users/michael/gbk.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;gbk&#x27;</span>, errors=<span class="string">&#x27;ignore&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="写文件"><a href="#写文件" class="headerlink" title="写文件"></a>写文件</h2><ul>
<li>调用open()函数时，传入标识符’w’或者’wb’表示写文本文件或写二进制文件</li>
<li>用<code>with</code>语句保险</li>
<li>以’w’模式写入文件时，如果文件已存在，会直接覆盖。</li>
<li>传入’a’以追加（append）模式写入。</li>
</ul>
<h2 id="StringIO"><a href="#StringIO" class="headerlink" title="StringIO"></a>StringIO</h2><p>可以用一个str初始化StringIO，然后，像读文件一样读取<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> io <span class="keyword">import</span> StringIO</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = StringIO() <span class="comment"># 创建StringIO对象</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.write(<span class="string">&#x27;hello&#x27;</span>)</span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.write(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.write(<span class="string">&#x27;world!&#x27;</span>)</span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(f.getvalue()) <span class="comment">#getvalue()用于获得写入后的str</span></span><br><span class="line">hello world!</span><br></pre></td></tr></table></figure></p>
<h2 id="BytesIO"><a href="#BytesIO" class="headerlink" title="BytesIO"></a>BytesIO</h2><p>StringIO操作的只能是str，如果要操作二进制数据，就需要使用BytesIO。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = BytesIO()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.write(<span class="string">&#x27;中文&#x27;</span>.encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(f.getvalue())</span><br><span class="line"><span class="string">b&#x27;\xe4\xb8\xad\xe6\x96\x87&#x27;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="操作文件和目录"><a href="#操作文件和目录" class="headerlink" title="操作文件和目录"></a>操作文件和目录</h2><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><p>在操作系统中定义的环境变量，全部保存在os.environ这个变量中，可以直接查看<code>os.vnviron</code>, 是个dict<br>要获取某个环境变量的值，可以调用<code>os.environ.get(&#39;key&#39;)</code></p>
<ul>
<li>操作文件和目录的函数一部分放在os模块中，一部分放在os.path模块中</li>
<li>shutil模块有很多对os的补充<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看当前目录的绝对路径:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>os.path.abspath(<span class="string">&#x27;.&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;/Users/michael&#x27;</span></span><br><span class="line"><span class="comment"># 在某个目录下创建一个新目录，首先把新目录的完整路径表示出来:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>os.path.join(<span class="string">&#x27;/Users/michael&#x27;</span>, <span class="string">&#x27;testdir&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;/Users/michael/testdir&#x27;</span></span><br><span class="line"><span class="comment"># 然后创建一个目录:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>os.mkdir(<span class="string">&#x27;/Users/michael/testdir&#x27;</span>)</span><br><span class="line"><span class="comment"># 删掉一个目录:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>os.rmdir(<span class="string">&#x27;/Users/michael/testdir&#x27;</span>)</span><br><span class="line"></span><br><span class="line">os.path.join() <span class="comment"># 把两个路径合成一个</span></span><br><span class="line">os.path.split() <span class="comment"># 拆分路径</span></span><br><span class="line">os.path.splitext() <span class="comment"># 直接得到文件扩展名</span></span><br><span class="line"><span class="comment"># 这些合并、拆分路径的函数并不要求目录和文件要真实存在，它们只对字符串进行操作。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对文件重命名:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>os.rename(<span class="string">&#x27;test.txt&#x27;</span>, <span class="string">&#x27;test.py&#x27;</span>)</span><br><span class="line"><span class="comment"># 删掉文件:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>os.remove(<span class="string">&#x27;test.py&#x27;</span>)</span><br></pre></td></tr></table></figure>
一些应用：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>[x <span class="keyword">for</span> x <span class="keyword">in</span> os.listdir(<span class="string">&#x27;.&#x27;</span>) <span class="keyword">if</span> os.path.isdir(x)] <span class="comment"># 列出当前目录下的所有目录</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[x <span class="keyword">for</span> x <span class="keyword">in</span> os.listdir(<span class="string">&#x27;.&#x27;</span>) <span class="keyword">if</span> os.path.isfile(x) <span class="keyword">and</span> os.path.splitext(x)[<span class="number">1</span>]==<span class="string">&#x27;.py&#x27;</span>] <span class="comment"># 列出所有.py文件</span></span><br></pre></td></tr></table></figure>
<h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2>pickle模块，用到再看。</li>
</ul>
<h1 id="进程和线程"><a href="#进程和线程" class="headerlink" title="进程和线程"></a>进程和线程</h1><p>线程是最小的执行单元，而进程由至少一个线程组成。如何调度进程和线程，完全由操作系统决定，程序自己不能决定什么时候执行，执行多长时间。</p>
<h2 id="多进程-multiprocessing"><a href="#多进程-multiprocessing" class="headerlink" title="多进程(multiprocessing)"></a>多进程(multiprocessing)</h2><p>Unix/Linux操作系统提供了一个<code>fork()</code>系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是<code>fork()</code>调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。</p>
<ul>
<li>子进程永远返回0，而父进程返回子进程的ID。</li>
<li>Python的os模块封装了常见的系统调用，其中就包括fork.</li>
<li>multiprocessing模块就是跨平台版本的多进程模块。multiprocessing模块提供了一个Process类来代表一个进程对象。</li>
<li>如果要启动大量的子进程，可以用进程池(Pool)的方式批量创建子进程。</li>
</ul>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>language</tag>
      </tags>
  </entry>
  <entry>
    <title>吴恩达机器学习笔记</title>
    <url>/2021/02/03/ML-AN/</url>
    <content><![CDATA[<h1 id="Lecture-1"><a href="#Lecture-1" class="headerlink" title="Lecture 1"></a>Lecture 1</h1><h2 id="监督学习-Supervised-Learning"><a href="#监督学习-Supervised-Learning" class="headerlink" title="监督学习(Supervised Learning)"></a>监督学习(Supervised Learning)</h2><ul>
<li>监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成，再根据这些样本作出预测。</li>
<li>回归问题，即通过回归来推出一个<strong>连续</strong>的输出。</li>
<li>分类问题，其目标是推出一组<strong>离散</strong>的结果。</li>
</ul>
<a id="more"></a>
<h2 id="无监督学习-Unsupervised-Learning"><a href="#无监督学习-Unsupervised-Learning" class="headerlink" title="无监督学习(Unsupervised Learning)"></a>无监督学习(Unsupervised Learning)</h2><ul>
<li>在无监督学习中，我们已知的数据。不同于监督学习的数据，即无监督学习中没有任何的标签或者是有相同的标签或者就是没标签。</li>
<li>监督学习算法可能会把这些数据分成不同的簇，所以叫做聚类算法。</li>
</ul>
<p>分离音频代码: <code>[W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x&#39;)</code></p>
<hr>
<h1 id="Lecture-2"><a href="#Lecture-2" class="headerlink" title="Lecture 2"></a>Lecture 2</h1><p>本章主要介绍单变量线性回归问题，其中的一些基本概念和梯度下降法解决回归问题。</p>
<h2 id="模型表示-Model-Representation"><a href="#模型表示-Model-Representation" class="headerlink" title="模型表示(Model Representation)"></a>模型表示(Model Representation)</h2><p>在监督学习中我们有一个数据集，这个数据集被称训练集(Training Set)。</p>
<ul>
<li>$m$: 训练集中实例的数量</li>
<li>$x$: 特征/输入变量</li>
<li>$y$: 目标/输出变量</li>
<li>$(x, y)$: 训练集中的实例  </li>
<li>$(x^{(i)}, y^{(i)})$: 第$i$个观察实例  </li>
<li>$h$: 代表学习算法的解决方案或函数也称为假设函数(hypothesis), 从$x$到$y$的映射<ul>
<li>一种$h$的表达方式: $h_{\theta}(x) = \theta_{0} + \theta_{1}x$. 因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。</li>
</ul>
</li>
</ul>
<h2 id="代价函数-Cost-Function"><a href="#代价函数-Cost-Function" class="headerlink" title="代价函数(Cost Function)"></a>代价函数(Cost Function)</h2><p>我们现在要做的便是为我们的模型选择合适的<strong>参数</strong>(<strong>parameters</strong>) $\theta_{0}$和$\theta_{1}$, 我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距就是<strong>建模误差</strong>(<strong>modeling error</strong>)<br>我们的目标便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数 $J \left( \theta_0, \theta_1 \right) = \dfrac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$ 最小。</p>
<ul>
<li><strong>代价函数</strong>也被称作平方误差函数，有时也被称为平方误差代价函数。对于大多数问题，特别是回归问题，都是一个合理的选择。</li>
</ul>
<h2 id="梯度下降-Gradient-Descent"><a href="#梯度下降-Gradient-Descent" class="headerlink" title="梯度下降(Gradient Descent)"></a>梯度下降(Gradient Descent)</h2><p>梯度下降是一个用来求函数最小值的算法，背后的思想是：开始时我们随机选择一个参数的组合$\left( {\theta_{0}},{\theta_{1}},……,{\theta_{n}} \right)$，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到找到一个局部最小值（<strong>local minimum</strong>），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（<strong>global minimum</strong>），选择不同的初始参数组合，可能会找到不同的局部最小值。<br>批量梯度下降(<strong>batch gradient descent</strong>)算法的公式为:<br><strong>repeat until convergence{</strong></p>
<script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \dfrac{\partial}{\partial \theta_{j}} J\left(\theta_{0}, \theta_{1}\right)</script><p><strong>}</strong></p>
<p>其中$\alpha$是学习率(<strong>learning rate</strong>)，它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都<strong>同时</strong>让所有的参数减去学习速率乘以代价函数的导数。</p>
<h2 id="梯度下降的线性回归-Gradient-Descent-For-Linear-Regression"><a href="#梯度下降的线性回归-Gradient-Descent-For-Linear-Regression" class="headerlink" title="梯度下降的线性回归(Gradient Descent For Linear Regression)"></a>梯度下降的线性回归(Gradient Descent For Linear Regression)</h2><p>对我们之前的线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即:<br>$\dfrac{\partial}{\partial \theta_{j}} J(\theta_{0}, \theta_{1}) = \dfrac{\partial}{\partial \theta_{j}} \dfrac{1}{2m}\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^{2}$</p>
<p>$j=0$ 时：$\dfrac{\partial}{\partial \theta_{0}}J(\theta_{0}, \theta_{1}) = \dfrac{1}{m}\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})$</p>
<p>$j=1$ 时：$\dfrac{\partial}{\partial \theta_{1}}J(\theta_{0}, \theta_{1}) = \dfrac{1}{m}\sum\limits_{i=1}^{m}((h_{\theta}(x^{(i)}) - y^{(i)}) \cdot x^{(i)})$</p>
<p>则算法改写成:<br><strong>repeat until convergence{</strong><br>$\qquad \theta_{0} := \theta_{0} - \alpha \dfrac{1}{m}\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})$<br>$\qquad \theta_{1} := \theta_{1} - \alpha \dfrac{1}{m}\sum\limits_{i=1}^{m}((h_{\theta}(x^{(i)}) - y^{(i)}) \cdot x^{(i)})$<br>​               <strong>}</strong></p>
<ul>
<li>如果你之前学过线性代数, 你应该知道有一种计算代价函数$J$最小值的数值解法, 这是另一种称为正规方程(<strong>normal equations</strong>)的方法。</li>
<li>实际上在数据量较大的情况下，梯度下降法比正规方程要更适用一些。</li>
</ul>
<hr>
<h1 id="Lecture-3"><a href="#Lecture-3" class="headerlink" title="Lecture 3"></a>Lecture 3</h1><p>本章主要复习线性代数，虽然我线代没考好但是我觉得掌握的只是应该够用了，略(doge)。</p>
<hr>
<h1 id="Lecture-4"><a href="#Lecture-4" class="headerlink" title="Lecture 4"></a>Lecture 4</h1><h2 id="多维特征-Multiple-Features"><a href="#多维特征-Multiple-Features" class="headerlink" title="多维特征 Multiple Features"></a>多维特征 Multiple Features</h2><p>多个变量的模型中的特征为$\left( {x_{1}},{x_{2}},…,{x_{n}} \right)$。<br>新的注释:</p>
<ul>
<li>$n$: 特征的数量</li>
<li>$x^{\left( i \right)}$: 第 $i$ 个训练实例，是特征矩阵中的第$i$行，是一个<strong>向量</strong>（<strong>vector</strong>）</li>
<li>$x_{j}^{\left( i \right)}$: 特征矩阵中第 $i$ 行的第 $j$ 个特征，也就是第 $i$ 个训练实例的第 $j$ 个特征</li>
</ul>
<p>支持多变量的假设 $h$ 表示为：$h_{\theta}\left( x \right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}$，这个公式中有$n+1$个参数和$n$个变量，为了使得公式能够简化一些，引入$x_{0}=1$，则公式转化为：$h_{\theta} \left( x \right)=\theta_{0}x_{0} + \theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}$<br>此时模型中的参数是一个$n+1$维的向量，任何一个训练实例也都是$n+1$维的向量，特征矩阵$X$的维度是 $m*(n+1)$。 因此公式可以简化为：$h_{\theta} \left( x \right)=\theta^{T}X$，其中上标$T$代表矩阵转置。</p>
<h2 id="多变量梯度下降-Gradient-Descent-for-Multiple-Variables"><a href="#多变量梯度下降-Gradient-Descent-for-Multiple-Variables" class="headerlink" title="多变量梯度下降 Gradient Descent for Multiple Variables"></a>多变量梯度下降 Gradient Descent for Multiple Variables</h2><p>代价函数: $J\left( \theta_{0},\theta_{1}…\theta_{n} \right)=\dfrac{1}{2m}\sum\limits_{i=1}^{m}\left( h_{\theta} \left(x^{\left( i \right)} \right)-y^{\left( i \right)} \right)^{2}$<br>其中：$h_{\theta}\left( x \right)=\theta^{T}X=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+…+\theta_{n}x_{n}$， 我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。<br>算法为:<br><strong>repeat until convergence{</strong></p>
<script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \dfrac{\partial}{\partial \theta_{j}} J\left(\theta_{0}, \theta_{1}, ..., \theta_{n} \right)</script><p><strong>}</strong><br>即:<br><strong>repeat until convergence{</strong></p>
<script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \dfrac{\partial}{\partial \theta_{j}} \dfrac{1}{2m} \sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)}) ^{2}</script><p><strong>}</strong><br>求导后得:<br><strong>repeat until convergence{</strong></p>
<script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \dfrac{1}{m} \sum\limits_{i=1}^{m}((h_{\theta}(x^{(i)}) - y^{(i)}) * x_{j}^{(i)})</script><p>(simultaneously update $\theta_{j}$ for $i = 1,2,…,n$)</p>
<p><strong>}</strong><br>我们开始随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span>(<span class="params">X, y, theta</span>):</span></span><br><span class="line">    inner = np.power(((X * theta.T) - y), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(inner) / (<span class="number">2</span> * <span class="built_in">len</span>(X))</span><br></pre></td></tr></table></figure>
<h2 id="特征缩放-Feature-Scaling"><a href="#特征缩放-Feature-Scaling" class="headerlink" title="特征缩放 Feature Scaling"></a>特征缩放 Feature Scaling</h2><p>在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛(标准化，归一化)。</p>
<ul>
<li>最简单的方法是令：$x_{n}=\dfrac{x_{n}-\mu_{n}}{s_{n}}$，其中 $\mu_{n}$是平均值，$s_{n}$是标准差。</li>
</ul>
<h2 id="学习率-Learning-Rate"><a href="#学习率-Learning-Rate" class="headerlink" title="学习率 Learning Rate"></a>学习率 Learning Rate</h2><p>梯度下降算法的每次迭代受到学习率的影响，如果学习率$\alpha$过小，则达到收敛所需的迭代次数会非常高；如果学习率$\alpha$过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。</p>
<h2 id="特征和多项式回归-Features-and-Polynomial-Regression"><a href="#特征和多项式回归-Features-and-Polynomial-Regression" class="headerlink" title="特征和多项式回归 Features and Polynomial Regression"></a>特征和多项式回归 Features and Polynomial Regression</h2><p>线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据, 通常我们需要先观察数据然后再决定准备尝试怎样的模型。</p>
<ul>
<li>我们可以令：$x_{2}=x^{2},x_{3}=x^{3}$，从而将模型转化为线性回归模型。</li>
<li>如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。</li>
</ul>
<h2 id="正规方程-Normal-Equation"><a href="#正规方程-Normal-Equation" class="headerlink" title="正规方程 Normal Equation"></a>正规方程 Normal Equation</h2><p>正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\dfrac{\partial}{\partial \theta_{j}}J\left( \theta_{j} \right)=0$ 。<br> 假设我们的训练集特征矩阵为 $X$（包含了 $x_{0}=1$）并且我们的训练集结果为向量 $y$，则利用正规方程解出向量 $\theta =\left( X^{T}X \right)^{-1}X^{T}y$.<br>上标$T$代表矩阵转置，上标$-1$代表矩阵的逆。</p>
<ul>
<li>只要特征变量的数目并不大，标准方程是一个很好的计算参数$\theta $的替代方法。具体地，只要特征变量数量小于一万，标准方程法，而不使用梯度下降法。</li>
</ul>
<h2 id><a href="#" class="headerlink" title></a><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalEqn</span>(<span class="params">X, y</span>):</span></span><br><span class="line">    theta = np.linalg.inv(X.T@X)@X.T@y <span class="comment">#X.T@X等价于X.T.dot(X)</span></span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure></h2><h1 id="Lecture-5"><a href="#Lecture-5" class="headerlink" title="Lecture 5"></a>Lecture 5</h1><p>本章主要是<strong>Ovtave</strong>的语法学习，现在基本使用<strong>Python</strong>，跳过本章。</p>
<hr>
<h1 id="Lecture-6"><a href="#Lecture-6" class="headerlink" title="Lecture 6"></a>Lecture 6</h1><p>本章主要是对于解决分类问题的逻辑回归算法的讲解。</p>
<h2 id="分类问题-Classification"><a href="#分类问题-Classification" class="headerlink" title="分类问题 Classification"></a>分类问题 Classification</h2><p>我们将因变量(<strong>dependent variable</strong>)可能属于的两个类分别称为负向类（<strong>negative class</strong>）和正向类（<strong>positive class</strong>），则因变量$y\in \{ 0,1 \}$ ，其中 0 表示负向类，1 表示正向类。</p>
<h2 id="假说表示-Hypothesis-Representation"><a href="#假说表示-Hypothesis-Representation" class="headerlink" title="假说表示 Hypothesis Representation"></a>假说表示 Hypothesis Representation</h2><p>我们希望我们的分类器的输出值在<strong>0和1之间</strong>，因此，我们希望想出一个满足某个性质的假设函数，这个性质是它的预测值要在0和1之间。<br>我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在0和1之间。<br>逻辑回归模型的假设是： $h_{\theta}(x) = g(\theta^{T}X)$<br>其中：<br>$X$ 代表特征向量<br>$g$ 代表逻辑函数（<strong>logistic function</strong>)是一个常用的逻辑函数为<strong>S</strong>形函数（<strong>Sigmoid function</strong>），公式为： $g(z) = \dfrac{1}{1 + e^{-z}}$.</p>
<figure class="highlight python"><figcaption><span>sigmoid</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br></pre></td></tr></table></figure>
<p>函数图像为:<br><img data-src="g.jpg" alt="sigmoid(z)"><br>$h_{\theta}( x )$的作用是，对于给定的输入变量，根据选择的参数计算输出变量=1的可能性（<strong>estimated probablity</strong>）即$h_{\theta} ( x )=P( y=1|x;\theta)$</p>
<h2 id="判定边界-Decision-Boundary"><a href="#判定边界-Decision-Boundary" class="headerlink" title="判定边界 Decision Boundary"></a>判定边界 Decision Boundary</h2><p>线性规划？反正就是约束条件所导致的曲线分界线。<br>我们可以用非常复杂的模型来适应非常复杂形状的判定边界。</p>
<h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>对于线性回归模型，我们定义的代价函数是所有模型误差的平方和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将$h_{\theta}(x) = \dfrac{1}{1 + e^{-\theta^{T}X}}$带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（<strong>non-convexfunction</strong>）。<br>这意味着我们的代价函数有许多<strong>局部最小值</strong>，这将影响梯度下降算法寻找全局最小值。<br>我们重新定义逻辑回归的代价函数为: $J(\theta) = \dfrac{1}{m}\sum\limits_{i=1}^{n} Cost(h_{\theta}(x^{(i)}), y^{(i)})$, 其中</p>
<script type="math/tex; mode=display">cost(h_{\theta}(x^{(i)}), y^{(i)})=\left\{
\begin{aligned}
 \log{(h_{\theta}(x))}, &y = 1\\
 \log{(1- h_{\theta}(x))}, &y = 0
\end{aligned}
\right.</script><p>再简化得:</p>
<script type="math/tex; mode=display">Cost(h_{\theta}(x^{(i)}), y^{(i)}) = -y \times \log{(h_{\theta}(x))} - (1 - y) \times \log{(1- h_{\theta}(x))}</script><p>带入代价函数得:</p>
<script type="math/tex; mode=display">J(\theta) = -\dfrac{1}{m}\sum\limits_{i=1}^{n} [y \times \log{(h_{\theta}(x))} + (1 - y) \times \log{(1- h_{\theta}(x))}]</script><figure class="highlight python"><figcaption><span>cost</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span>(<span class="params">theta, X, y</span>):</span></span><br><span class="line">    theta = np.matrix(theta)</span><br><span class="line">    X = np.matrix(X)</span><br><span class="line">    y = np.matrix(y)</span><br><span class="line">    first = np.multiply(-y, np.log(sigmoid(X* theta.T)))</span><br><span class="line">    second = np.multiply((<span class="number">1</span> - y), np.log(<span class="number">1</span> - sigmoid(X* theta.T)))</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(first - second) / (<span class="built_in">len</span>(X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># python实现的tips: 一般都是将输入用np.matrix向量化，再通过np的一些方法(np.sum, np.multiply)来运算，减少循环</span></span><br></pre></td></tr></table></figure>
<p>凸性分析的内容是超出这门课的范围的，但是可以证明我们所选的代价值函数会给我们一个凸优化问题。代价函数$J(\theta)$会是一个凸函数，并且没有局部最优值。</p>
<p>在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。<br><strong>repeat until convergence{</strong></p>
<script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \dfrac{\partial}{\partial \theta_{j}} J\left(\theta \right)</script><p><strong>}</strong></p>
<p>考虑$h_{\theta}(x) = \dfrac{1}{1 + e^{-\theta^{T}X}}$, 有：</p>
<script type="math/tex; mode=display">\dfrac{\partial}{\partial \theta_{j}} J(\theta) = \dfrac{1}{m} \sum\limits_{i=1}^{n} \left[(h_{\theta}(x^{(i)}) -y^{(i)}) x_{j}^{(i)}\right]</script><p><del>推导略，不信的话自己去算算</del></p>
<ul>
<li>虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的$h_{\theta}(x)$与线性回归中不同，所以实际上是不一样的。另外，在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。</li>
</ul>
<p>除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。这些算法有：<strong>共轭梯度</strong>（<strong>Conjugate Gradient</strong>），<strong>局部优化法</strong>(<strong>Broyden fletcher goldfarb shann,BFGS</strong>)和<strong>有限内存局部优化法</strong>(<strong>LBFGS</strong>) </p>
<h2 id="多类别分类-Multiclass-Classification"><a href="#多类别分类-Multiclass-Classification" class="headerlink" title="多类别分类 Multiclass Classification"></a>多类别分类 Multiclass Classification</h2><p>对每一个类别，创建一个新“伪”的训练集，拟合一个合适的分类器。<br>就是将一个类标为正类，其他都为负类，得到一系列模型记为$h_{\theta}^{(i)}(x) = p(y = i | x;\theta), i = 1,2,…,k$<br>最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。</p>
<hr>
<h1 id="Lecture-7"><a href="#Lecture-7" class="headerlink" title="Lecture 7"></a>Lecture 7</h1><p>本章主要介绍<strong>正则化方法(Regularization)</strong>，它可以改善或者减少过度拟合问题。</p>
<h2 id="过拟合问题-Overfitting"><a href="#过拟合问题-Overfitting" class="headerlink" title="过拟合问题 Overfitting"></a>过拟合问题 Overfitting</h2><ul>
<li>丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如<strong>PCA</strong>）</li>
<li>正则化。 保留所有的特征，但是减少参数的大小（<strong>magnitude</strong>）。</li>
</ul>
<h2 id="代价函数-1"><a href="#代价函数-1" class="headerlink" title="代价函数"></a>代价函数</h2><p>某些项导致了过拟合的产生，所以如果我们能让这些项系数接近于0的话，我们就能很好的拟合了。<br>所以我们要做的就是在一定程度上减小这些参数$\theta$ 的值，这就是正则化的基本方法。我们要做的便是修改代价函数，为$\theta$设置一点惩罚。<br>假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：</p>
<script type="math/tex; mode=display">J(\theta) = \dfrac{1}{2m}\left[\sum\limits_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^{2} + \lambda \sum\limits_{j=1}^{n} \theta_{j}^{2}\right]</script><p>其中$\lambda$又称为正则化参数（<strong>Regularization Parameter</strong>）。 注：根据惯例，我们不对$\theta_{0}$ 进行惩罚。</p>
<h2 id="正则化线性回归-Regularized-Linear-Regression"><a href="#正则化线性回归-Regularized-Linear-Regression" class="headerlink" title="正则化线性回归 Regularized Linear Regression"></a>正则化线性回归 Regularized Linear Regression</h2><p><strong>repeat until convergence{</strong></p>
<script type="math/tex; mode=display">\theta_{0}:=\theta_{0}-\alpha \dfrac{1}{m} \sum_{i=1}^{m}((h_{\theta}(x^{(i)}) - y^{(i)})</script><script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \left[\dfrac{1}{m} \sum_{i=1}^{m}((h_{\theta}(x^{(i)}) - y^{(i)}) * x_{j}^{(i)}) + \dfrac{\lambda}{m}\theta_{j}\right] \quad (for~j=1,2,...,n)</script><p><strong>}</strong><br>对$j=1,2,…,n$时的更新式子整理得：</p>
<script type="math/tex; mode=display">\theta_{j}:=\theta_{j}(1 - \alpha \dfrac{\lambda}{m})-\alpha \dfrac{1}{m} \sum_{i=1}^{m}((h_{\theta}(x^{(i)}) - y^{(i)}) * x_{j}^{(i)})</script><p>可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令$\theta$值减少了一个额外的值。</p>
<p>我们同样也可以利用正规方程来求解正则化线性回归模型：</p>
<script type="math/tex; mode=display">\theta=\left(X^{T} X+\lambda\left[\begin{array}{cccc}
0 & & & \\
& 1 & & \\
& & 1 & & \\
& & & \ddots & \\
& & & & 1
\end{array}\right]\right)^{-1} X^{T} y</script><p>矩阵尺寸为 $(n+1)*(n+1)$。</p>
<h2 id="正则化的逻辑回归模型-Regularized-Logistic-Regression"><a href="#正则化的逻辑回归模型-Regularized-Logistic-Regression" class="headerlink" title="正则化的逻辑回归模型 Regularized Logistic Regression"></a>正则化的逻辑回归模型 Regularized Logistic Regression</h2><script type="math/tex; mode=display">J(\theta) = -\dfrac{1}{m}\sum\limits_{i=1}^{n} [y \times \log{(h_{\theta}(x))} + (1 - y) \times \log{(1- h_{\theta}(x))}] + \dfrac{\lambda}{2m} \sum\limits_{j=1}^{n}\theta_{j}^{2}</script><figure class="highlight python"><figcaption><span>python代码</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costReg</span>(<span class="params">theta, X, y, learningRate</span>):</span></span><br><span class="line">    theta = np.matrix(theta)</span><br><span class="line">    X = np.matrix(X)</span><br><span class="line">    y = np.matrix(y)</span><br><span class="line">    first = np.multiply(-y, np.log(sigmoid(X*theta.T)))</span><br><span class="line">    second = np.multiply((<span class="number">1</span> - y), np.log(<span class="number">1</span> - sigmoid(X*theta.T)))</span><br><span class="line">    reg = (learningRate / (<span class="number">2</span> * <span class="built_in">len</span>(X))* np.<span class="built_in">sum</span>(np.power(theta[:,<span class="number">1</span>:theta.shape[<span class="number">1</span>]],<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(first - second) / (<span class="built_in">len</span>(X)) + reg</span><br></pre></td></tr></table></figure>
<p>梯度下降：<br><strong>repeat until convergence{</strong></p>
<script type="math/tex; mode=display">\theta_{0}:=\theta_{0}-\alpha \dfrac{1}{m} \sum_{i=1}^{m}((h_{\theta}(x^{(i)}) - y^{(i)})</script><script type="math/tex; mode=display">\theta_{j}:=\theta_{j}-\alpha \left[\dfrac{1}{m} \sum_{i=1}^{m}((h_{\theta}(x^{(i)}) - y^{(i)}) * x_{j}^{(i)}) + \dfrac{\lambda}{m}\theta_{j}\right] \quad (for~j=1,2,...,n)</script><p><strong>}</strong><br>注：看上去同线性回归一样，但是知道 $h_\theta ( x )=g( \theta^{T} X )$，所以与线性回归不同。</p>
<hr>
<h1 id="Lecture-8"><a href="#Lecture-8" class="headerlink" title="Lecture 8"></a>Lecture 8</h1><p>本章主要对<strong>神经网络Neural Network</strong>进行表述。</p>
<h2 id="非线性假设-Non-linear-Hypotheses"><a href="#非线性假设-Non-linear-Hypotheses" class="headerlink" title="非线性假设 Non-linear Hypotheses"></a>非线性假设 Non-linear Hypotheses</h2><p>当特征太多时，计算的负荷会非常大。<br>普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们需要神经网络。</p>
<h2 id="神经元和大脑-Neurons-and-the-Brain"><a href="#神经元和大脑-Neurons-and-the-Brain" class="headerlink" title="神经元和大脑 Neurons and the Brain"></a>神经元和大脑 Neurons and the Brain</h2><p><del>生物知识</del>。人工智能的梦想就是：有一天能制造出真正的智能机器。</p>
<h2 id="模型表示-Model-Representation-1"><a href="#模型表示-Model-Representation-1" class="headerlink" title="模型表示 Model Representation"></a>模型表示 Model Representation</h2><p>神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元，<strong>activation unit</strong>）采纳一些特征作为输出，并且根据本身的模型提供一个输出。</p>
<p>下图是一个以逻辑回归模型作为自身学习模型的神经元示例，在神经网络中，参数又可被成为权重（<strong>weight</strong>）。</p>
<p><img data-src="neutral.png" alt="Neuron model"></p>
<p>我们设计出了类似于神经元的神经网络，效果如下：</p>
<p><img data-src="network.png" alt="Neural Network"></p>
<p>其中$x_1$, $x_2$, $x_3$是输入单元（<strong>input units</strong>），我们将原始数据输入给它们。</p>
<p>$a_1$, $a_2$, $a_3$是中间单元，它们负责将数据进行处理，然后呈递到下一层。</p>
<p>最后是输出单元，它负责计算$h_\theta ( x )$。</p>
<p>神经网络模型是许多逻辑单元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。上图为一个三层神经网络，第一层称为输入层（<strong>Input Layer</strong>），最后一层称为输出层（<strong>Output Layer</strong>），中间一层成为隐藏层（<strong>Hidden Layers</strong>）。我们为每一层都增加一个偏差单位（<strong>bias unit</strong>）。</p>
<p>下面引入一些标记法来帮助描述模型：</p>
<ul>
<li>$a_{i}^{(j)}$：第$j$ 层的第 $i$ 个激活单元。</li>
<li>$\theta^{(j)}$：从第 $j$ 层映射到第$ j+1$ 层时的权重的矩阵。其尺寸为：以第 $j+1$层的激活单元数量为行数，以第 $j$ 层的激活单元数加一为列数的矩阵。</li>
</ul>
<p>对于上图所示的模型，激活单元和输出分别表达为：<br>$a_{1}^{(2)} = g(\Theta_{10}^{(1)}x_{0} + \Theta_{11}^{(1)}x_{1} + \Theta_{12}^{(1)}x_{2} +  \Theta_{13}^{(1)}x_{3})$</p>
<p>$a_{2}^{(2)} = g(\Theta_{20}^{(1)}x_{0} + \Theta_{21}^{(1)}x_{1} + \Theta_{22}^{(1)}x_{2} +  \Theta_{23}^{(1)}x_{3})$</p>
<p>$a_{3}^{(2)} = g(\Theta_{30}^{(1)}x_{0} + \Theta_{31}^{(1)}x_{1} + \Theta_{32}^{(1)}x_{2} +  \Theta_{33}^{(1)}x_{3})$</p>
<p>$h_{\Theta}(x) = g(\Theta_{10}^{(2)}a_{0} + \Theta_{11}^{(2)}a_{1} + \Theta_{12}^{(2)}a_{2} +  \Theta_{13}^{(2)}a_{3})$</p>
<p>上面进行的讨论中只是将特征矩阵中的一列（一个训练实例）喂给了神经网络，我们需要将整个训练集都喂给我们的神经网络算法来学习模型。</p>
<p>我们可以知道：每一个$a$都是由上一层所有的$x$和每一个$x$所对应的决定的。<br>（我们把这样从左到右的算法称为前向传播算法( <strong>FORWARD PROPAGATION</strong> )）</p>
<p>把$x$, $\theta$, $a$ 分别用矩阵表示，我们可以得到$\Theta \cdot X=a$ 。</p>
<p>我们可以把$a_0, a_1, a_2, a_3$看成更为高级的特征值，也就是$x_0, x_1, x_2, x_3$的进化体，并且它们是由 $x$与$\theta$决定的，因为是梯度下降的，所以$a$是变化的，并且变得越来越厉害，所以这些更高级的特征值远比仅仅将 $x$次方厉害，也能更好的预测新数据。</p>
<p>这就是神经网络相比于逻辑回归和线性回归的优势。</p>
<h2 id="多类分类"><a href="#多类分类" class="headerlink" title="多类分类"></a>多类分类</h2><p>当我们有不止两种分类时，输出层多个神经元来表示多类。</p>
<p>也就是每一个数据在输出层都会出$\{ a_{1}, a_{2}, …, a_{k} \}$，且$a_{i}$中仅有一个为1，表示当前类。</p>
<hr>
<h1 id="Lecture-9"><a href="#Lecture-9" class="headerlink" title="Lecture 9"></a>Lecture 9</h1><p>本章主要是神经网络的学习训练，其代价函数，检验以及初始化相关的介绍。</p>
<h2 id="代价函数-2"><a href="#代价函数-2" class="headerlink" title="代价函数"></a>代价函数</h2><p>首先引入一些标记：<br>假设神经网络的训练样本有 $m$ 个，每个包含一组输入 $x$ 和一组输出信号 $y$，$L$ 表示神经网络层数，$S_I$ 表示每层的<strong>neuron</strong>个数($S_l$ 表示输出层神经元个数)，$S_L$ 代表最后一层中处理单元的个数。<br>二类分类： $S_L = 0, y = 0 ~ or ~ 1$；<br>$K$类分类： $S_L = K, y_i = 1$ 表示分到第$i$类（$k &gt;2$）。</p>
<p>逻辑回归中的代价函数为：</p>
<script type="math/tex; mode=display">J(\theta) = -\dfrac{1}{m}\sum\limits_{i=1}^{n} [y \times \log{(h_{\theta}(x))} + (1 - y) \times \log{(1- h_{\theta}(x))}]</script><p>在逻辑回归中，只有一个输出变量，又称标量（<strong>scalar</strong>），也只有一个因变量$y$，但是在神经网络中，可以有很多输出变量，我们的$h_\theta(x)$是一个维度为$K$的向量，并且我们训练集中的因变量也是同样维度的一个向量，因此我们的代价函数会比逻辑回归更加复杂一些，为：<br>$h_{\theta} (x) \in \mathbb{R}^K$，$(h_{\theta} (x))_i = i$-th output</p>
<p>$J(\theta) = - \dfrac{1}{m} \left[ \sum\limits_{i = 1}^{m} \sum\limits_{k=1}^{K} y_k^{(i)} \log{(h_{\theta}(x^{(i)})_k)} + (1 - y_k^{(i)})\log{(1 - (h_{\theta} (x^{(i)}))_k)} + \dfrac{\lambda}{2m} \sum\limits_{l = 1}^{L-1} \sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{l+1}} \right]$</p>
<p>这个看起来复杂很多的代价函数背后的思想还是一样的，我们希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出$K$个预测，基本上我们可以利用循环，对每一行特征都预测$K$个不同结果，然后在利用循环在$K$个预测中选择可能性最高的一个，将其与$y$中的实际数据进行比较。</p>
<p>$h_\theta(x)$与真实值之间的距离为每个样本-每个类输出的加和，对参数进行<strong>regularization</strong>的<strong>bias</strong>项处理所有参数的平方和。</p>
<h2 id="反向传播算法-Backpropagation-Algorithm"><a href="#反向传播算法-Backpropagation-Algorithm" class="headerlink" title="反向传播算法 Backpropagation Algorithm"></a>反向传播算法 Backpropagation Algorithm</h2><p>为了计算代价函数的偏导数$\dfrac{\partial}{\partial\Theta^{(l)}_{ij}}J\left(\Theta\right)$，我们需要采用一种反向传播算法。也就是首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。</p>
<p>e.g.<br>$K=4, S_L = 4,L=4$</p>
<p>从最后一层的误差开始计算，误差是激活单元的预测（$a^{(4)}$）与实际值（$y^k$）之间的误差，（$k=1:k$）。</p>
<p>我们用$\delta$来表示误差，则：$\delta^{(4)}=a^{(4)}-y$，然后利用这个误差值来计算前一层误差，则：$\delta^{(3)} = \left( \Theta^{(3)} \right)^T \delta^{(4)} * g’\left( z^{3} \right)$。</p>
<p>其中 $g’(z^{(3)})$是 $S$ 形函数的导数，$g’(z^{(3)})=a^{(3)}\ast(1-a^{(3)})$。而$(θ^{(3)})^{T}\delta^{(4)}$则是权重导致的误差的和。</p>
<p>我们有了所有的误差的表达式后，便可以计算代价函数的偏导数了，假设$λ=0$，即我们不做任何正则化处理时有：<br>$\dfrac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=a_{j}^{(l)} \delta_{i}^{l+1}$</p>
<p>上面式子中上下标的含义：</p>
<ul>
<li>$l$：目前计算的是第几层。</li>
<li>$j$：目前计算的激活单元的下标，也将是下一层的第$j$个输入变量的下标。</li>
<li>$i$：下一层中误差单元的下标，是受到权重矩阵中第$i$行影响的下一层中的误差单元的下标。</li>
</ul>
<p>误差单元也是一个矩阵，我们用$\Delta^{(l)}_{ij}$来表示这个误差矩阵。第 $l$  层的第 $i$ 个激活单元受到第 $j$ 个参数影响而导致的误差。</p>
<p>算法表示为：<br><img data-src="backpropagation.jpg" alt="backporpagation algorithm"></p>
<p>即首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。</p>
<p>在求出了$\Delta_{ij}^{(l)}$之后，我们便可以计算代价函数的偏导数了，计算方法如下：<br>$D_{ij}^{(l)} :=\dfrac{1}{m}\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}$              ${if}\; j \neq  0$</p>
<p>$D_{ij}^{(l)} :=\dfrac{1}{m}\Delta_{ij}^{(l)}$                             ${if}\; j = 0$</p>
<ul>
<li>我们可以想象 $\delta^{(l)}_{j}$ 为函数求导时迈出的那一丁点微分，所以更准确的说 $\delta^{(l)}_{j}=\frac{\partial}{\partial z^{(l)}_{j}}cost(i)$</li>
</ul>
<h2 id="梯度检验-Gradient-Checking"><a href="#梯度检验-Gradient-Checking" class="headerlink" title="梯度检验 Gradient Checking"></a>梯度检验 Gradient Checking</h2><p>当我们对一个较为复杂的模型（例如神经网络）使用梯度下降算法时，可能会存在一些不容易察觉的错误，意味着，虽然代价看上去在不断减小，但最终的结果可能并不是最优解。<br>为了避免这样的问题，我们采取一种叫做梯度的数值检验（<strong>Numerical Gradient Checking</strong>）方法。这种方法的思想是通过估计梯度值来检验我们计算的导数值是否真的是我们要求的。<br>（就模拟求导看看我们用$\delta$算出来的对不对）</p>
<p>对$\theta_1$进行检验：<br> $\dfrac{\partial}{\partial\theta_1}=\dfrac{J\left(\theta_1+\varepsilon_1,\theta_2,\theta_3…\theta_n \right)-J \left( \theta_1-\varepsilon_1,\theta_2,\theta_3…\theta_n \right)}{2\varepsilon}$</p>
<p> 根据上面的算法，计算出的偏导数存储在矩阵 $D_{ij}^{(l)}$ 中。检验时，我们要将该矩阵展开成为向量，同时我们也将 $\theta$ 矩阵展开为向量，我们针对每一个 $\theta$ 都计算一个近似的梯度值，将这些值存储于一个近似梯度矩阵中，最终将得出的这个矩阵同 $D_{ij}^{(l)}$ 进行比较。</p>
<h2 id="随机初始化-Random-Initialization"><a href="#随机初始化-Random-Initialization" class="headerlink" title="随机初始化 Random Initialization"></a>随机初始化 Random Initialization</h2><p> 任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非0的数，结果也是一样的。</p>
<ul>
<li>我们通常初始参数为正负ε之间的随机值。</li>
</ul>
<h2 id="综合起来-Put-It-Together"><a href="#综合起来-Put-It-Together" class="headerlink" title="综合起来 Put It Together"></a>综合起来 Put It Together</h2><p>小结一下使用神经网络时的步骤：</p>
<ul>
<li><p>网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。</p>
<ul>
<li>第一层的单元数即我们训练集的特征数量。</li>
<li>最后一层的单元数是我们训练集的结果的类的数量。</li>
<li>如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。</li>
<li>我们真正要决定的是隐藏层的层数和每个中间层的单元数。</li>
</ul>
</li>
<li><p>训练神经网络</p>
<ol>
<li>参数的随机初始化。</li>
<li>利用正向传播方法计算所有的$h_{\theta}(x)$。</li>
<li>编写计算代价函数 $J$ 的代码。</li>
<li>利用反向传播方法计算所有偏导数。</li>
<li>利用数值检验方法检验这些偏导数。</li>
<li>使用优化算法来最小化代价函数。</li>
</ol>
</li>
</ul>
<hr>
<h1 id="Lecture-10"><a href="#Lecture-10" class="headerlink" title="Lecture 10"></a>Lecture 10</h1><p>本章主要是吴恩达对于机器学习的一些技巧经验建议，包括如何调参，如何判断欠拟合或过拟合等等。<strong>本章非常重要</strong>，掌握了调参技巧可以节约非常多的<del>生命</del>时间。</p>
<h2 id="下一步做什么-Decide-What-to-Try-Next"><a href="#下一步做什么-Decide-What-to-Try-Next" class="headerlink" title="下一步做什么 Decide What to Try Next"></a>下一步做什么 Decide What to Try Next</h2><p>当我们运用训练好了的模型来预测未知数据的时候发现有较大的误差，我们下一步可以做什么？</p>
<ol>
<li>获得更多的训练样本——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。</li>
<li>尝试减少特征的数量</li>
<li>尝试获得更多的特征</li>
<li>尝试增加多项式特征</li>
<li>尝试减少正则化程度$\lambda$</li>
<li>尝试增加正则化程度$\lambda$</li>
</ol>
<h2 id="评估一个假设-Evaluating-a-hypothesis"><a href="#评估一个假设-Evaluating-a-hypothesis" class="headerlink" title="评估一个假设 Evaluating a hypothesis"></a>评估一个假设 Evaluating a hypothesis</h2><p>当我们确定学习算法的参数的时候，我们考虑的是选择参量来使训练误差最小化，有人认为得到一个非常小的训练误差一定是一件好事，但我们已经知道，仅仅是因为这个假设具有很小的训练误差，并不能说明它就一定是一个好的假设函数。而且我们也学习了过拟合假设函数的例子，所以这推广到新的训练集上是不适用的。</p>
<p>所以我们时常需要判断一个假设函数是否过拟合或欠拟合。</p>
<p>我们可以对假设函数$h(x)$进行画图，然后观察图形趋势，但对于有很多特征变量的问题，想要通过画出假设函数来进行观察，就会变得很难甚至是不可能实现。</p>
<p>另一种方法检验是否过拟合，将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集，如果数据有某种规律，要打乱再分。</p>
<p>测试集是用来评估训练集训练出的学习模型的参数，对测试集运用该模型，计算误差：</p>
<ul>
<li>线性回归模型，计算测试集数据的代价函数 $J$</li>
<li>逻辑回归模型，除了代价函数 $J_{t e s t}(\theta)=-\dfrac{1}{m_{\text {test }}} \sum\limits_{i=1}^{m_{test }} y_{test}^{(i)} \log h_{\theta}(x_{test }^{(i)})+(1-y_{test }^{(i)}) \log h_{\theta}(x_{test }^{(i)})$，我们还可以计算错误分类的比率: $err(h_{\theta}(x), y)=\left\{\begin{array}{c}<br>1 \text { ,if } h(x) \geq 0.5 \text { and } y=0, \text { or if } h(x)&lt;0.5 \text { and } y=1 \\<br>0 \text { ,otherwise }<br>\end{array}\right.$ ，再求平均。</li>
</ul>
<h2 id="模型选择和交叉验证集-Model-Selection-and-Train-Validation-Test-Sets"><a href="#模型选择和交叉验证集-Model-Selection-and-Train-Validation-Test-Sets" class="headerlink" title="模型选择和交叉验证集 Model Selection and Train_Validation_Test Sets"></a>模型选择和交叉验证集 Model Selection and Train_Validation_Test Sets</h2><p>我们需要使用交叉验证集来帮助选择模型（多项式次数）。</p>
<p>使用60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用20%的数据作为测试集。</p>
<p>模型选择的方法：</p>
<ol>
<li>用训练集训练出10个模型</li>
<li>用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）</li>
<li>选取代价函数值最小的模型</li>
<li>用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值）</li>
</ol>
<h2 id="诊断偏差和方差-Diagnosing-Bias-vs-Variance"><a href="#诊断偏差和方差-Diagnosing-Bias-vs-Variance" class="headerlink" title="诊断偏差和方差 Diagnosing Bias vs. Variance"></a>诊断偏差和方差 Diagnosing Bias vs. Variance</h2><p>当运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。</p>
<p>我们通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析：</p>
<p><img data-src="train_cv_with_degree.png" alt title="训练集和交叉验证集的代价函数和次数的关系"></p>
<p>对于训练集，当 $d$ 较小时，模型拟合程度更低，误差较大；随着 $d$ 的增长，拟合程度提高，误差减小。</p>
<p>对于交叉验证集，当 $d$ 较小时，模型拟合程度低，误差较大；但是随着 $d$ 的增长，误差呈现先减小后增大的趋势，转折点是我们的模型开始过拟合训练数据集的时候。</p>
<ul>
<li>训练集误差和交叉验证集误差近似时：偏差/欠拟合。</li>
<li>交叉验证集误差远大于训练集误差时：方差/过拟合。</li>
</ul>
<h2 id="正则化和偏差-方差-Regularization-and-Bias-Variance"><a href="#正则化和偏差-方差-Regularization-and-Bias-Variance" class="headerlink" title="正则化和偏差/方差 Regularization and Bias_Variance"></a>正则化和偏差/方差 Regularization and Bias_Variance</h2><p>在我们在训练模型的过程中，一般会使用一些正则化方法来防止过拟合。但是我们可能会正则化的程度太高或太小了，即我们在选择λ的值时也需要思考与刚才选择多项式模型次数类似的问题。</p>
<p>我们选择一系列的想要测试的 $\lambda$ 值，通常是 0-10之间的呈现2倍关系的值（如：$0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10$共12个）。 我们同样把数据分为训练集、交叉验证集和测试集。</p>
<p>选择 $\lambda$ 的方法为：</p>
<ol>
<li>使用训练集训练出12个不同程度正则化的模型</li>
<li>用12个模型分别对交叉验证集计算的出交叉验证误差</li>
<li>选择得出交叉验证误差<strong>最小</strong>的模型</li>
<li>运用步骤3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上：<ul>
<li>注意到这里训练的时候加正则项，评估的时候不要正则项。</li>
</ul>
</li>
</ol>
<p><img data-src="lambda.png" alt title="训练集和交叉验证集的代价函数和lambda的关系"></p>
<p>• 当 $\lambda$ 较小时，训练集误差较小（过拟合）而交叉验证集误差较大</p>
<p>• 随着 $\lambda$ 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加</p>
<h2 id="学习曲线-Learning-Curves"><a href="#学习曲线-Learning-Curves" class="headerlink" title="学习曲线 Learning Curves"></a>学习曲线 Learning Curves</h2><p>学习曲线是学习算法的一个很好的<strong>合理检验</strong>（<strong>sanity check</strong>）。学习曲线是将训练集误差和交叉验证集误差作为训练集样本数量（$m$）的函数绘制的图表。</p>
<p>当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。</p>
<ul>
<li>高偏差/欠拟合时，交叉验证集和训练集的曲线会很接近，无论训练集有多么大误差都不会有太大改观：<img data-src="high_bias.png" alt title="高偏差的学习曲线"></li>
</ul>
<ul>
<li><p>高方差/过拟合时，训练集误差很小，交叉验证集误差远大于训练集误差，往训练集增加更多数据可以提高模型的效果：</p>
<p><img data-src="high_variance.png" alt title="高方差的学习曲线"></p>
</li>
</ul>
<h2 id="决定下一步做什么-Deciding-What-to-Do-Next-Revisite"><a href="#决定下一步做什么-Deciding-What-to-Do-Next-Revisite" class="headerlink" title="决定下一步做什么 Deciding What to Do Next Revisite"></a>决定下一步做什么 Deciding What to Do Next Revisite</h2><p>回顾 1.1 中提出的六种可选的下一步，让我们来看一看我们在什么情况下应该怎样选择：</p>
<ol>
<li><p>获得更多的训练样本——解决高方差</p>
</li>
<li><p>尝试减少特征的数量——解决高方差</p>
</li>
<li><p>尝试获得更多的特征——解决高偏差</p>
</li>
<li><p>尝试增加多项式特征——解决高偏差</p>
</li>
<li><p>尝试减少正则化程度λ——解决高偏差</p>
</li>
<li><p>尝试增加正则化程度λ——解决高方差</p>
</li>
</ol>
<p>神经网络的方差和偏差：<br><img data-src="/neutral_overfitting.png" alt title="神经网路的过拟合"></p>
<p>使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。</p>
<ul>
<li>通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。</li>
</ul>
<p>对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络，<br>然后选择交叉验证集代价最小的神经网络。</p>
<hr>
<h1 id="Lecture-11"><a href="#Lecture-11" class="headerlink" title="Lecture 11"></a>Lecture 11</h1><p>本章主要是讲述机器学习系统的设计技巧，对于误差的分析。</p>
<h2 id="误差分析-Error-Analysis"><a href="#误差分析-Error-Analysis" class="headerlink" title="误差分析 Error Analysis"></a>误差分析 Error Analysis</h2><blockquote>
<p>构造机器学习应用程序，最好的实践方法不是建立一个非常复杂的系统，拥有多么复杂的变量；而是构建一个简单的算法，这样你可以很快地实现它。</p>
</blockquote>
<p>构建一个学习算法的推荐方法为：</p>
<ol>
<li>从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法。</li>
<li>绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择。（参考 Lecture 10）</li>
<li>进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的样本，看看这些样本是否有某种系统化的趋势。</li>
</ol>
<p>误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要尝试不同的模型，然后进行比较，在模型比较时，用数值来判断哪一个模型更好更有效，通常我们是看交叉验证集的误差。</p>
<h2 id="类偏斜的误差度量-Error-Metrics-for-Skewed-Classes"><a href="#类偏斜的误差度量-Error-Metrics-for-Skewed-Classes" class="headerlink" title="类偏斜的误差度量 Error Metrics for Skewed Classes"></a>类偏斜的误差度量 Error Metrics for Skewed Classes</h2><p>误差分析就是用某个实数来评估你的学习算法，所以我们要选择合适的误差度量值，关注类偏斜（Skewed Classes）的问题。</p>
<p>类偏斜情况表现为我们的训练集中有非常多的同一种类的样本，只有很少或没有其他类的样本。</p>
<p>通过<strong>查准率</strong>（<strong>Precision</strong>）和<strong>查全率</strong>（<strong>Recall</strong>） 我们将算法预测的结果分成四种情况：</p>
<ol>
<li><p><strong>正确肯定</strong>（<strong>True Positive,TP</strong>）：预测为真，实际为真</p>
</li>
<li><p><strong>正确否定</strong>（<strong>True Negative,TN</strong>）：预测为假，实际为假</p>
</li>
<li><p><strong>错误肯定</strong>（<strong>False Positive,FP</strong>）：预测为真，实际为假</p>
</li>
<li><p><strong>错误否定</strong>（<strong>False Negative,FN</strong>）：预测为假，实际为真</p>
</li>
</ol>
<p>查准率 $P = \dfrac{TP}{TP + FP}$，就是所有预测为真的中预测对了的。</p>
<p>查全率 $R = \dfrac{TP}{TP + FN}$，就是所有真的是真的中猜对了多少。</p>
<h2 id="查准率和查全率之间的权衡-Trading-Off-Precision-and-Recall"><a href="#查准率和查全率之间的权衡-Trading-Off-Precision-and-Recall" class="headerlink" title="查准率和查全率之间的权衡 Trading Off Precision and Recall"></a>查准率和查全率之间的权衡 Trading Off Precision and Recall</h2><p>我们一般采用<strong>F1值</strong> $F_1 = 2\dfrac{PR}{P + R}$</p>
<h2 id="机器学习的数据-Data-For-Machine-Learning"><a href="#机器学习的数据-Data-For-Machine-Learning" class="headerlink" title="机器学习的数据 Data For Machine Learning"></a>机器学习的数据 Data For Machine Learning</h2><blockquote>
<p>特征值有足够的信息量，且我们有一类很好的函数，这是为什么能保证低误差的关键所在。它有大量的训练数据集，这能保证得到更多的方差值，因此这给我们提出了一些可能的条件，如果你有大量的数据，而且你训练了一种带有很多参数的学习算法，那么这将会是一个很好的方式，来提供一个高性能的学习算法。</p>
</blockquote>
<p><em>按我的理解就是，我们有一个参数很多的模型、一个非常好的学习算法，使得这个模型可以拟合得非常好（低偏差）。然后我们有一个很大很大的数据集，这使得模型不太可能过拟合（高方差），所以测试集误差和训练集误差会差不多，就会很小。</em></p>
<hr>
<h1 id="Lecture-12"><a href="#Lecture-12" class="headerlink" title="Lecture 12"></a>Lecture 12</h1><p>本章主要讲述支持向量机（Support Vector Machines），它在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。</p>
<h2 id="优化目标-Optimization-Objective"><a href="#优化目标-Optimization-Objective" class="headerlink" title="优化目标 Optimization Objective"></a>优化目标 Optimization Objective</h2><p>这章将逻辑回归的公式变了一下，一个是将 $\log$ 表示的代价函数换成两个线性的，一个是将正则化 $\lambda$ 参数移到了前面 $C$。</p>
<script type="math/tex; mode=display">
\min_{\theta} \sum\limits_{i = 1}^{m} \left[C y^{(i)}cost_{1}(\theta^T x^{(i)}) + (1 - y^{(i)})cost_{0}(\theta^Tx^{(i)}) + \dfrac{1}{2}\sum\limits_{i=1}^{n}\theta^2 \right]</script><p>他说这个就叫支持向量机（？？）</p>
<h2 id="大边界-Large-Margin"><a href="#大边界-Large-Margin" class="headerlink" title="大边界 Large Margin"></a>大边界 Large Margin</h2><p>支持向量机是个大间距分类器，这就是它具有鲁棒性的原因。总之SVM努力将正样本和负样本用最大的间距分开。</p>
<ul>
<li><p>$C$ 较大时，相当于 $\lambda$ 较小，可能会导致过拟合，高方差，容易受异常点影响。（其实只有在 $C$ 大的时候会导致大间距）</p>
</li>
<li><p>$C$ 较小时，相当于$\lambda$较大，可能会导致低拟合，高偏差。</p>
</li>
</ul>
<h2 id="背后的数学-Mathematics-Behind"><a href="#背后的数学-Mathematics-Behind" class="headerlink" title="背后的数学 Mathematics Behind"></a>背后的数学 Mathematics Behind</h2><p>从数学解释了为什么SVM会产生大边界分类。</p>
<h2 id="核函数-Kernels"><a href="#核函数-Kernels" class="headerlink" title="核函数 Kernels"></a>核函数 Kernels</h2><p>我们的模型 $h_\theta(x) = \theta_0 + \theta_1 f_1 + \theta_2 f_2…$，我们可以考虑用更好的方法来构造特征 $f_1, f_2, f_3…$，而不仅仅是用 $x_1, x_2, x_3,…$ 的组合。</p>
<p>给定一个训练样本$x$，我们利用$x$的各个特征与我们预先选定的<strong>地标</strong>(<strong>landmarks</strong>)$l^{(1)},l^{(2)},l^{(3)}$的近似程度来选取新的特征$f_1,f_2,f_3$。</p>
<p>例如：$f_1 = similarity(x, l^{(1)}) = exp(\dfrac{| x - l^{(1)}|^2}{2\sigma^2})$。</p>
<p>这个例子的函数是一个<strong>高斯核函数</strong>，很常用，度量样本 $x$ 和地标 $l$ 的近似度。</p>
<p>关于如何选择地标。</p>
<p>我们通常是根据训练集的数量选择地标的数量，即如果训练集中有$m$个样本，则我们选取$m$个地标，并且令:$l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},…..,l^{(m)}=x^{(m)}$。这样做的好处在于：现在我们得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的。</p>
<p>修改模型为：</p>
<script type="math/tex; mode=display">
\min_{\theta} \left[ C \sum\limits_{i = 1}^{m} y^{(i)}cost_{1}(\theta^T f^{(i)}) + (1 - y^{(i)})cost_{0}(\theta^Tf^{(i)}) + \dfrac{1}{2}\sum\limits_{i=1}^{n=m}\theta^2 \right]</script><p>正则化项 $\sum \theta^2 = \theta^T \theta$ 调整为 $\theta^T M \theta$，其中 $M$ 是根据核函数选择的矩阵。</p>
<p><em>理论上讲，我们也可以在逻辑回归中使用核函数，但是上面使用 $M$来简化计算的方法不适用与逻辑回归，因此计算将非常耗费时间。</em></p>
<p>另外，支持向量机也可以不使用核函数，不使用核函数又称为<strong>线性核函数</strong>(<strong>linear kernel</strong>)，当我们不采用非常复杂的函数，或者我们的训练集特征非常多而样本非常少的时候，可以采用这种不带核函数的支持向量机。</p>
<p>关于 $ \sigma$:</p>
<ul>
<li><p>$\sigma$ 较大时，可能会导致低方差，高偏差；</p>
</li>
<li><p>$\sigma$ 较小时，可能会导致低偏差，高方差。</p>
</li>
</ul>
<h2 id="使用支持向量机-Using-An-SVM"><a href="#使用支持向量机-Using-An-SVM" class="headerlink" title="使用支持向量机 Using An SVM"></a>使用支持向量机 Using An SVM</h2><p>在高斯核函数之外我们还有其他一些选择，如多项式核函数（<strong>Polynomial Kerne</strong>l），字符串核函数（<strong>String kernel</strong>），卡方核函数（ <strong>chi-square kernel</strong>），直方图交集核函数（<strong>histogram intersection kernel</strong>）等。</p>
<p>这些核函数的目标也都是根据训练集和地标之间的距离来构建新特征，这些核函数需要满足Mercer’s定理，才能被支持向量机的优化软件正确处理。</p>
<p>对于多分类问题，大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。</p>
<p>我们要做的就是调参<span class="github-emoji" alias="joy" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f602.png?v8">&#x1f602;</span>，对于 $C$ 和核函数的内参数。</p>
<p>从逻辑回归模型，我们得到了支持向量机模型，在两者之间，我们应该如何选择呢？</p>
<p><strong>下面是一些普遍使用的准则：</strong></p>
<p>$n$为特征数，$m$为训练样本数。</p>
<ol>
<li>如果相较于$m$而言，$n$要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。</li>
<li>如果$n$较小，而且$m$大小中等，例如$n$在 1-1000 之间，而$m$在10-10000之间，使用高斯核函数的支持向量机。</li>
<li>如果$n$较小，而$m$较大，例如$n$在1-1000之间，而$m$大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。</li>
</ol>
<p>值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。</p>
<hr>
<h1 id="Lecture-13"><a href="#Lecture-13" class="headerlink" title="Lecture 13"></a>Lecture 13</h1><p>本章开始进入无监督学习，首先介绍的是<strong>聚类（clustering）</strong>，k-means算法。</p>
<h2 id="K-均值算法-K-Means-Algorithm"><a href="#K-均值算法-K-Means-Algorithm" class="headerlink" title="K-均值算法 K-Means Algorithm"></a>K-均值算法 K-Means Algorithm</h2><p><strong>K-均值</strong>是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组，它是一个迭代算法。</p>
<p>算法运行过程为：</p>
<ol>
<li>选择$K$个随机的点，称为<strong>聚类中心</strong>（<strong>cluster centroids</strong>）。</li>
<li>对于数据集中的每一个数据，按照距离$K$个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。</li>
<li>计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。</li>
<li>重复步骤2-3直至中心点不再变化。</li>
</ol>
<p>用 $\mu_1, \mu_2, … , \mu_k$ 来表示聚类中心，用$c^{(1)}$,$c^{(2)}$,…,$c^{(m)}$来存储与第$i$个实例数据最近的聚类中心的索引，<strong>K-均值</strong>算法的伪代码如下：</p>
<p><strong>Repeat{</strong></p>
<p>$\qquad$for $i = 1$ to $m$:</p>
<p>$\qquad \qquad c^{(i)} := $ index (from $1$ to $K$) of cluster centroid closest to $x^{(i)}$;</p>
<p>$\qquad$ for $k = 1$ to $K$:</p>
<p>$\qquad \qquad \mu_k := $ average (mean) of points assigned to cluster $k$;</p>
<p><strong>}</strong></p>
<h2 id="优化目标-Optimization-Objective-1"><a href="#优化目标-Optimization-Objective-1" class="headerlink" title="优化目标 Optimization Objective"></a>优化目标 Optimization Objective</h2><p>对于聚类中心的初始化，我们可以：</p>
<ol>
<li>我们应该选择$K&lt;m$，即聚类中心点的个数要小于所有训练集实例的数量。</li>
<li>随机选择$K$个训练实例，然后令$K$个聚类中心分别与这$K$个训练实例相等。</li>
</ol>
<p>问题在于，它有可能会停留在一个局部最小值处，而这取决于初始化的情况。</p>
<p>为了解决这个问题，我们通常需要多次运行<strong>K-均值</strong>算法，每一次都重新进行随机初始化，最后再比较多次运行<strong>K-均值</strong>的结果，选择代价函数最小的结果。在 $K$ 不是很大的时候效果不错。</p>
<h2 id="选择聚类数-Choosing-the-Number-of-Clusters"><a href="#选择聚类数-Choosing-the-Number-of-Clusters" class="headerlink" title="选择聚类数 Choosing the Number of Clusters"></a>选择聚类数 Choosing the Number of Clusters</h2><p>“肘部法则”，选代价函数关于 $K$ 的拐点。</p>
<p>根据聚类之后的问题，就是根据实际情况选择。</p>
<h2 id="一些参考参数"><a href="#一些参考参数" class="headerlink" title="一些参考参数"></a>一些参考参数</h2><ul>
<li><p>相似度/距离计算方法总结</p>
<ul>
<li><p>闵可夫斯基距离<strong>Minkowski</strong>/（其中欧式距离：$p=2$) </p>
</li>
<li><p>杰卡德相似系数(<strong>Jaccard</strong>)</p>
</li>
<li><p>余弦相似度(<strong>cosine similarity</strong>)</p>
</li>
<li>Pearson皮尔逊相关系数</li>
</ul>
</li>
<li><p>聚类的衡量指标</p>
<ul>
<li>均一性：$p$。<ul>
<li>类似于精确率，一个簇中只包含一个类别的样本，则满足均一性。其实也可以认为就是正确率(每个 聚簇中正确分类的样本数占该聚簇总样本数的比例和)</li>
</ul>
</li>
<li>完整性：$r$。<ul>
<li>类似于召回率，同类别样本被归类到相同簇中，则满足完整性;每个聚簇中正确分类的样本数占该类型的总样本数比例的和</li>
</ul>
</li>
<li><strong>V-measure</strong>。<ul>
<li>均一性和完整性的加权平均。</li>
<li>$V = \dfrac{(1+\beta^2)\times pr}{\beta^2\times p+r}$。</li>
</ul>
</li>
<li>轮廓系数。样本$i$的轮廓系数：$s(i)$。<ul>
<li>簇内不相似度:计算样本$i$到同簇其它样本的平均距离为$a(i)$，应尽可能小。</li>
<li>簇间不相似度:计算样本$i$到其它簇$C_j$的所有样本的平均距离$b_{ij}$，应尽可能大。</li>
<li>轮廓系数：$s(i)$值越接近1表示样本$i$聚类越合理，越接近-1，表示样本$i$应该分类到 另外的簇中，近似为0，表示样本$i$应该在边界上;所有样本的$s(i)$的均值被成为聚类结果的轮廓系数。 </li>
<li>$s(i) = \dfrac{b(i)-a(i)}{max\{a(i),b(i)\}}$</li>
</ul>
</li>
<li><strong>ARI</strong></li>
</ul>
</li>
</ul>
<hr>
<h1 id="Lecture-14"><a href="#Lecture-14" class="headerlink" title="Lecture 14"></a>Lecture 14</h1><p>本章主要介绍的数据降维，把多个特征值压缩到更少的数量表示。</p>
<h2 id="主成分分析问题-Principal-Component-Analysis-Problem-Formulation"><a href="#主成分分析问题-Principal-Component-Analysis-Problem-Formulation" class="headerlink" title="主成分分析问题 Principal Component Analysis Problem Formulation"></a>主成分分析问题 Principal Component Analysis Problem Formulation</h2><p>主成分分析(<strong>PCA</strong>)是最常见的降维算法。</p>
<p>在<strong>PCA</strong>中，我们要做的是找到一个方向向量（<strong>Vector direction</strong>），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。</p>
<p>问题是要将$n$维数据降至$k$维，目标是找到向量$u^{(1)}$,$u^{(2)}$,…,$u^{(k)}$使得总的投射误差最小。（其实就是找到线性无关的 $k$ 个向量，张成的线性子空间，然后要其他所有的向量在子空间投影最小。）</p>
<p><strong>PCA</strong>技术的一个很大的优点是，它是完全无参数限制的。在<strong>PCA</strong>的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。</p>
<h2 id="主成分分析算法-Principal-Component-Analysis-Algorithm"><a href="#主成分分析算法-Principal-Component-Analysis-Algorithm" class="headerlink" title="主成分分析算法 Principal Component Analysis Algorithm"></a>主成分分析算法 Principal Component Analysis Algorithm</h2><ol>
<li>均值归一化。我们需要计算出所有特征的均值，然后令 $x_j= x_j-μ_j$。如果特征是在不同的数量级上，我们还需要将其除以标准差 $\sigma$。</li>
<li>计算<strong>协方差矩阵</strong>（<strong>covariance matrix</strong>）$Σ$：$\Sigma = \dfrac{1}{m} \sum\limits_{i = 1}^{m}(x^{(i)})(x^{(i)})^T$。</li>
<li>计算协方差矩阵$\Sigma$的<strong>特征向量</strong>（<strong>eigenvectors</strong>）。<ul>
<li>在 <strong>Octave</strong> 里我们可以利用<strong>奇异值分解</strong>（<strong>singular value decomposition</strong>）来求解，<code>[U, S, V]= svd(sigma)</code>。</li>
</ul>
</li>
</ol>
<p>$U$是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从$n$维降至$k$维，我们只需要从$U$中选取前$k$个向量，获得一个$n×k$维度的矩阵，我们用$U_{reduce}$表示，然后通过如下计算获得要求的新特征向量$z^{(i)}$: $z^{(i)}=U^{T}_{reduce}*x^{(i)}$。</p>
<h2 id="选择主成分的数量-Choosing-The-Number-Of-Principal-Components"><a href="#选择主成分的数量-Choosing-The-Number-Of-Principal-Components" class="headerlink" title="选择主成分的数量 Choosing The Number Of Principal Components"></a>选择主成分的数量 Choosing The Number Of Principal Components</h2><p>主要成分分析是减少投射的平均均方误差：</p>
<p>训练集的方差为：$\dfrac{1}{m}\sum^{m}_{i=1}\left| x^{\left( i\right) }\right|^{2}$</p>
<p>我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的$k$值。</p>
<p>当我们在<strong>Octave</strong>中调用“<strong>svd</strong>”函数的时候，我们获得三个参数：<code>[U, S, V] = svd(sigma)</code>。其中 $S$ 是一个对角阵，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例：</p>
<script type="math/tex; mode=display">
\dfrac {\dfrac {1}{m}\sum\limits^{m}_{i=1}\left\| x^{\left( i\right) }-x^{\left( i\right) }_{approx}\right\| ^{2}}{\dfrac{1}{m}\sum\limits^{m}_{i=1}\left\| x^{(i)}\right\| ^{2}}=1-\dfrac {\sum\limits^{k}_{i=1}S_{ii}}{\sum\limits^{m}_{i=1}S_{ii}}\leq 1\%</script><p>也就是 $\dfrac{\sum\limits^{k}_{i=1}S_{ii}}{\sum\limits^{n}_{i=1}s_{ii}}\geq 0.99$</p>
<p>在压缩过数据后，我们可以采用如下方法来近似地获得原有的特征：<script type="math/tex">x^{\left( i\right) }_{approx}=U_{reduce}z^{(i)}</script></p>
<h2 id="主成分分析法的应用建议-Advice-for-Applying-PCA"><a href="#主成分分析法的应用建议-Advice-for-Applying-PCA" class="headerlink" title="主成分分析法的应用建议 Advice for Applying PCA"></a>主成分分析法的应用建议 Advice for Applying PCA</h2><ul>
<li>PCA不应该用来减少过拟合。</li>
<li>PCA不是必要的，建议先从原始特征开始训练。</li>
</ul>
<hr>
<h1 id="Lecture-15"><a href="#Lecture-15" class="headerlink" title="Lecture 15"></a>Lecture 15</h1><p>本章主要介绍<strong>异常检测（Anomaly Detection）</strong>，通过概率分布 $p(x)$ 判断一组数据的可能性，检测异常。</p>
<h2 id="高斯分布-Gaussian-Distribution"><a href="#高斯分布-Gaussian-Distribution" class="headerlink" title="高斯分布 Gaussian Distribution"></a>高斯分布 Gaussian Distribution</h2><p>就是正态分布$x \sim N(\mu, \sigma^2)$，概率密度函数：</p>
<script type="math/tex; mode=display">
p(x; \mu, \sigma^2) = \dfrac{1}{\sqrt{2\pi \sigma^2}} \exp{\left(- \dfrac{(x - \mu)^2}{2 \sigma^2}\right)}</script><p>我们可以利用已有的数据来预测总体中的参数：</p>
<ul>
<li>$\mu =  \dfrac{1}{m} \sum\limits_{i = 1}^{m}x^{(i)}$</li>
<li>$\sigma^2=\dfrac{1}{m}\sum\limits_{i=1}^{m}(x^{(i)}-\mu)^2$</li>
</ul>
<h2 id="算法-Algorithm"><a href="#算法-Algorithm" class="headerlink" title="算法 Algorithm"></a>算法 Algorithm</h2><p>对于给定的数据集 $x^{(1)}, …, x^{(m)}$，每个特征都会有一个分布，即 $\mu_j, \sigma_j^2, j = 1, 2, …, n$。</p>
<p>一旦我们获得了平均值和方差的估计值，给定新的一个训练实例，根据模型计算 $p(x)$：</p>
<script type="math/tex; mode=display">
p(x) = \prod\limits_{j = 1}^{n}p(x_j; \mu_j, \sigma_j^2) = \prod_{j = 1}^{n} \dfrac{1}{\sqrt{2\pi }\sigma_j} \exp{\left(- \dfrac{(x_j - \mu_j)^2}{2 \sigma_j^2}\right)}</script><p>当 $p(x) &lt; \varepsilon$ 时，为异常。</p>
<h2 id="开发和评价-Developing-and-Evaluating"><a href="#开发和评价-Developing-and-Evaluating" class="headerlink" title="开发和评价 Developing and Evaluating"></a>开发和评价 Developing and Evaluating</h2><p>异常检测算法是一个非监督学习算法，意味着我们无法根据结果变量 $ y$ 的值来告诉我们数据是否真的是异常的。我们需要另一种方法来帮助检验算法是否有效。</p>
<p>具体评价方法如下：</p>
<ol>
<li>根据测试集数据，我们估计特征的平均值和方差并构建$p(x)$函数。</li>
<li>对交叉检验集，我们尝试使用不同的$\varepsilon$值作为阀值，并预测数据是否异常，根据$F1$值或者查准率与查全率的比例来选择 $\varepsilon$。</li>
<li>选出 $\varepsilon$ 后，针对测试集进行预测，计算异常检验系统的$F1$值，或者查准率与查全率之比。</li>
</ol>
<h2 id="异常检测与监督学习对比-Anomaly-Detection-vs-Supervised-Learning"><a href="#异常检测与监督学习对比-Anomaly-Detection-vs-Supervised-Learning" class="headerlink" title="异常检测与监督学习对比 Anomaly Detection vs. Supervised Learning"></a>异常检测与监督学习对比 Anomaly Detection vs. Supervised Learning</h2><div class="table-container">
<table>
<thead>
<tr>
<th>异常检测</th>
<th>监督学习</th>
</tr>
</thead>
<tbody>
<tr>
<td>非常少量的正向类（异常数据 $y=1$）, 大量的负向类（$y=0$）</td>
<td>同时有大量的正向类和负向类</td>
</tr>
<tr>
<td>许多不同种类的异常，非常难。根据非常 少量的正向类数据来训练算法。</td>
<td>有足够多的正向类实例，足够用于训练 算法，未来遇到的正向类实例可能与训练集中的非常近似。</td>
</tr>
<tr>
<td>未来遇到的异常可能与已掌握的异常、非常的不同。</td>
<td></td>
</tr>
<tr>
<td>例如： 欺诈行为检测 生产（例如飞机引擎）检测数据中心的计算机运行状况</td>
<td>例如：邮件过滤器 天气预报 肿瘤分类</td>
</tr>
</tbody>
</table>
</div>
<p>通常来说，正样本的数量很少，甚至有时候是0，也就是说，出现了太多没见过的不同的异常类型，那么对于这些问题，通常应该使用的算法就是异常检测算法。</p>
<h2 id="选择特征-Choosing-What-Features-to-Use"><a href="#选择特征-Choosing-What-Features-to-Use" class="headerlink" title="选择特征 Choosing What Features to Use"></a>选择特征 Choosing What Features to Use</h2><p>对于异常检测算法，我们使用的特征是至关重要的。</p>
<p>异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布。比如：</p>
<ul>
<li>对数函数 $x := \log{(x + c)}, c &gt; 0$。python 中有 <code>np.log1p()</code></li>
<li>$x := x^c, 0 &lt; c &lt; 1$</li>
</ul>
<p>一个常见的问题是一些异常的数据可能也会有较高的 $p(x)$ 值，因而被算法认为是正常的。这种情况下<strong>误差分析</strong>能够帮助我们，我们可以分析那些被算法错误预测为正常的数据，观察能否找出一些问题。我们可能能从问题中发现我们需要增加一些新的特征，增加这些新特征后获得的新算法能够帮助我们更好地进行异常检测。</p>
<p>我们通常可以通过将一些相关的特征进行组合，来获得一些新的更好的特征。</p>
<h2 id="多元高斯函数-Multivariate-Gaussian-Distribution"><a href="#多元高斯函数-Multivariate-Gaussian-Distribution" class="headerlink" title="多元高斯函数 Multivariate Gaussian Distribution"></a>多元高斯函数 Multivariate Gaussian Distribution</h2><p>其实就是用来解决特征间有相关性，前面都是假设特征相互独立。</p>
<p>在多元高斯分布模型中，我们将构建特征的协方差矩阵，用所有的特征一起来计算 $p(x)$。</p>
<ul>
<li>平均值 $\mu =  \dfrac{1}{m} \sum\limits_{i = 1}^{m}x^{(i)}$。（这是一个向量）</li>
<li><p>协方差矩阵 $\Sigma = \dfrac{1}{m}\sum\limits_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T = \dfrac{1}{m}(X-\mu)^T(X-\mu)$。</p>
</li>
<li><p>$p(x)=\dfrac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}\exp{\left(-\dfrac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)}$。（$|\Sigma|$ 是 $\Sigma$ 的行列式，$\Sigma^{-1}$ 是逆矩阵）</p>
</li>
</ul>
<p><em>原本的高斯分布模型是多元高斯分布模型的一个子集，如果协方差矩阵只在对角线的单位上有非零的值时，即为原本的高斯分布模型了。</em></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>原高斯分布模型</th>
<th>多元高斯分布模型</th>
</tr>
</thead>
<tbody>
<tr>
<td>不能捕捉特征之间的相关性 但可以通过将特征进行组合的方法来解决</td>
<td>自动捕捉特征之间的相关性</td>
</tr>
<tr>
<td>计算代价低，能适应大规模的特征</td>
<td>计算代价较高 训练集较小时也同样适用</td>
</tr>
<tr>
<td></td>
<td>必须要有 $m&gt;n$，不然的话协方差矩阵$\Sigma$不可逆的，通常需要 $m&gt;10n$ 另外特征冗余也会导致协方差矩阵不可逆</td>
</tr>
</tbody>
</table>
</div>
<p>原高斯分布模型被广泛使用着，如果特征之间在某种程度上存在相互关联的情况，我们可以通过构造新新特征的方法来捕捉这些相关性。</p>
<p>如果训练集不是太大，并且没有太多的特征，我们可以使用多元高斯分布模型。</p>
<hr>
<h1 id="Lecture-16"><a href="#Lecture-16" class="headerlink" title="Lecture 16"></a>Lecture 16</h1><p>本章主要介绍<strong>推荐系统（Recommender System）</strong>。</p>
<h2 id="问题形式化-Problem-Formulation"><a href="#问题形式化-Problem-Formulation" class="headerlink" title="问题形式化 Problem Formulation"></a>问题形式化 Problem Formulation</h2><p>主要通过一个电影的例子，定义推荐系统的问题。</p>
<p>引入一些标记：</p>
<ul>
<li>$n_u$：用户数量。</li>
<li>$n_m$：电影数量。</li>
<li>$r(i,j)$：如果用户 $j$ 给电影 $i$ 打过分则 $r(i,j) = 1$。</li>
<li>$y^{(i,j)}$：用户 $j$ 给 $i$ 打的分。</li>
<li>$m_j$：用户 $j$ 评过分的电影数。</li>
</ul>
<h2 id="基于内容的推荐系统-Content-Based-Recommendations"><a href="#基于内容的推荐系统-Content-Based-Recommendations" class="headerlink" title="基于内容的推荐系统 Content Based Recommendations"></a>基于内容的推荐系统 Content Based Recommendations</h2><p>在一个基于内容的推荐系统算法中，我们假设对于我们希望推荐的东西有一些数据，这些数据是有关这些东西的特征。</p>
<p>每部电影就都有一个特征向量，给予这些特征来构建推荐系统算法。</p>
<p>采用线性回归模型，针对每一个用户训练一个线性回归模型。</p>
<p>于是我们有：</p>
<ul>
<li>$\theta^{(j)}$：用户 $j$ 的模型参数向量。</li>
<li>$x^{(i)}$：电影 $i$ 的特征向量。</li>
</ul>
<p>第 $j$ 个用户对第 $i$ 个电影的评分就预测为 $(\theta^{(j)})^T x^{(i)}$。</p>
<p>针对用户 $j$ 的代价函数就是：</p>
<script type="math/tex; mode=display">
\min\limits_{\theta^{(j)}} \dfrac{1}{2}\sum\limits_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\dfrac{\lambda}{2}\left(\theta_{k}^{(j)}\right)^2</script><p>在一般的线性回归模型中，误差项和正则项应该都是乘以$1/2m$，在这里我们将$m$去掉。并且我们不对方差项$\theta_0$进行正则化处理。</p>
<p>上面的代价函数只是针对一个用户的，为了学习所有用户，我们将所有用户的代价函数求和：</p>
<script type="math/tex; mode=display">
\min\limits_{\theta^{(1)},...,\theta^{(n_u)}} \dfrac{1}{2}\sum\limits_{j=1}^{n_u}\sum\limits_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\dfrac{\lambda}{2}\sum\limits_{j=1}^{n_u}\sum\limits_{k=1}^{n}(\theta_k^{(j)})^2</script><p>如果我们要用梯度下降法来求解最优解，我们计算代价函数的偏导数后得到梯度下降的更新公式为：</p>
<script type="math/tex; mode=display">
\theta_k^{(j)}:=\theta_k^{(j)}-\alpha\sum\limits_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_{k}^{(i)} \quad (\text{for} \, k = 0)</script><script type="math/tex; mode=display">
\theta_k^{(j)}:=\theta_k^{(j)}-\alpha\left(\sum\limits_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_{k}^{(i)}+\lambda\theta_k^{(j)}\right) \quad (\text{for} \, k\neq 0)</script><h2 id="协同过滤-Collaborative-Filtering"><a href="#协同过滤-Collaborative-Filtering" class="headerlink" title="协同过滤 Collaborative Filtering"></a>协同过滤 Collaborative Filtering</h2><p>在之前的基于内容的推荐系统中，对于每一部电影，我们都掌握了可用的特征，使用这些特征训练出了每一个用户的参数。相反地，如果我们拥有用户的参数，我们可以学习得出电影的特征。</p>
<script type="math/tex; mode=display">
\min\limits_{x^{(1)},...,x^{(n_m)}}\dfrac{1}{2}\sum\limits_{i=1}^{n_m}\sum\limits_{j:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\dfrac{\lambda}{2}\sum\limits_{i=1}^{n_m}\sum\limits_{k=1}^{n}\left(x_k^{(i)}\right)^2</script><p>但是如果我们既没有用户的参数，也没有电影的特征，这两种方法都不可行了。协同过滤算法可以同时学习这两者。</p>
<p>我们的优化目标便改为同时针对$x$和$\theta$进行。</p>
<script type="math/tex; mode=display">
J(x^{(1)},...x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)})=\dfrac{1}{2}\sum\limits_{(i:j):r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\dfrac{\lambda}{2}\sum\limits_{i=1}^{n_m}\sum\limits_{k=1}^{n}\left(x_k^{(j)}\right)^2+\dfrac{\lambda}{2}\sum\limits_{j=1}^{n_u}\sum\limits_{k=1}^{n}\left(\theta_k^{(j)}\right)^2</script><p>对代价函数求偏导数的结果如下：</p>
<script type="math/tex; mode=display">
x_k^{(i)}:=x_k^{(i)}-\alpha\left(\sum\limits_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})\theta_k^{(j)}+\lambda x_k^{(i)}\right)</script><script type="math/tex; mode=display">
\theta_k^{(i)}:=\theta_k^{(i)}-\alpha\left(\sum\limits_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}+\lambda \theta_k^{(j)}\right)</script><p>在协同过滤从算法中，我们通常不使用截距项，如果需要的话，算法会自动学得。</p>
<p>协同过滤算法使用步骤如下：</p>
<ol>
<li><p>初始 $x^{(1)},x^{(1)},…x^{(n_m)},\ \theta^{(1)},\theta^{(2)},…,\theta^{(n_u)}$为一些随机小值。</p>
</li>
<li><p>使用梯度下降算法最小化代价函数。</p>
</li>
<li><p>在训练完算法后，我们预测 $(\theta^{(j)})^T x^{(i)}$ 为用户 $j$ 给电影 $i$ 的评分。</p>
</li>
</ol>
<h2 id="向量化：低秩矩阵分解-Vectorization-Low-Rank-Matrix-Factorization"><a href="#向量化：低秩矩阵分解-Vectorization-Low-Rank-Matrix-Factorization" class="headerlink" title="向量化：低秩矩阵分解 Vectorization_Low Rank Matrix Factorization"></a>向量化：低秩矩阵分解 Vectorization_Low Rank Matrix Factorization</h2><p>矩阵 $Y$ 保存所有用户和电影的评分数据。</p>
<p>其实我们在协同过滤中做的事叫低秩矩阵分解（当然不懂也没关系）。</p>
<p>推荐：如果一位用户正在观看电影 $x^{(i)}$，我们可以寻找另一部电影$x^{(j)}$，依据两部电影的特征向量之间的距离$\left|x^{(i)}-x^{(j)} \right|$的大小。</p>
<h2 id="均值归一化-Mean-Normalization"><a href="#均值归一化-Mean-Normalization" class="headerlink" title="均值归一化 Mean Normalization"></a>均值归一化 Mean Normalization</h2><p>这主要是处理对于新用户的处理。</p>
<p>我们首先需要对结果 $Y $矩阵进行均值归一化处理，将每一个用户对某一部电影的评分减去所有用户对该电影评分的平均值。</p>
<p>然后我们利用这个新的 $Y$ 矩阵来训练算法。</p>
<p>如果我们要用新训练出的算法来预测评分，则需要将平均值重新加回去，预测$(\theta^{(j)})^T x^{(i)}+\mu_i$。</p>
<hr>
<h1 id="Lecture-17"><a href="#Lecture-17" class="headerlink" title="Lecture 17"></a>Lecture 17</h1><p>本章主要介绍大规模机器学习中提升效率的一些技巧。</p>
<h2 id="大型数据集的学习-Learning-With-Large-Datasets"><a href="#大型数据集的学习-Learning-With-Large-Datasets" class="headerlink" title="大型数据集的学习 Learning With Large Datasets"></a>大型数据集的学习 Learning With Large Datasets</h2><p>首先应该做的事是去检查一个这么大规模的训练集是否真的必要，也许我们只用一个较小的训练集也能获得较好的效果，我们可以绘制学习曲线来帮助判断。</p>
<h2 id="随机梯度下降法-Stochastic-Gradient-Descent"><a href="#随机梯度下降法-Stochastic-Gradient-Descent" class="headerlink" title="随机梯度下降法 Stochastic Gradient Descent"></a>随机梯度下降法 Stochastic Gradient Descent</h2><p>在随机梯度下降法中，我们定义代价函数为一个单一训练实例的代价：</p>
<script type="math/tex; mode=display">
cost\left(  \theta, \left( x^{(i)} , y^{(i)} \right)  \right) = \dfrac{1}{2}\left( h_{\theta}\left(x^{(i)}\right)-y^{(i)} \right)^{2}</script><p>随机梯度算法为：首先对训练集要进行随机“洗牌”，然后</p>
<p> <strong>Repeat</strong> (usually anywhere between1-10) {</p>
<p>$\qquad$ for $i = 1 \text{ to } m$ {</p>
<p>$\qquad \qquad \theta_j := \theta_j - \alpha (h_\theta(x^{(i)} ) - y^{(i)}) x_j^{(i)}$ (for $j = 0 \text{ to } n$)</p>
<p>$\qquad$ }</p>
<p>}</p>
<p>随机梯度下降算法在每一次计算之后便更新参数 $$ ，而不需要首先将所有的训练集求和，在梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。但是这样的算法存在的问题是，不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊。</p>
<h2 id="小批量梯度下降-Mini-Batch-Gradient-Descent"><a href="#小批量梯度下降-Mini-Batch-Gradient-Descent" class="headerlink" title="小批量梯度下降 Mini-Batch Gradient Descent"></a>小批量梯度下降 Mini-Batch Gradient Descent</h2><p>小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数$b$次训练实例，便更新一次参数  $\theta$ 。</p>
<p> <strong>Repeat</strong> (usually anywhere between1-10) {</p>
<p>$\qquad$ for $i = 1 \text{ to } m$ {</p>
<p>$\qquad \qquad \theta_j := \theta_j - \alpha \dfrac{1}{b} \sum\limits_{k = i}^{i + b -1} (h_\theta(x^{(k)} ) - y^{(k)}) x_j^{(k)}$ (for $j = 0 \text{ to } n$)</p>
<p>$\qquad \qquad i += 10$</p>
<p>$\qquad$ }</p>
<p>}</p>
<p>通常我们会令 $b$ 在 2-100 之间。这样做的好处在于，我们可以用向量化的方式来循环 $b$个训练实例，如果我们用的线性代数函数库比较好，能够支持平行处理，那么算法的总体表现将不受影响（与随机梯度下降相同）。</p>
<h2 id="随机梯度下降收敛-Stochastic-Gradient-Descent-Convergence"><a href="#随机梯度下降收敛-Stochastic-Gradient-Descent-Convergence" class="headerlink" title="随机梯度下降收敛 Stochastic Gradient Descent Convergence"></a>随机梯度下降收敛 Stochastic Gradient Descent Convergence</h2><p>在随机梯度下降中，我们在每一次更新 $\theta $ 之前都计算一次代价，然后每$x$次迭代后，求出这 $x$ 次对训练实例计算代价的平均值，然后绘制这些平均值与$x$次迭代的次数之间的函数图表。</p>
<p>我们可以令学习率随着迭代次数的增加而减小，例如令：</p>
<p>​                            <script type="math/tex">\alpha = \dfrac{const1}{iterationNumber + const2}</script></p>
<p>随着我们不断地靠近全局最小值，通过减小学习率，我们迫使算法收敛而非在最小值附近徘徊。</p>
<p>我们介绍了一种方法，近似地监测出随机梯度下降算法在最优化代价函数中的表现，这种方法不需要定时地扫描整个训练集，来算出整个样本集的代价函数，而是只需要每次对最后一些个，求一下平均值。应用这种方法，既可以保证随机梯度下降法正在正常运转和收敛，也可以用它来调整学习速率$α$的大小。</p>
<h2 id="在线学习-Online-Learning"><a href="#在线学习-Online-Learning" class="headerlink" title="在线学习 Online Learning"></a>在线学习 Online Learning</h2><p>一个算法来从中学习的时候来模型化问题，在线学习算法指的是对数据流而非离线的静态数据集的学习。许多在线网站都有持续不断的用户流，对于每一个用户，网站希望能在不将数据存储到数据库中便顺利地进行算法学习。</p>
<p>在线学习的算法与随机梯度下降算法有些类似，我们对单一的实例进行学习，而非对一个提前定义的训练集进行循环。</p>
<p><strong>Repeat</strong> forever (as long as the website is running) {</p>
<p>$\qquad$Get $\left(x,y\right)$ corresponding to the current user </p>
<p>$\qquad \theta:=\theta_{j}-\alpha\left(h_{\theta}\left(x\right)-y \right)x_{j}$  (for $j=0 \text{ to }n$) </p>
<p>}</p>
<p>一旦对一个数据的学习完成了，我们便可以丢弃该数据，不需要再存储它了。这种方式的好处在于，我们的算法可以很好的适应用户的倾向性，算法可以针对用户的当前行为不断地更新模型以适应该用户。</p>
<p><em>在线学习的一个优点就是，如果你有一个变化的用户群，又或者你在尝试预测的事情，在缓慢变化，就像你的用户的品味在缓慢变化，这个在线学习算法，可以慢慢地调试你所学习到的假设，将其调节更新到最新的用户行为。</em></p>
<h2 id="映射化简和数据并行-Map-Reduce-and-Data-Parallelism"><a href="#映射化简和数据并行-Map-Reduce-and-Data-Parallelism" class="headerlink" title="映射化简和数据并行 Map Reduce and Data Parallelism"></a>映射化简和数据并行 Map Reduce and Data Parallelism</h2><p>如果我们能够将我们的数据集分配给不多台计算机，让每一台计算机处理数据集的一个子集，然后我们将计所的结果汇总在求和。这样的方法叫做映射简化。</p>
<p>具体而言，如果任何学习算法能够表达为，对训练集的函数的求和，那么便能将这个任务分配给多台计算机（或者同一台计算机的不同<strong>CPU</strong> 核心），以达到加速处理的目的。</p>
<p>很多高级的线性代数函数库已经能够利用多核<strong>CPU</strong>的多个核心来并行地处理矩阵运算，这也是算法的向量化实现如此重要的缘故（比调用循环快）。</p>
<h1 id="Lecture-18"><a href="#Lecture-18" class="headerlink" title="Lecture 18"></a>Lecture 18</h1><p>本节主要是介绍一个应用实例——文字图片识别（Photo OCR），科普性质。</p>
<h2 id="问题描述和流程图-Problem-Description-and-Pipeline"><a href="#问题描述和流程图-Problem-Description-and-Pipeline" class="headerlink" title="问题描述和流程图 Problem Description and Pipeline"></a>问题描述和流程图 Problem Description and Pipeline</h2><p>图像文字识别应用所作的事是，从一张给定的图片中识别文字。</p>
<p>为了完成这样的工作，需要采取如下步骤：</p>
<ol>
<li>文字侦测（<strong>Text detection</strong>）——将图片上的文字与其他环境对象分离开来</li>
<li>字符切分（<strong>Character segmentation</strong>）——将文字分割成一个个单一的字符</li>
<li>字符分类（<strong>Character classification</strong>）——确定每一个字符是什么<br>可以用任务流程图来表达这个问题，每一项任务可以由一个单独的小队来负责解决：</li>
</ol>
<pre class="mermaid">graph LR;
  A(Image) --> B(Text detection)
  B --> C(Character segmentation)
  C --> D(Character classification)</pre>

<h2 id="滑动窗口-Sliding-Windows"><a href="#滑动窗口-Sliding-Windows" class="headerlink" title="滑动窗口 Sliding Windows"></a>滑动窗口 Sliding Windows</h2><p>滑动窗口是一项用来从图像中抽取对象的技术。假使我们需要在一张图片中识别行人，首先要做的是用许多固定尺寸的图片来训练一个能够准确识别行人的模型。然后我们用之前训练识别行人的模型时所采用的图片尺寸在我们要进行行人识别的图片上进行剪裁，然后将剪裁得到的切片交给模型，让模型判断是否为行人，然后在图片上滑动剪裁区域重新进行剪裁，将新剪裁的切片也交给模型进行判断，如此循环直至将图片全部检测完。</p>
<p>一旦完成后，我们按比例放大剪裁的区域，再以新的尺寸对图片进行剪裁，将新剪裁的切片按比例缩小至模型所采纳的尺寸，交给模型进行判断，如此循环。</p>
<p>滑动窗口技术也被用于文字识别，首先训练模型能够区分字符与非字符，然后，运用滑动窗口技术识别字符，一旦完成了字符的识别，我们将识别得出的区域进行一些扩展，然后将重叠的区域进行合并。接着我们以宽高比作为过滤条件，过滤掉高度比宽度更大的区域（认为单词的长度通常比高度要大）。</p>
<h2 id="获取大量数据和人工数据-Getting-Lots-of-Data-and-Artificial-Data"><a href="#获取大量数据和人工数据-Getting-Lots-of-Data-and-Artificial-Data" class="headerlink" title="获取大量数据和人工数据 Getting Lots of Data and Artificial Data"></a>获取大量数据和人工数据 Getting Lots of Data and Artificial Data</h2><p>数据不总是可以直接获得的，我们有可能需要人工地创造一些数据。</p>
<p>有关获得更多数据的几种方法：</p>
<ol>
<li>人工数据合成</li>
<li>手动收集、标记数据</li>
<li>众包</li>
</ol>
<h2 id="上限分析-Ceiling-Analysis"><a href="#上限分析-Ceiling-Analysis" class="headerlink" title="上限分析 Ceiling Analysis"></a>上限分析 Ceiling Analysis</h2><p>在机器学习的应用中，我们通常需要通过几个步骤才能进行最终的预测，上限分析是用来判断哪一部分最值得我们花时间和精力去改善。</p>
<p>总体就是一个调试的过程，我们按照流程一步一步手工提供100%正确的输入，看哪一个步骤的提升最多，就意味着有改进空间。</p>
<h1 id="Lecture-19"><a href="#Lecture-19" class="headerlink" title="Lecture 19"></a>Lecture 19</h1><p>完结撒花！<span class="github-emoji" alias="rose" style fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f339.png?v8">&#x1f339;</span></p>
<h2 id="一个自己的-Conclusion"><a href="#一个自己的-Conclusion" class="headerlink" title="一个自己的 Conclusion"></a>一个自己的 Conclusion</h2><p>首先反省一下自己的拖沓，本来其实能很快看完的一套课硬是看了一年。。。</p>
<p>看下来能够吴恩达老师是一个比较负责且耐心且有实力的老师，真不愧是斯坦福的。</p>
<p>作为一套机器学习的入门课，这门课程的设置和讲授还是相当深入浅出的，唯一不足就是可能有点过于老旧，一些概念和技术可以得到更新，比如现在更多地使用 <code>python</code> 而不是老师的 <code>Ovctor</code>，然后神经网络在当今的应用其实非常广泛。</p>
<p>然后作为斯坦福的一个面向全社会的公开课，很多的概念内容其实讲得比较泛化且偏基础，缺少更多的数学（想要数学的可以看西瓜书），但是很多基本模型的介绍也算是让人能够很好地掌握。看了一下油管上吴恩达直接在斯坦福的课，确实比这套课更加详细并且有更多实操的东西（其实从课时上也看出，一节课就四五十分钟总共18节课，<del>还要啥自行车</del>）。但作为一个入门足够了，要深入了解 machine learning 还是要继续大量的学习。</p>
<p>最后看一下我们学了什么。</p>
<ul>
<li>监督学习<ul>
<li>线性回归</li>
<li>逻辑回归</li>
<li>神经网络</li>
<li>支持向量机（SVM）</li>
</ul>
</li>
<li>无监督学习<ul>
<li>K-均值聚类（K-means）</li>
<li>主成分分析（PCA）</li>
<li>异常检测</li>
</ul>
</li>
<li>其他主题<ul>
<li>推荐系统</li>
<li>并行系统和映射简化</li>
</ul>
</li>
</ul>
<p>然后最主要的是我觉得这门课讲了很多在设计和调试机器学习系统模型的工具和技巧：</p>
<ul>
<li>偏差和方差的问题</li>
<li>方差问题的正则化</li>
<li>学习算法的评价法</li>
<li>查准率、召回率以及F1分数</li>
<li>训练集、交叉验证集和测试集</li>
<li>学习曲线，误差分析、上限分析</li>
</ul>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
</search>
